[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Last Updated: Saturday 03 01, 2025 at 17:28PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "",
    "text": "The New York City’s new Commission to Analyze Taxpayer Spending (CATS) intended to understand the city’s expenses and wondered if there were any opportunities to spend taxpayers’ money more efficiently. As a senior technical analyst, I will help analyze the city payroll and identify the instances where senior agency officials make significantly more than rank-and-file city employees. I will use City payroll data from NYC Open Data and highlight possible savings to be submitted to the commission."
  },
  {
    "objectID": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay-if-needed-assume-a-standard-2000-hour-work-year-and-no-overtime.",
    "href": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay-if-needed-assume-a-standard-2000-hour-work-year-and-no-overtime.",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.1 Which job title has the highest base rate of pay? (If needed, assume a standard 2000 hour work year and no overtime.)",
    "text": "3.1 Which job title has the highest base rate of pay? (If needed, assume a standard 2000 hour work year and no overtime.)\n\n\nShow the code\ntotal_compensation |&gt; mutate(\n  base_rate = case_when(\n    pay_basis == \"per Annum\" ~ base_salary, \n    pay_basis == \"per Hour\" ~ base_salary * 2000,\n    pay_basis == \"per Day\" ~ base_salary * 2000/7.5,\n    TRUE ~ NA_real_  \n  )\n) |&gt; group_by(title_description) |&gt; summarize(mean_rate=mean(base_rate)) |&gt;\n  arrange(desc(mean_rate))\n\n\n# A tibble: 1,977 × 2\n   title_description                                 mean_rate\n   &lt;chr&gt;                                                 &lt;dbl&gt;\n 1 Custodian Engineer                               120870091.\n 2 Member, Civilian Complaint Review Board             639785.\n 3 Medical Investigator                                623926.\n 4 Chairman                                            379618.\n 5 Member Of The Environmental Control Board - Oath    350188.\n 6 Chief Actuary                                       296470.\n 7 Pension Investment Advisor                          295814.\n 8 Chancellor                                          287066.\n 9 Captain - Chief Of Staff                            276588 \n10 First Deputy Mayor                                  274919.\n# ℹ 1,967 more rows\n\n\nWe can see the list of job titles with the highest base rate of pay. But the first title seems an outlier, I would say the member of civilian complaint review board has the highest base rate of pay."
  },
  {
    "objectID": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-payroll-regular-and-overtime-combined",
    "href": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-payroll-regular-and-overtime-combined",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.2 Which individual & in what year had the single highest city total payroll (regular and overtime combined)?",
    "text": "3.2 Which individual & in what year had the single highest city total payroll (regular and overtime combined)?\n\n\nShow the code\ntotal_compensation |&gt; \n  summarize(fiscal_year,first_name,last_name,total_compensation) |&gt;\n  arrange(desc(total_compensation))\n\n\n# A tibble: 6,225,611 × 4\n   fiscal_year first_name last_name   total_compensation\n         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                    &lt;dbl&gt;\n 1        2022 Gregory    Russ                    414707\n 2        2021 Gregory    Russ                    414707\n 3        2020 Gregory    Russ                    414707\n 4        2024 Lisa       Bova-Hiatt              393532\n 5        2024 Steven     Meier                   373320\n 6        2023 David      Banks                   363346\n 7        2023 David      Banks                   363346\n 8        2022 David      Banks                   363346\n 9        2022 Meisha     Ross Porter             363346\n10        2021 Meisha     Ross Porter             363346\n# ℹ 6,225,601 more rows\n\n\nWe can see Gregory Russ had the highest payroll of 414707 in 2020, 2021, and 2022."
  },
  {
    "objectID": "mp01.html#which-individual-worked-the-most-overtime-hours-in-this-data-set",
    "href": "mp01.html#which-individual-worked-the-most-overtime-hours-in-this-data-set",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.3 Which individual worked the most overtime hours in this data set?",
    "text": "3.3 Which individual worked the most overtime hours in this data set?\n\n\nShow the code\ntotal_compensation |&gt; summarize(fiscal_year,first_name,last_name,ot_hours) |&gt;\n  arrange(desc(ot_hours))\n\n\n# A tibble: 6,225,611 × 4\n   fiscal_year first_name last_name   ot_hours\n         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;\n 1        2022 James      Internicola    3693.\n 2        2022 Michael    Thompson       3618.\n 3        2022 Timothy    Sands          3556.\n 4        2014 John       Murphy         3348.\n 5        2022 Seeta      Deochan        3341.\n 6        2022 Dion       Middleton      3328.\n 7        2024 Kashwayne  Burnett        3303.\n 8        2022 Anthony    Messam         3271 \n 9        2022 Davey      Payne          3241 \n10        2022 Andre      Boucaud        3214 \n# ℹ 6,225,601 more rows\n\n\nJames Internicola worked the most overtime hours of 3693."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll-base-and-overtime-pay-per-employee",
    "href": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll-base-and-overtime-pay-per-employee",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.4 Which agency has the highest average total annual payroll (base and overtime pay per employee)?",
    "text": "3.4 Which agency has the highest average total annual payroll (base and overtime pay per employee)?\n\n\nShow the code\ntotal_compensation |&gt; group_by(agency_name) |&gt;\n  summarize(avg_tot_pay=mean(total_compensation)) |&gt;\n  arrange(desc(avg_tot_pay))\n\n\n# A tibble: 169 × 2\n   agency_name                    avg_tot_pay\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 Office Of Racial Equity            151093.\n 2 Commission On Racial Equity        136191 \n 3 Districting Commission             120641.\n 4 Office Of Criminal Justice         117280.\n 5 Office Of Collective Bargainin     114884.\n 6 Financial Info Svcs Agency         111230.\n 7 Office Of The Actuary              106924.\n 8 Bronx Community Board #3           105814.\n 9 Public Administrator-Richmond      105752.\n10 Municipal Water Fin Authority      101300.\n# ℹ 159 more rows\n\n\nOffice of Racial Equity has the highest average total annual payroll."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-most-employees-on-payroll-in-each-year",
    "href": "mp01.html#which-agency-has-the-most-employees-on-payroll-in-each-year",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.5 Which agency has the most employees on payroll in each year?",
    "text": "3.5 Which agency has the most employees on payroll in each year?\n\n\nShow the code\ntotal_compensation |&gt;\n  group_by(fiscal_year, agency_name) |&gt;\n  summarize(num_empl=n(),.groups=\"drop\") |&gt;\n  group_by(fiscal_year) |&gt;\n  slice_max(num_empl,n=1)\n\n\n# A tibble: 11 × 3\n# Groups:   fiscal_year [11]\n   fiscal_year agency_name            num_empl\n         &lt;dbl&gt; &lt;chr&gt;                     &lt;int&gt;\n 1        2014 Dept Of Ed Pedagogical   100589\n 2        2015 Dept Of Ed Pedagogical   111857\n 3        2016 Dept Of Ed Pedagogical   106263\n 4        2017 Dept Of Ed Pedagogical   104629\n 5        2018 Dept Of Ed Pedagogical   107956\n 6        2019 Dept Of Ed Pedagogical   112067\n 7        2020 Dept Of Ed Pedagogical   114999\n 8        2021 Dept Of Ed Pedagogical   113523\n 9        2022 Dept Of Ed Pedagogical   120453\n10        2023 Dept Of Ed Pedagogical   106882\n11        2024 Dept Of Ed Pedagogical   108209\n\n\nDepartment of Ed Pedagogical has the most employees in every year."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-overtime-usage-compared-to-regular-hours",
    "href": "mp01.html#which-agency-has-the-highest-overtime-usage-compared-to-regular-hours",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.6 Which agency has the highest overtime usage (compared to regular hours)?",
    "text": "3.6 Which agency has the highest overtime usage (compared to regular hours)?\n\n\nShow the code\ntotal_compensation |&gt;\n  group_by(agency_name) |&gt;\n  summarize(ot_usage=mean(ot_hours)/mean(regular_hours)) |&gt;\n  arrange(desc(ot_usage))\n\n\n# A tibble: 169 × 2\n   agency_name                  ot_usage\n   &lt;chr&gt;                           &lt;dbl&gt;\n 1 Board Of Election              0.200 \n 2 Fire Department                0.190 \n 3 Department Of Correction       0.185 \n 4 Department Of Sanitation       0.138 \n 5 Police Department              0.125 \n 6 Dept Of Citywide Admin Svcs    0.120 \n 7 Department Of Transportation   0.115 \n 8 Dept. Of Homeless Services     0.109 \n 9 Nyc Housing Authority          0.106 \n10 Admin For Children's Svcs      0.0812\n# ℹ 159 more rows\n\n\nBoard of Election has the highest overtime usage."
  },
  {
    "objectID": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs-that-is-whose-work_location_borough-is-not-one-of-the-five-counties.",
    "href": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs-that-is-whose-work_location_borough-is-not-one-of-the-five-counties.",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.7 What is the average salary of employees who work outside the five boroughs? (That is, whose work_location_borough is not one of the five counties.)",
    "text": "3.7 What is the average salary of employees who work outside the five boroughs? (That is, whose work_location_borough is not one of the five counties.)\n\n\nShow the code\ntotal_compensation |&gt;\n  filter(!work_location_borough %in% c(\"Manhattan\", \"Brooklyn\", \"Queens\", \n                                     \"Bronx\", \"Richmond\")) |&gt;\n  summarize(avg_salary=mean(total_compensation,na.rm=TRUE))\n\n\n# A tibble: 1 × 1\n  avg_salary\n       &lt;dbl&gt;\n1     55503.\n\n\nThe average salary of employees working outside the New York City is 55503."
  },
  {
    "objectID": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "href": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.8 How much has the city’s aggregate payroll grown over the past 10 years?",
    "text": "3.8 How much has the city’s aggregate payroll grown over the past 10 years?\n\n\nShow the code\ngrowth &lt;- total_compensation |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(agg_pay=mean(total_compensation,na.rm=TRUE))\n\nagg_pay_2024 &lt;- growth |&gt; filter(fiscal_year==\"2024\") |&gt;\n  pull(agg_pay)\n\nagg_pay_2014 &lt;- growth |&gt; filter(fiscal_year==\"2014\") |&gt;\n  pull(agg_pay)\n\ngrowth_amount &lt;- agg_pay_2024 - agg_pay_2014\ngrowth_percentage &lt;- growth_amount/agg_pay_2014*100\ngrowth_amount\n\n\n[1] 13384.23\n\n\nShow the code\ngrowth_percentage\n\n\n[1] 32.2868\n\n\nThe city’s payroll has grown 13384.23 dollars and 32.29% over the past 10 years.\nNow, I will analyze three possible policy changes to see their impact on overall spending. I have two policies suggested and will come up with my own policy proposal for analysis. For each policy, I will i) compute its impact on city payroll, ii) determine any other staffing adjustments required, and iii) make a recommendation to the CATS."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "2025 ‘Greenest Transit’ Awards Go to…",
    "section": "",
    "text": "This year’s Greenest Transit results just came out. The greeest transit agency awards have three categories: Large, Medium and Small, in terms of the sizes of agencies. Now, Let’s announce who the winners are! The greenest agencies are respectively MTA NYC Transit, Birmingham-Jefferson County Transit Authority, and Albany Transit System. The agencies with most emissions avoided are MTA New York City Transit, Hudson Transit Lines, and Hampton Jitney. The agencies with the highest electirfication level are TriMet, University of Georgia, and Connecticut Department of Transportation. We also have “winners” for the worst performance in terms of green and they are Washington State Ferries, SeaStreak, and Alaska Railroad Corporation. The relevent metrics with their values can be found in the appendix. To better demonstrate the ‘green-ness’ of the award winners, we have created some visualizations."
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThis press release announces the winners in each category. The greenest transit for large size agencies is King County Metro, in Seattle, Washington. For medium size agencies, Everett Transit in Everett, Washington is the greenest. The winner for small size agencies goes to RiverCities Transit from Longview, Washington."
  },
  {
    "objectID": "mp02.html#data-import",
    "href": "mp02.html#data-import",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "2 Data Import",
    "text": "2 Data Import\nWe download the EIA State Electricity Profiles, then use it to estimate the environmental impact of the electricity used to run certain transit systems.\n\n\nCode\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(stringr)\nensure_package(dplyr)\n\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n               # * 1000 kWh to MWH \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\n\nWe use the following code to create a table.\n\n\nCode\nensure_package(scales)\n\nEIA_SEP_REPORT |&gt; \n    select(-abbreviation) |&gt;\n    arrange(desc(CO2_MWh)) |&gt;\n    mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n           electricity_price_MWh = dollar(electricity_price_MWh), \n           generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n    rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n           `Primary Source of Electricity Generation`=primary_source, \n           `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n           `Total Generation Capacity (MWh)`= generation_MWh, \n           State=state) |&gt;\n    datatable()"
  },
  {
    "objectID": "mp02.html#initial-analysis-of-sep-data",
    "href": "mp02.html#initial-analysis-of-sep-data",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "3 Initial Analysis of SEP Data",
    "text": "3 Initial Analysis of SEP Data\nHere are some quick facts gained using the EIA_SEP_REPORT data.\n\nHawaii has the most expensive retail electricity with a price of $386 per MWh.\n\n\n\nCode\n# Which state has the most expensive retail electricity?\nstate_highest_price &lt;- EIA_SEP_REPORT |&gt;\n  select(electricity_price_MWh, state) |&gt;\n  slice_max(electricity_price_MWh, n=1)\n\n\n\nWest Virginia, whose primary source of electricity generation is coal, has the ‘dirtiest’ electricity mix with 1925 pounds of CO2 emitted per MWh.\n\n\n\nCode\n# Which state has the 'dirtiest' electricity mix?\nstate_dirtiest_electricity &lt;- EIA_SEP_REPORT |&gt;\n  select(CO2_MWh, primary_source, state) |&gt;\n  slice_max(CO2_MWh, n=1)\n\n\n\nOn average, 805.37 pounds of CO2 are emitted per MWh of electricity produced in the US.\n\n\n\nCode\n# On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US? (Note that you will need to use a suitably weighted average here.)\naverage_co2_emitted &lt;- EIA_SEP_REPORT |&gt;\n  summarize(\n    average_co2 = sum(CO2_MWh * generation_MWh) / sum(generation_MWh)\n  )\n\n\n\nPetroleum is the rarest primary energy source in the US used only in one state. The associated cost of electricity is $386 per MWh and it is used in Hawaii.\n\n\n\nCode\n# What is the rarest primary energy source in the US? \nrarest_source &lt;- EIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(num_state_source = n()) |&gt;\n  slice_min(num_state_source, n = 1)\n\n# What is the associated cost of electricity and where is it used?\nrarest_source_table &lt;- EIA_SEP_REPORT |&gt;\n  select(primary_source, electricity_price_MWh, state) |&gt;\n  filter(primary_source == \"Petroleum\")\n\n\n\nNY’s energy mix is 1.64 times cleaner than that of Texas.\n\n\n\nCode\n# How many times cleaner is NY’s energy mix than that of Texas?\nco2_emitted_NY &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"New York\") |&gt;\n  summarize(CO2_MWh)\n\nco2_emitted_Texas &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"Texas\") |&gt;\n  summarize(CO2_MWh)\n\ntimes_cleaner &lt;- co2_emitted_Texas / co2_emitted_NY\n\n\nWe now download the 2023 Annual Database Energy Consumption report and do some clean up. After that, we recode the Mode column.\n\n\nCode\n# import the data: 2023 Annual Database Energy Consumption\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n    DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                  destfile=NTD_ENERGY_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\n\n\n\nCode\n# do some basic clean-up\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n    replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(across(-c(`Agency Name`, \n                     `Mode`,\n                     `TOS`), \n                  to_numeric_fill_0)) |&gt;\n    group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n    summarize(across(where(is.numeric), sum), \n              .groups = \"keep\") |&gt;\n    mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n    filter(ENERGY &gt; 0) |&gt;\n    select(-ENERGY) |&gt;\n    ungroup()\n\n\n\n\nCode\n# clean up mode column in ntd energy\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"AR\" ~ \"Alaska Railroad\",\n        Mode == \"CB\" ~ \"Commuter Bus\",\n        Mode == \"CC\" ~ \"Cable Car\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"IP\" ~ \"Inclined Plane\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"MB\" ~ \"Bus\",\n        Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n        Mode == \"PB\" ~ \"Publico\",\n        Mode == \"RB\" ~ \"Bus Rapid Transit\",\n        Mode == \"SR\" ~ \"Streetcar Rail\",\n        Mode == \"TB\" ~ \"Trolleybus\",\n        Mode == \"TR\" ~ \"Aerial Tramways\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"YR\" ~ \"Hybrid Rail\",\n        TRUE ~ \"Unknown\"))\n\n\nNext, we download the 2023 Service by Agency report, from which we will extract characteristics of typical passenger trips on each transit service. We will also explore the data by answering some questions using the service data.\n\n\nCode\n# download the 2023 service by agency report\nlibrary(readr)\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n\n\n\nCode\n#clean up the service data\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n    mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n    rename(Agency = agency, \n           City   = max_city, \n           State  = max_state,\n           UPT    = sum_unlinked_passenger_trips_upt, \n           MILES  = sum_passenger_miles) |&gt;\n    select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n    filter(MILES &gt; 0)"
  },
  {
    "objectID": "mp02.html#recoding-the-mode-column",
    "href": "mp02.html#recoding-the-mode-column",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "4 Recoding the mode column",
    "text": "4 Recoding the mode column\n\n\nCode\n# clean up mode column in ntd energy\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"AR\" ~ \"Alaska Railroad\",\n        Mode == \"CB\" ~ \"Commuter Bus\",\n        Mode == \"CC\" ~ \"Cable Car\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"IP\" ~ \"Inclined Plane\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"MB\" ~ \"Bus\",\n        Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n        Mode == \"PB\" ~ \"Publico\",\n        Mode == \"RB\" ~ \"Bus Rapid Transit\",\n        Mode == \"SR\" ~ \"Streetcar Rail\",\n        Mode == \"TB\" ~ \"Trolleybus\",\n        Mode == \"TR\" ~ \"Aerial Tramways\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"YR\" ~ \"Hybrid Rail\",\n        TRUE ~ \"Unknown\"))\n\n\n\n\nCode\n# download the 2023 service by agency report\nlibrary(readr)\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n\n\n\nCode\n#clean up the service data\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n    mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n    rename(Agency = agency, \n           City   = max_city, \n           State  = max_state,\n           UPT    = sum_unlinked_passenger_trips_upt, \n           MILES  = sum_passenger_miles) |&gt;\n    select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n    filter(MILES &gt; 0)"
  },
  {
    "objectID": "mp02.html#explore-ntd-service-data",
    "href": "mp02.html#explore-ntd-service-data",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "4 Explore NTD Service Data",
    "text": "4 Explore NTD Service Data\n\n\nCode\n# Which transit service has the most UPT annually?\nlibrary(knitr)\n\nmost_upt &lt;- NTD_SERVICE |&gt;\n  mutate(UPT = comma(UPT)) |&gt;\n  slice_max(UPT, n = 1)\n\nkable(most_upt, caption = \"Transit Service with the Most UPT Annually\")\n\n\n\nTransit Service with the Most UPT Annually\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\n\n\n\n\nRockford Mass Transit District\nRockford\nIL\n994,754\n5035893\n50058\n\n\n\n\n\n\n\nCode\n# What is the average trip length of a trip on MTA NYC?\nmta_avg_trip &lt;- NTD_SERVICE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  mutate(`MTA Average Trip Length` = MILES / UPT) |&gt;\n  select(`NTD ID`, Agency, City, State, `MTA Average Trip Length`)\n\nkable(mta_avg_trip, caption = \"Average Trip Length on MTA NYC\")\n\n\n\nAverage Trip Length on MTA NYC\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency\nCity\nState\nMTA Average Trip Length\n\n\n\n\n20008\nMTA New York City Transit\nBrooklyn\nNY\n3.644089\n\n\n\n\n\n\n\nCode\n# Which transit service in NYC has the longest average trip length?\nnyc_longest_trip &lt;- NTD_SERVICE |&gt;\n  filter(State == \"NY\", City == \"New York\" | City == \"Brooklyn\") |&gt;\n  mutate(`Average Trip Length` = MILES / UPT) |&gt;\n  slice_max(`Average Trip Length`, n = 1)\n\nkable(nyc_longest_trip, caption = \"Transit Service in NYC with the Longest Average Trip Length\")\n\n\n\nTransit Service in NYC with the Longest Average Trip Length\n\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\nAverage Trip Length\n\n\n\n\nMTA Long Island Rail Road\nNew York\nNY\n83835706\n2033685836\n20100\n24.25799\n\n\n\n\n\n\n\nCode\n# Which state has the fewest total miles travelled by public transit?\nstate_fewest_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(`Total Miles` = comma(sum(MILES, na.rm = TRUE))) |&gt;\n  slice_min(`Total Miles`, n = 1)\n\nkable(state_fewest_miles, caption = \"State with the Fewest Total Miles Travelled by Public Transit\")\n\n\n\nState with the Fewest Total Miles Travelled by Public Transit\n\n\nState\nTotal Miles\n\n\n\n\nWA\n1,059,910,614\n\n\n\n\n\n\n\nCode\n# Are all states represented in this data? If no, which ones are missing? The state.name and state.abb objects we used above may be useful here.\nstates_missing &lt;- EIA_SEP_REPORT |&gt;\n  anti_join(NTD_SERVICE, join_by(\"abbreviation\" == \"State\")) |&gt;\n  rename(`Missing States` = state) |&gt;\n  select(`Missing States`)\n\nkable(states_missing, caption = \"Missing States\")\n\n\n\nMissing States\n\n\nMissing States\n\n\n\n\nArizona\n\n\nArkansas\n\n\nCalifornia\n\n\nColorado\n\n\nHawaii\n\n\nIowa\n\n\nKansas\n\n\nLouisiana\n\n\nMissouri\n\n\nMontana\n\n\nNebraska\n\n\nNevada\n\n\nNew Mexico\n\n\nNorth Dakota\n\n\nOklahoma\n\n\nSouth Dakota\n\n\nTexas\n\n\nUtah\n\n\nWyoming"
  },
  {
    "objectID": "mp02.html#analysis",
    "href": "mp02.html#analysis",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "5 Analysis",
    "text": "5 Analysis\nWe’re now ready to start putting these datasets together and using them to identify America’s greenest transit agencies.\n\n\nCode\n# Task 5: join the three tables\nagency_mode_pair &lt;- NTD_SERVICE |&gt;\n  inner_join(NTD_ENERGY |&gt;\n      select(-c(`Kerosene`, `Bunker Fuel`, `Ethanol`, `Methonal`)), #clean it up \n    join_by(\"NTD ID\" == \"NTD ID\")\n    ) |&gt; \n  inner_join(EIA_SEP_REPORT,\n    join_by(State == \"abbreviation\")\n    ) |&gt;\n  select(-c(`Agency Name`, State)) |&gt;\n  rename(\n    agency = `Agency`,\n    city = `City`,\n    mode = `Mode`,\n    upt = `UPT`,\n    miles = `MILES`,\n    ntd_id = `NTD ID`,\n    biodiesel = `Bio-Diesel`,\n    cnaturalgas = `C Natural Gas`,\n    diesel = `Diesel Fuel`,\n    gasoline = `Gasoline`,\n    liqnatgas = `Liquified Nat Gas`,\n    liqpetgas =`Liquified Petroleum Gas`,\n    electric_battery = `Electric Battery`,\n    electric_propulsion = `Electric Propulsion`,\n    co2_mwh = CO2_MWh\n    ) |&gt;\n  mutate(\n    electricbattery_emission = electric_battery * (co2_mwh / 1000),\n    electricprop_emission = electric_propulsion * (co2_mwh / 1000),\n    total_emission = (\n        electricbattery_emission +\n        electricprop_emission\n      )\n  ) |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(agency_total_emission = sum(total_emission)) |&gt;\n  ungroup()\n\n# Task 6\n# use percentiles to define small, medium, and large agencies. \npercentile_1 &lt;- quantile(agency_mode_pair |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.3)\npercentile_2 &lt;- quantile(agency_mode_pair |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.7)\n\n# updating agency_mode_pair with agency size\n# adding green level\nagency_mode_pair &lt;-\n  agency_mode_pair |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(\n    agency_emission_per_capita = sum(total_emission) / upt,\n    emission_per_transit = sum(total_emission) / miles,\n    green_level = agency_emission_per_capita * emission_per_transit,\n    size = case_when(\n      upt &lt; percentile_1 ~ \"Small\",\n      upt &gt;= percentile_1 & upt &lt;= percentile_2 ~ \"Medium\",\n      upt &gt; percentile_2 ~ \"Large\",\n    )\n    ) |&gt;\n  ungroup()\n\n\nBesed on our analysis, the Greenest Transit Agency award goes to King County Metro for large size, it goes to Everett Transist for medium size, and it goes to RiverCities Transit for small size.\n\n\nCode\n# grouping th data\ngreen_levels &lt;-\n  agency_mode_pair |&gt;\n  group_by(size, ntd_id) |&gt;\n  summarize(\n    agency = first(agency),\n    city = first(city),\n    state = first(state),\n    green_level = first(green_level),\n    .groups = 'drop'\n  ) |&gt;\n  ungroup()\n\n# Select the greenest agency in each size\ngreenest_agency &lt;-\n  green_levels |&gt;\n  group_by(size) |&gt;\n  slice(1) |&gt;\n  select(agency, city, state)\n\n# Compute median green\nmedian_green &lt;- green_levels |&gt;\n  group_by(size) |&gt;\n  summarise(median_green = median(green_level, na.rm = TRUE))\n\n# visualize greenest agenciies\ngreenest_agency |&gt;\n  left_join(median_green, join_by(\"size\" == \"size\")) |&gt;\n  select(agency, city, state) |&gt;\n  kable(caption = \"Greenest Transit Agency\")\n\n\n\nGreenest Transit Agency\n\n\n\n\n\n\n\n\nsize\nagency\ncity\nstate\n\n\n\n\nLarge\nKing County, dba: King County Metro\nSeattle\nWashington\n\n\nMedium\nCity of Everett, dba: Everett Transit\nEverett\nWashington\n\n\nSmall\nCity of Longview, dba: RiverCities Transit\nLongview\nWashington"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "",
    "text": "Green Transit Alliance for Investigation of Variance (GTA IV) gave a series of awards for the greenest public transit angencies. The winners of the various GTA IV awards went to King County Metro, Everett Transit and RiverCities Transit. We will explore US Public Transit systems to assess their environmental efficiency and will use a variety of data sources to i) determine how many riders are served by different transit systems; ii) determine how far each public transit system transports an average rider; and iii) investigate the effective emissions associated with each form of transit."
  },
  {
    "objectID": "mp02_v1.html",
    "href": "mp02_v1.html",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "",
    "text": "Green Transit Alliance for Investigation of Variance (GTA IV) gave a series of awards for the greenest public transit angencies. The winners of the various GTA IV awards went to King County Metro, Everett Transit and RiverCities Transit. We will explore US Public Transit systems to assess their environmental efficiency and will use a variety of data sources to i) determine how many riders are served by different transit systems; ii) determine how far each public transit system transports an average rider; and iii) investigate the effective emissions associated with each form of transit."
  },
  {
    "objectID": "mp02_v1.html#introduction",
    "href": "mp02_v1.html#introduction",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "",
    "text": "Green Transit Alliance for Investigation of Variance (GTA IV) gave a series of awards for the greenest public transit angencies. The winners of the various GTA IV awards went to King County Metro, Everett Transit and RiverCities Transit. We will explore US Public Transit systems to assess their environmental efficiency and will use a variety of data sources to i) determine how many riders are served by different transit systems; ii) determine how far each public transit system transports an average rider; and iii) investigate the effective emissions associated with each form of transit."
  },
  {
    "objectID": "mp02_v1.html#data-import",
    "href": "mp02_v1.html#data-import",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "2 Data Import",
    "text": "2 Data Import\nWe download the EIA State Electricity Profiles, then use it to estimate the environmental impact of the electricity used to run certain transit systems.\n\n\nCode\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(stringr)\nensure_package(dplyr)\n\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n               # * 1000 kWh to MWH \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\n\nWe use the following code to create a table.\n\n\nCode\nensure_package(scales)\n\nEIA_SEP_REPORT |&gt; \n    select(-abbreviation) |&gt;\n    arrange(desc(CO2_MWh)) |&gt;\n    mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n           electricity_price_MWh = dollar(electricity_price_MWh), \n           generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n    rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n           `Primary Source of Electricity Generation`=primary_source, \n           `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n           `Total Generation Capacity (MWh)`= generation_MWh, \n           State=state) |&gt;\n    datatable()"
  },
  {
    "objectID": "mp02_v1.html#initial-analysis-of-sep-data",
    "href": "mp02_v1.html#initial-analysis-of-sep-data",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "3 Initial Analysis of SEP Data",
    "text": "3 Initial Analysis of SEP Data\nHere are some quick facts gained using the EIA_SEP_REPORT data.\n\nHawaii has the most expensive retail electricity with a price of $386 per MWh.\n\n\n\nCode\n# Which state has the most expensive retail electricity?\nstate_highest_price &lt;- EIA_SEP_REPORT |&gt;\n  select(electricity_price_MWh, state) |&gt;\n  slice_max(electricity_price_MWh, n=1)\n\n\n\nWest Virginia, whose primary source of electricity generation is coal, has the ‘dirtiest’ electricity mix with 1925 pounds of CO2 emitted per MWh.\n\n\n\nCode\n# Which state has the 'dirtiest' electricity mix?\nstate_dirtiest_electricity &lt;- EIA_SEP_REPORT |&gt;\n  select(CO2_MWh, primary_source, state) |&gt;\n  slice_max(CO2_MWh, n=1)\n\n\n\nOn average, 805.37 pounds of CO2 are emitted per MWh of electricity produced in the US.\n\n\n\nCode\n# On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US? (Note that you will need to use a suitably weighted average here.)\naverage_co2_emitted &lt;- EIA_SEP_REPORT |&gt;\n  summarize(\n    average_co2 = sum(CO2_MWh * generation_MWh) / sum(generation_MWh)\n  )\n\n\n\nPetroleum is the rarest primary energy source in the US used only in one state. The associated cost of electricity is $386 per MWh and it is used in Hawaii.\n\n\n\nCode\n# What is the rarest primary energy source in the US? \nrarest_source &lt;- EIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(num_state_source = n()) |&gt;\n  slice_min(num_state_source, n = 1)\n\n# What is the associated cost of electricity and where is it used?\nrarest_source_table &lt;- EIA_SEP_REPORT |&gt;\n  select(primary_source, electricity_price_MWh, state) |&gt;\n  filter(primary_source == \"Petroleum\")\n\n\n\nNY’s energy mix is 1.64 times cleaner than that of Texas.\n\n\n\nCode\n# How many times cleaner is NY’s energy mix than that of Texas?\nco2_emitted_NY &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"New York\") |&gt;\n  summarize(CO2_MWh)\n\nco2_emitted_Texas &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"Texas\") |&gt;\n  summarize(CO2_MWh)\n\ntimes_cleaner &lt;- co2_emitted_Texas / co2_emitted_NY\n\n\nWe now download the 2023 Annual Database Energy Consumption report and do some clean up. After that, we recode the Mode column.\n\n\nCode\n# import the data: 2023 Annual Database Energy Consumption\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n    DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                  destfile=NTD_ENERGY_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\n\n\n\nCode\n# do some basic clean-up\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n    replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(across(-c(`Agency Name`, \n                     `Mode`,\n                     `TOS`), \n                  to_numeric_fill_0)) |&gt;\n    group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n    summarize(across(where(is.numeric), sum), \n              .groups = \"keep\") |&gt;\n    mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n    filter(ENERGY &gt; 0) |&gt;\n    select(-ENERGY) |&gt;\n    ungroup()\n\n\n\n\nCode\n# clean up mode column in ntd energy\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"AR\" ~ \"Alaska Railroad\",\n        Mode == \"CB\" ~ \"Commuter Bus\",\n        Mode == \"CC\" ~ \"Cable Car\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"IP\" ~ \"Inclined Plane\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"MB\" ~ \"Bus\",\n        Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n        Mode == \"PB\" ~ \"Publico\",\n        Mode == \"RB\" ~ \"Bus Rapid Transit\",\n        Mode == \"SR\" ~ \"Streetcar Rail\",\n        Mode == \"TB\" ~ \"Trolleybus\",\n        Mode == \"TR\" ~ \"Aerial Tramways\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"YR\" ~ \"Hybrid Rail\",\n        TRUE ~ \"Unknown\"))\n\n\nNext, we download the 2023 Service by Agency report, from which we will extract characteristics of typical passenger trips on each transit service. We will also explore the data by answering some questions using the service data.\n\n\nCode\n# download the 2023 service by agency report\nlibrary(readr)\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n\n\n\nCode\n#clean up the service data\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n    mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n    rename(Agency = agency, \n           City   = max_city, \n           State  = max_state,\n           UPT    = sum_unlinked_passenger_trips_upt, \n           MILES  = sum_passenger_miles) |&gt;\n    select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n    filter(MILES &gt; 0)"
  },
  {
    "objectID": "mp02_v1.html#explore-ntd-service-data",
    "href": "mp02_v1.html#explore-ntd-service-data",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "4 Explore NTD Service Data",
    "text": "4 Explore NTD Service Data\n\n\nCode\n# Which transit service has the most UPT annually?\nlibrary(knitr)\n\nmost_upt &lt;- NTD_SERVICE |&gt;\n  mutate(UPT = comma(UPT)) |&gt;\n  slice_max(UPT, n = 1)\n\nkable(most_upt, caption = \"Transit Service with the Most UPT Annually\")\n\n\n\nTransit Service with the Most UPT Annually\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\n\n\n\n\nRockford Mass Transit District\nRockford\nIL\n994,754\n5035893\n50058\n\n\n\n\n\n\n\nCode\n# What is the average trip length of a trip on MTA NYC?\nmta_avg_trip &lt;- NTD_SERVICE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  mutate(`MTA Average Trip Length` = MILES / UPT) |&gt;\n  select(`NTD ID`, Agency, City, State, `MTA Average Trip Length`)\n\nkable(mta_avg_trip, caption = \"Average Trip Length on MTA NYC\")\n\n\n\nAverage Trip Length on MTA NYC\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency\nCity\nState\nMTA Average Trip Length\n\n\n\n\n20008\nMTA New York City Transit\nBrooklyn\nNY\n3.644089\n\n\n\n\n\n\n\nCode\n# Which transit service in NYC has the longest average trip length?\nnyc_longest_trip &lt;- NTD_SERVICE |&gt;\n  filter(State == \"NY\", City == \"New York\" | City == \"Brooklyn\") |&gt;\n  mutate(`Average Trip Length` = MILES / UPT) |&gt;\n  slice_max(`Average Trip Length`, n = 1)\n\nkable(nyc_longest_trip, caption = \"Transit Service in NYC with the Longest Average Trip Length\")\n\n\n\nTransit Service in NYC with the Longest Average Trip Length\n\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\nAverage Trip Length\n\n\n\n\nMTA Long Island Rail Road\nNew York\nNY\n83835706\n2033685836\n20100\n24.25799\n\n\n\n\n\n\n\nCode\n# Which state has the fewest total miles travelled by public transit?\nstate_fewest_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(`Total Miles` = comma(sum(MILES, na.rm = TRUE))) |&gt;\n  slice_min(`Total Miles`, n = 1)\n\nkable(state_fewest_miles, caption = \"State with the Fewest Total Miles Travelled by Public Transit\")\n\n\n\nState with the Fewest Total Miles Travelled by Public Transit\n\n\nState\nTotal Miles\n\n\n\n\nWA\n1,059,910,614\n\n\n\n\n\n\n\nCode\n# Are all states represented in this data? If no, which ones are missing? The state.name and state.abb objects we used above may be useful here.\nstates_missing &lt;- EIA_SEP_REPORT |&gt;\n  anti_join(NTD_SERVICE, join_by(\"abbreviation\" == \"State\")) |&gt;\n  rename(`Missing States` = state) |&gt;\n  select(`Missing States`)\n\nkable(states_missing, caption = \"Missing States\")\n\n\n\nMissing States\n\n\nMissing States\n\n\n\n\nArizona\n\n\nArkansas\n\n\nCalifornia\n\n\nColorado\n\n\nHawaii\n\n\nIowa\n\n\nKansas\n\n\nLouisiana\n\n\nMissouri\n\n\nMontana\n\n\nNebraska\n\n\nNevada\n\n\nNew Mexico\n\n\nNorth Dakota\n\n\nOklahoma\n\n\nSouth Dakota\n\n\nTexas\n\n\nUtah\n\n\nWyoming"
  },
  {
    "objectID": "mp02_v1.html#analysis",
    "href": "mp02_v1.html#analysis",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "5 Analysis",
    "text": "5 Analysis\nWe’re now ready to start putting these datasets together and using them to identify America’s greenest transit agencies.\n\n\nCode\n# Task 5: join the three tables\nagency_mode_pair &lt;- NTD_SERVICE |&gt;\n  inner_join(NTD_ENERGY |&gt;\n      select(-c(`Kerosene`, `Bunker Fuel`, `Ethanol`, `Methonal`)), #clean it up \n    join_by(\"NTD ID\" == \"NTD ID\")\n    ) |&gt; \n  inner_join(EIA_SEP_REPORT,\n    join_by(State == \"abbreviation\")\n    ) |&gt;\n  select(-c(`Agency Name`, State)) |&gt;\n  rename(\n    agency = `Agency`,\n    city = `City`,\n    mode = `Mode`,\n    upt = `UPT`,\n    miles = `MILES`,\n    ntd_id = `NTD ID`,\n    biodiesel = `Bio-Diesel`,\n    cnaturalgas = `C Natural Gas`,\n    diesel = `Diesel Fuel`,\n    gasoline = `Gasoline`,\n    liqnatgas = `Liquified Nat Gas`,\n    liqpetgas =`Liquified Petroleum Gas`,\n    electric_battery = `Electric Battery`,\n    electric_propulsion = `Electric Propulsion`,\n    co2_mwh = CO2_MWh\n    ) |&gt;\n  mutate(\n    electricbattery_emission = electric_battery * (co2_mwh / 1000),\n    electricprop_emission = electric_propulsion * (co2_mwh / 1000),\n    total_emission = (\n        electricbattery_emission +\n        electricprop_emission\n      )\n  ) |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(agency_total_emission = sum(total_emission)) |&gt;\n  ungroup()\n\n# Task 6\n# use percentiles to define small, medium, and large agencies. \npercentile_1 &lt;- quantile(agency_mode_pair |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.3)\npercentile_2 &lt;- quantile(agency_mode_pair |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.7)\n\n# updating agency_mode_pair with agency size\n# adding green level\nagency_mode_pair &lt;-\n  agency_mode_pair |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(\n    agency_emission_per_capita = sum(total_emission) / upt,\n    emission_per_transit = sum(total_emission) / miles,\n    green_level = agency_emission_per_capita * emission_per_transit,\n    size = case_when(\n      upt &lt; percentile_1 ~ \"Small\",\n      upt &gt;= percentile_1 & upt &lt;= percentile_2 ~ \"Medium\",\n      upt &gt; percentile_2 ~ \"Large\",\n    )\n    ) |&gt;\n  ungroup()\n\n\nBesed on our analysis, the Greenest Transit Agency award goes to King County Metro for large size, it goes to Everett Transist for medium size, and it goes to RiverCities Transit for small size.\n\n\nCode\n# grouping th data\ngreen_levels &lt;-\n  agency_mode_pair |&gt;\n  group_by(size, ntd_id) |&gt;\n  summarize(\n    agency = first(agency),\n    city = first(city),\n    state = first(state),\n    green_level = first(green_level),\n    .groups = 'drop'\n  ) |&gt;\n  ungroup()\n\n# Select the greenest agency in each size\ngreenest_agency &lt;-\n  green_levels |&gt;\n  group_by(size) |&gt;\n  slice(1) |&gt;\n  select(agency, city, state)\n\n# Compute median green\nmedian_green &lt;- green_levels |&gt;\n  group_by(size) |&gt;\n  summarise(median_green = median(green_level, na.rm = TRUE))\n\n# visualize greenest agenciies\ngreenest_agency |&gt;\n  left_join(median_green, join_by(\"size\" == \"size\")) |&gt;\n  select(agency, city, state) |&gt;\n  kable(caption = \"Greenest Transit Agency\")\n\n\n\nGreenest Transit Agency\n\n\n\n\n\n\n\n\nsize\nagency\ncity\nstate\n\n\n\n\nLarge\nKing County, dba: King County Metro\nSeattle\nWashington\n\n\nMedium\nCity of Everett, dba: Everett Transit\nEverett\nWashington\n\n\nSmall\nCity of Longview, dba: RiverCities Transit\nLongview\nWashington"
  },
  {
    "objectID": "mp02_v1.html#conclusion",
    "href": "mp02_v1.html#conclusion",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThis press release announces the winners in each category. The greenest transit for large size agencies is King County Metro, in Seattle, Washington. For medium size agencies, Everett Transit in Everett, Washington is the greenest. The winner for small size agencies goes to RiverCities Transit from Longview, Washington."
  },
  {
    "objectID": "mp02.html#greenest-transit-awards",
    "href": "mp02.html#greenest-transit-awards",
    "title": "2025 ‘Greenest Transit’ Awards Go to…",
    "section": "",
    "text": "This year’s Greenest Transit results just came out. The greeest transit agency awards have three categories: Large, Medium and Small, in terms of the sizes of agencies. Now, Let’s announce who the winners are! The greenest agencies are respectively MTA NYC Transit, Birmingham-Jefferson County Transit Authority, and Albany Transit System. The agencies with most emissions avoided are MTA New York City Transit, Hudson Transit Lines, and Hampton Jitney. The agencies with the highest electirfication level are TriMet, University of Georgia, and Connecticut Department of Transportation. We also have “winners” for the worst performance in terms of green and they are Washington State Ferries, SeaStreak, and Alaska Railroad Corporation. The relevent metrics with their values can be found in the appendix. To better demonstrate the ‘green-ness’ of the award winners, we have created some visualizations."
  },
  {
    "objectID": "mp02.html#appendix",
    "href": "mp02.html#appendix",
    "title": "2025 ‘Greenest Transit’ Awards Go to…",
    "section": "2 Appendix",
    "text": "2 Appendix\n\n\nCode\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(dplyr)\nensure_package(stringr)\nensure_package(tidyr)\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\n\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n               # * 1000 kWh to MWH \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\nensure_package(scales)\nensure_package(DT)\n\nlibrary(knitr)\n# Which state has the most expensive retail electricity?\nstate_most_expensive &lt;- EIA_SEP_REPORT |&gt;\n  slice_max(electricity_price_MWh, n = 1) |&gt;\n  mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n         electricity_price_MWh = dollar(electricity_price_MWh), \n         generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n  rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n         `Primary Source of Electricity Generation`=primary_source, \n         `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n         `Total Generation Capacity (MWh)`= generation_MWh, \n         State=state)\nkable(state_most_expensive, caption = \"State with the Most Expensive Retail Electricity\")\n\n\n\nState with the Most Expensive Retail Electricity\n\n\n\n\n\n\n\n\n\n\nPounds of CO2 Emitted per MWh of Electricity Produced\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nTotal Generation Capacity (MWh)\nState\nabbreviation\n\n\n\n\n1,444\nPetroleum\n$386\n9,194,164\nHawaii\nHI\n\n\n\n\n\nCode\n# Which state has the ‘dirtiest’ electricity mix?\nstate_dirtiest &lt;- EIA_SEP_REPORT |&gt;\n  slice_max(CO2_MWh, n = 1) |&gt;\n  mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n         electricity_price_MWh = dollar(electricity_price_MWh), \n         generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n  rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n         `Primary Source of Electricity Generation`=primary_source, \n         `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n         `Total Generation Capacity (MWh)`= generation_MWh, \n         State=state)\nkable(state_dirtiest, caption = \"State with the 'Dirtiest' Electricity Mix\")\n\n\n\nState with the ‘Dirtiest’ Electricity Mix\n\n\n\n\n\n\n\n\n\n\nPounds of CO2 Emitted per MWh of Electricity Produced\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nTotal Generation Capacity (MWh)\nState\nabbreviation\n\n\n\n\n1,925\nCoal\n$102.60\n52,286,784\nWest Virginia\nWV\n\n\n\n\n\nCode\n# On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US?\navg_co2 &lt;- EIA_SEP_REPORT |&gt;\n  summarize(avg_co2 = sum(CO2_MWh * generation_MWh) / sum(generation_MWh)) |&gt;\n  mutate(avg_co2= number(avg_co2, big.mark = \",\")) |&gt;\n  rename(`Average CO2 emissions per MWh`=avg_co2)\nkable(avg_co2, caption = \"Average CO2 Emissions per MWh in the US\")\n\n\n\nAverage CO2 Emissions per MWh in the US\n\n\nAverage CO2 emissions per MWh\n\n\n\n\n805\n\n\n\n\n\nCode\n# What is the rarest primary energy source in the US?\nrarest_source &lt;- EIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(Number = n()) |&gt;\n  slice_min(Number, n = 1) |&gt;\n  pull(primary_source)\n# What is the associated cost of electricity and where is it used?\nrarest &lt;- EIA_SEP_REPORT |&gt;\n  filter(primary_source==rarest_source) |&gt;\n  select(primary_source, electricity_price_MWh, state) |&gt;\n  mutate(electricity_price_MWh = dollar(electricity_price_MWh)) |&gt;\n  rename(`Primary Source of Electricity Generation`=primary_source, \n         `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n         State=state)\nkable(rarest, caption =\"Rarest Source and Its Cost and Location\")\n\n\n\nRarest Source and Its Cost and Location\n\n\n\n\n\n\n\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nState\n\n\n\n\nPetroleum\n$386\nHawaii\n\n\n\n\n\nCode\n# My home state, Texas, has a reputation as being the home of “dirty fossil fuels” \n# while NY has a reputation as a leader in clean energy. How many times cleaner is \n# NY’s energy mix than that of Texas?\nco2_ny &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"New York\") |&gt;\n  pull(CO2_MWh)\nco2_tx &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"Texas\") |&gt;\n  pull(CO2_MWh)\ntimes_cleaner &lt;- EIA_SEP_REPORT |&gt;\n  filter(state %in% c(\"New York\",\"Texas\")) |&gt;\n  mutate(times = CO2_MWh / co2_ny) |&gt;\n  select(state, primary_source, CO2_MWh, times) |&gt;\n  rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n         `Primary Source of Electricity Generation`=primary_source, \n         `New York is Cleaner than this State with Times`=times,\n         State=state)\nkable(times_cleaner,caption=\"New York is cleaner than Texas\")\n\n\n\nNew York is cleaner than Texas\n\n\n\n\n\n\n\n\nState\nPrimary Source of Electricity Generation\nPounds of CO2 Emitted per MWh of Electricity Produced\nNew York is Cleaner than this State with Times\n\n\n\n\nNew York\nNatural gas\n522\n1.000000\n\n\nTexas\nNatural gas\n855\n1.637931\n\n\n\n\n\nCode\n# 2023 ntd energy\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n  DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                      destfile=NTD_ENERGY_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\n# basic clean up\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n  x &lt;- if_else(x == \"-\", NA, x)\n  replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n  select(-c(`Reporter Type`, \n            `Reporting Module`, \n            `Other Fuel`, \n            `Other Fuel Description`)) |&gt;\n  mutate(across(-c(`Agency Name`, \n                   `Mode`,\n                   `TOS`), \n                to_numeric_fill_0)) |&gt;\n  group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n  summarize(across(where(is.numeric), sum), \n            .groups = \"keep\") |&gt;\n  mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n  filter(ENERGY &gt; 0) |&gt;\n  select(-ENERGY) |&gt;\n  ungroup()\n\n## This code needs to be modified\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n  mutate(Mode=case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\", \n    Mode == \"CB\" ~ \"Commuter Bus\", \n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\", \n    Mode == \"DR\" ~ \"Demand Response\", \n    Mode == \"FB\" ~ \"Ferryboat\", \n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"IP\" ~ \"Inclined Plane\", \n    Mode == \"LR\" ~ \"Light Rail\", \n    Mode == \"MB\" ~ \"Bus\", \n    Mode == \"MG\" ~ \"Monorail/Automated Guideway\", \n    Mode == \"PB\" ~ \"Publico\", \n    Mode == \"RB\" ~ \"Bus Rapid Transit\", \n    Mode == \"SR\" ~ \"Streetcar Rail\", \n    Mode == \"TB\" ~ \"Trolleybus\", \n    Mode == \"TR\" ~ \"Aerial Tramway\", \n    Mode == \"VP\" ~ \"Vanpool\", \n    Mode == \"YR\" ~ \"Hybrid Rail\", \n    TRUE ~ \"Unknown\"))\n\n# download the service by agency\nlibrary(readr)\n\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n  DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                      destfile=NTD_SERVICE_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n# clean it up\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n  mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n  rename(Agency = agency, \n         City   = max_city, \n         State  = max_state,\n         UPT    = sum_unlinked_passenger_trips_upt, \n         MILES  = sum_passenger_miles) |&gt;\n  select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n  filter(MILES &gt; 0)\n\n# Which transit service has the most UPT annually?\nmost_upt &lt;- NTD_SERVICE |&gt;\n  slice_max(UPT, n=1) |&gt;\n  mutate(UPT=number(UPT, big.mark=\",\"),\n         MILES=number(MILES,big.mark=\",\"))\nkable(most_upt, caption=\"Transit Service with the Most UPT Annually\")\n\n\n\nTransit Service with the Most UPT Annually\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\n\n\n\n\nMTA New York City Transit\nBrooklyn\nNY\n2,632,003,044\n9,591,253,658\n20008\n\n\n\n\n\nCode\n# What is the average trip length of a trip on MTA NYC?\navg_length &lt;- NTD_SERVICE |&gt;\n  filter(Agency==\"MTA New York City Transit\") |&gt;\n  mutate(avg_length=MILES/UPT, big.mark=\",\") |&gt;\n  select(`NTD ID`, Agency, avg_length) |&gt;\n  rename(`Average Length`=avg_length)\nkable(avg_length,caption=\"Average Length of a Trip on MTA NYC\")\n\n\n\nAverage Length of a Trip on MTA NYC\n\n\nNTD ID\nAgency\nAverage Length\n\n\n\n\n20008\nMTA New York City Transit\n3.644089\n\n\n\n\n\nCode\n# Which transit service in NYC has the longest average trip length?\nlongest_transit &lt;- NTD_SERVICE |&gt;\n  filter(City %in% c(\"Brooklyn\", \"New York\", \"Staten Island\")) |&gt;\n  mutate(avg_trip_length=MILES/UPT) |&gt;\n  slice_max(avg_trip_length, n=1) |&gt;\n  rename(`Average Trip Length`=avg_trip_length)\nkable(longest_transit, caption=\"Transit Service in NYC with the longest average trip length\")\n\n\n\nTransit Service in NYC with the longest average trip length\n\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\nAverage Trip Length\n\n\n\n\nMTA Long Island Rail Road\nNew York\nNY\n83835706\n2033685836\n20100\n24.25799\n\n\n\n\n\nCode\n# Which state has the fewest total miles travelled by public transit?\nstate_fewest_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(total_miles=sum(MILES)) |&gt;\n  slice_min(total_miles, n=1)\nkable(state_fewest_miles, caption=\"State with the Fewest Total Miles\")\n\n\n\nState with the Fewest Total Miles\n\n\nState\ntotal_miles\n\n\n\n\nNH\n3749892\n\n\n\n\n\nCode\n# Are all states represented in this data? If no, which ones are missing? The \n# state.name and state.abb objects we used above may be useful here.\nmissing_states &lt;- EIA_SEP_REPORT |&gt;\n  anti_join(NTD_SERVICE, join_by(\"abbreviation\"==\"State\")) |&gt;\n  select(state) |&gt;\n  rename(`Missing States`=state)\nkable(missing_states, caption=\"States that are not represented\")\n\n\n\nStates that are not represented\n\n\nMissing States\n\n\n\n\nArizona\n\n\nArkansas\n\n\nCalifornia\n\n\nColorado\n\n\nHawaii\n\n\nIowa\n\n\nKansas\n\n\nLouisiana\n\n\nMissouri\n\n\nMontana\n\n\nNebraska\n\n\nNevada\n\n\nNew Mexico\n\n\nNorth Dakota\n\n\nOklahoma\n\n\nSouth Dakota\n\n\nTexas\n\n\nUtah\n\n\nWyoming\n\n\n\n\n\nCode\n# join the three tables\nagency_mode &lt;- NTD_SERVICE |&gt;\n  inner_join(NTD_ENERGY |&gt;\n               select(-`Bunker Fuel`,-Ethanol, -Methonal,-Kerosene), \n             join_by(`NTD ID`==`NTD ID`)\n             ) |&gt;\n  inner_join(EIA_SEP_REPORT, join_by(State == abbreviation)) |&gt;\n  select(-`Agency Name`, -State) |&gt;\n  rename(\n    agency=Agency,\n    mode=Mode,\n    biodiesel=`Bio-Diesel`,\n    cnatgas=`C Natural Gas`,\n    diesel=`Diesel Fuel`,\n    ebat=`Electric Battery`,\n    epro=`Electric Propulsion`,\n    gas=Gasoline,\n    hy=Hydrogen,\n    lnatgas=`Liquified Nat Gas`,\n    lpetgas=`Liquified Petroleum Gas`,\n    co2_mwh=CO2_MWh\n  ) |&gt;\n  group_by(agency, mode)\n\n# co2_vol_mass &lt;- read_xlsx(\"data/mp02/co2_vol_mass.xlsx\")\n\nagency_mode &lt;- agency_mode |&gt; \n  mutate(total_co2_emissions=\n           biodiesel*22.45+\n           cnatgas*0.134/1000*120.85+\n           diesel*22.45+\n           ebat/1000*co2_mwh+\n           epro/1000*co2_mwh+\n           gas*20.86+\n           lnatgas*0.134/1000*120.85+\n           lpetgas*12.68\n         ) \n\nagency_mode2 &lt;- agency_mode |&gt;\n  select(-biodiesel,-cnatgas,-diesel,\n         -ebat, -epro, -gas,\n         -hy, -lnatgas, -lpetgas,\n         -co2_mwh, -primary_source, -electricity_price_MWh,\n         -generation_MWh\n         ) |&gt;\n  group_by(`NTD ID`) |&gt;\n  mutate(tot_emissions_agency=sum(total_co2_emissions))\n\nagency_mode3 &lt;- agency_mode2 |&gt;\n  select(`NTD ID`, agency, City, \n         state, UPT, MILES, \n         tot_emissions_agency) |&gt;\n  mutate(per_upt = tot_emissions_agency/UPT,\n         per_passenger_mile = per_upt/MILES) |&gt;\n  ungroup()\n\n# lowest emissions per upt\nlowest_emissions_upt &lt;- agency_mode3 |&gt;\n  select(`NTD ID`, agency, City,\n         state, per_upt) |&gt;\n  slice_min(per_upt, n=1)\nkable(lowest_emissions_upt, caption=\"Agency with the lowest co2 emissions per upt\")\n\n\n\nAgency with the lowest co2 emissions per upt\n\n\n\n\n\n\n\n\n\nNTD ID\nagency\nCity\nstate\nper_upt\n\n\n\n\n23\nCity of Seattle, dba: Seattle Center Monorail\nSeattle\nWashington\n0.0691556\n\n\n\n\n\nCode\n# lowest emissions per passenger mile\nlowest_emi_upt_mile &lt;- agency_mode3 |&gt;\n  select(`NTD ID`, agency, City,\n         state, per_passenger_mile) |&gt;\n  mutate(per_pm_mg=per_passenger_mile*453.592*1000) |&gt; # CO2 emissions in miligrams\n  slice_min(per_pm_mg, n=1, with_ties = FALSE)\nkable(lowest_emi_upt_mile, caption=\"Agency with the lowest emissions(miligram) per passenger mile\")\n\n\n\nAgency with the lowest emissions(miligram) per passenger mile\n\n\n\n\n\n\n\n\n\n\nNTD ID\nagency\nCity\nstate\nper_passenger_mile\nper_pm_mg\n\n\n\n\n20008\nMTA New York City Transit\nBrooklyn\nNew York\n0\n2.8e-05\n\n\n\n\n\nCode\n# finding the values that will define small, medium, and large sized agencies. \nsize_1 &lt;- quantile(agency_mode3 |&gt; select(UPT) |&gt; unique() |&gt; pull(UPT), 0.1)\nsize_2 &lt;- quantile(agency_mode3 |&gt; select(UPT) |&gt; unique() |&gt; pull(UPT), 0.4)\nsize_3 &lt;- quantile(agency_mode3 |&gt; select(UPT) |&gt; unique() |&gt; pull(UPT), 0.7)\n\nagency_mode4 &lt;- agency_mode3 |&gt;\n  filter(UPT &gt; size_1) |&gt;\n  mutate(size = case_when(\n    UPT &lt;= size_2 ~ \"Small\",\n    UPT &gt; size_2 & UPT &lt;= size_3 ~ \"Medium\",\n    UPT &gt; size_3 ~ \"Large\")) |&gt;\n  mutate(per_pm_mg=per_passenger_mile*453.592*1000)\n\n# greenest  agencies with different sizes\ngreenest_agencies &lt;- agency_mode4 |&gt;\n  select(size, agency, City,state, per_pm_mg) |&gt;\n  unique() |&gt;\n  group_by(size) |&gt;\n  slice_min(per_pm_mg, n=1)\nkable(greenest_agencies,caption=\"The greenest agencies by size\")\n\n\n\nThe greenest agencies by size\n\n\n\n\n\n\n\n\n\nsize\nagency\nCity\nstate\nper_pm_mg\n\n\n\n\nLarge\nMTA New York City Transit\nBrooklyn\nNew York\n0.0000280\n\n\nMedium\nBirmingham-Jefferson County Transit Authority\nBirmingham\nAlabama\n0.0139570\n\n\nSmall\nCity of Albany , dba: Albany Transit System\nAlbany\nGeorgia\n0.0251236\n\n\n\n\n\nCode\n# most emissions avioded\nmost_emissions_avoided &lt;- agency_mode4 |&gt;\n  unique() |&gt;\n  mutate(emission_avoided=MILES/49*20.86-tot_emissions_agency) |&gt;\n  group_by(size) |&gt;\n  slice_max(emission_avoided,n=1) |&gt;\n  select(size, agency, City, state, emission_avoided)\nkable(most_emissions_avoided, caption=\"Agencies with the most emissions avioded\")\n\n\n\nAgencies with the most emissions avioded\n\n\n\n\n\n\n\n\n\nsize\nagency\nCity\nstate\nemission_avoided\n\n\n\n\nLarge\nMTA New York City Transit\nBrooklyn\nNew York\n2523693827\n\n\nMedium\nHudson Transit Lines, Inc., dba: Short Line\nMahwah\nNew Jersey\n17934431\n\n\nSmall\nHampton Jitney, Inc.\nCalverton\nNew York\n11184078\n\n\n\n\n\nCode\n# highest percentage of electrification\nhighest_electrification &lt;- agency_mode |&gt;\n  mutate(elec=ebat+epro, other=biodiesel+cnatgas+\n           diesel+gas+hy+lnatgas+lpetgas) |&gt;\n  group_by(agency) |&gt;\n  mutate(tot_elec=sum(elec), tot_other=sum(other)) |&gt;\n  unique() |&gt;\n  mutate(elec_level=tot_elec/tot_other)\n\nhighest_electrification_agency &lt;- highest_electrification |&gt;\n  inner_join(agency_mode4, join_by(\"NTD ID\"==\"NTD ID\")) |&gt;\n  group_by(size) |&gt;\n  filter(elec_level!=Inf) |&gt;\n  slice_max(elec_level,n=1)|&gt;\n  select(size,agency.x,City.x,state.x,elec_level) |&gt;\n  unique()\nkable(highest_electrification_agency, caption=\"Agencies with the highest electrification level\")\n\n\n\nAgencies with the highest electrification level\n\n\n\n\n\n\n\n\n\nsize\nagency.x\nCity.x\nstate.x\nelec_level\n\n\n\n\nLarge\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet\nPortland\nOregon\n117.43770\n\n\nMedium\nUniversity of Georgia, dba: Transportation and Parking Services\nAthens\nGeorgia\n10.36036\n\n\nSmall\nConnecticut Department of Transportation\nNewington\nConnecticut\n14.84685\n\n\n\n\n\nCode\n# worst of green agency\nworst_green_agency &lt;- highest_electrification |&gt;\n  inner_join(agency_mode4, join_by(\"NTD ID\"==\"NTD ID\")) |&gt;\n  group_by(size) |&gt;\n  filter(elec_level==0) |&gt;\n  slice_max(total_co2_emissions,n=1)|&gt;\n  select(size,agency.x,City.x,state.x,total_co2_emissions) |&gt;\n  unique()\nkable(worst_green_agency, caption=\"Agencies that are most 'un-green'\")\n\n\n\nAgencies that are most ‘un-green’\n\n\n\n\n\n\n\n\n\nsize\nagency.x\nCity.x\nstate.x\ntotal_co2_emissions\n\n\n\n\nLarge\nWashington State Ferries\nOlympia\nWashington\n344220193\n\n\nMedium\nSeaStreak, LLC\nAtlantic Highlands\nNew Jersey\n66376478\n\n\nSmall\nAlaska Railroad Corporation\nAnchorage\nAlaska\n21854963\n\n\n\n\n\nCode\n# visualization\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Example data \ngreenest_agencies &lt;- data.frame(\n  size = c('Small', 'Medium', 'Large'),\n  agency = c('Agency A', 'Agency B', 'Agency C'),\n  City = c('City X', 'City Y', 'City Z'),\n  state = c('State 1', 'State 2', 'State 3'),\n  per_pm_mg = c(0.025, 0.013, 0.000)\n)\n\n# Create the bar plot\nggplot(greenest_agencies, aes(x = size, y = per_pm_mg, fill = size)) +\n  geom_bar(stat = 'identity', show.legend = FALSE) +\n  geom_text(aes(label = round(per_pm_mg, 3)), vjust = -0.3, size = 5) +\n  scale_fill_brewer(palette = \"Greens\") +\n  labs(\n    title = 'Greenest Agencies by Size',\n    x = 'Agency Size',\n    y = 'Green Score (per pm mg)'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n#another vis\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n# Here is a sample structure\nmost_emissions_avoided &lt;- data.frame(\n  size = c('Small', 'Medium', 'Large'),\n  agency = c('Agency A', 'Agency B', 'Agency C'),\n  City = c('City X', 'City Y', 'City Z'),\n  state = c('State 1', 'State 2', 'State 3'),\n  emission_avoided = c(11000000, 17000000, 2523000000)\n)\n\n# Create a bar plot for the most emissions avoided by agency size\nggplot(most_emissions_avoided, aes(x = size, y = emission_avoided, fill = size)) +\n  geom_bar(stat = 'identity', show.legend = FALSE) +\n  geom_text(aes(label = round(emission_avoided, 0)), vjust = -0.3, size = 5) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(\n    title = 'Most Emissions Avoided by Agencies',\n    x = 'Agency Size',\n    y = 'Emissions Avoided (in units)'\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "My test doc",
    "section": "",
    "text": "this is italic and this is bold.\nI want link to Youtube."
  },
  {
    "objectID": "test.html#quarto",
    "href": "test.html#quarto",
    "title": "My test doc",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "test.html#running-code",
    "href": "test.html#running-code",
    "title": "My test doc",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "test.html#big-section",
    "href": "test.html#big-section",
    "title": "My test doc",
    "section": "Big Section",
    "text": "Big Section\ntext\n\nLittle Section\nlittle Text\n\\(\\frac{2}{3}\\) is more than \\(\\frac{1}{2}\\)\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "test_pre.html#quarto",
    "href": "test_pre.html#quarto",
    "title": "Test Presentation",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "test_pre.html#bullets",
    "href": "test_pre.html#bullets",
    "title": "Test Presentation",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "test_pre.html#code",
    "href": "test_pre.html#code",
    "title": "Test Presentation",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "test_pre.html#this-is-a-header",
    "href": "test_pre.html#this-is-a-header",
    "title": "Test Presentation",
    "section": "This is a header",
    "text": "This is a header\nsomething"
  },
  {
    "objectID": "test_pre.html#this-is-another-header",
    "href": "test_pre.html#this-is-another-header",
    "title": "Test Presentation",
    "section": "This is another header",
    "text": "This is another header"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "MP03",
    "section": "",
    "text": "This is the anchor song used to create the ultimate playlist"
  },
  {
    "objectID": "mp03.html#internets-best-playlist",
    "href": "mp03.html#internets-best-playlist",
    "title": "MP03",
    "section": "",
    "text": "This is the anchor song used to create the ultimate playlist"
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Creating the Ultimate Playlist",
    "section": "Introduction",
    "text": "Introduction\nIn this project, we will explore the world of music analytics aiming to create The Ultimate Playlist. In detail, we will use two data exports available from Spotify to identify the most popular songs on the platform, and also the characteristics of those songs. By using this data, we will create the ultimate playlist."
  },
  {
    "objectID": "mp03.html#conclusion",
    "href": "mp03.html#conclusion",
    "title": "Creating the Ultimate Playlist",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "mp03.html#data-acquisition",
    "href": "mp03.html#data-acquisition",
    "title": "Creating the Ultimate Playlist",
    "section": "Data Acquisition",
    "text": "Data Acquisition\nThe two Spotify data exports we will use are a data set of songs and their characteristics, and an export of user-created playlists.\n\nSong Characteristics\nFirst, we write a function called load_songs to download the Spotify song dataset from https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv and then read it into R.\n\n\nCode\n# Ensure packages are loaded\nlibrary(dplyr)\n\n# Task 1: Song Characteristics Dataset\nload_songs &lt;- function() {\n  # Define directory and file paths\n  dir_path &lt;- \"data/mp03\"\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name)\n  url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # Download the file if it doesn't exist\n  if (!file.exists(file_path)) {\n    download.file(url, destfile = file_path, method = \"auto\")\n  }\n  \n  # Read CSV into a data frame\n  songs_df &lt;- read.csv(file_path, stringsAsFactors = FALSE)\n  \n  # Return a well-formatted data frame\n  return(songs_df)\n}\n\n\nThe artists column of this data set is a bit oddly formatted: it contains multiple artists in a “list-type” format. We use the following code to split the artists across multiple rows.\n\n\nCode\n# Split the artists \nlibrary(tidyr)\nlibrary(stringr)\nclean_artist_string &lt;- function(x){\n  str_replace_all(x, \"\\\\['\", \"\") |&gt; \n    str_replace_all(\"'\\\\]\", \"\") \n}\nSONGS &lt;- load_songs()\nSONGS_CLEAN &lt;- SONGS |&gt; \n  separate_longer_delim(artists, \"', '\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\n\n\n\nPlaylists\nNext, we will download the Spotify Playlist dataset from spotify_million_playlist_dataset by writting a function called load_playlists to download all files from this repository (data1 directory). We store them locally, read them into R, and then concatenate them into a list object.\n\n\nCode\n# Task 2: Playlist dataset\n# Write a function to download playlists\nload_playlists &lt;- function(data_dir = \"spotify_data\", base_url = \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\", \n                           slices = seq(0, 999000, by = 1000)) {\n  if (!dir.exists(data_dir)) {\n    dir.create(data_dir)\n  }\n  \n  playlists &lt;- list()\n  \n  for (start in slices) {\n    end &lt;- start + 999\n    file_name &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n    file_path &lt;- file.path(data_dir, file_name)\n    file_url &lt;- paste0(base_url, file_name)\n    \n    # Check if file exists on the server\n    response &lt;- tryCatch({\n      httr::HEAD(file_url) # Send a HEAD request to check if the file exists\n    }, error = function(e) {\n      return(NULL)\n    })\n    \n    if (is.null(response) || response$status_code != 200) {\n      message(sprintf(\"File not found: %s\", file_name))\n      next  # Skip this file if it doesn't exist\n    }\n    \n    # Download the file if not already downloaded\n    if (!file.exists(file_path)) {\n      message(sprintf(\"Downloading %s...\", file_name))\n      download.file(file_url, destfile = file_path, mode = \"wb\")\n    } else {\n      message(sprintf(\"Already downloaded: %s\", file_name))\n    }\n    \n    # Read JSON file and extract playlists\n    json_data &lt;- jsonlite::fromJSON(file_path)\n    playlists[[file_name]] &lt;- json_data$playlists\n  }\n  \n  return(playlists)\n}\n\nall_playlists &lt;- load_playlists()\n\n\nThen, by using functions from the tidyr, purrr, and dplyr packages, we convert the playlist data into the rectangular format as a table of the following columns:\n\nPlaylist Name (playlist_name)\nPlaylist ID (playlist_id)\nPlaylist Position (playlist_position)\nPlaylist Followers (playlist_followers)\nArtist Name (artist_name)\nArtist ID (artist_id)\nTrack Name (track_name)\nTrack ID (track_id)\nAlbum Name (album_name)\nAlbum ID (album_id)\nDuration (duration)\n\nwhere each row is one “track” from a playlist. We also clean up the ID columns by striping the spotify:type: prefix.\n\n\nCode\n# Task 3: Rectangle the playlist data\nlibrary(tidyverse)\n\n# Flatten all playlists into a single data frame\ntidy_playlists &lt;- all_playlists |&gt;\n  # Combine all playlist chunks\n  bind_rows() |&gt;\n  # Use purrr::map_df to unnest the 'tracks' list-column\n  mutate(playlist_name = name,\n         playlist_id = pid,\n         playlist_position = row_number(),\n         playlist_followers = num_followers) |&gt;\n  select(playlist_name, playlist_id, playlist_position, playlist_followers, tracks) |&gt;\n  unnest(tracks, keep_empty = TRUE) |&gt;\n  transmute(\n    playlist_name,\n    playlist_id,\n    playlist_position,\n    playlist_followers,\n    artist_name = artist_name,\n    artist_id = artist_uri,\n    track_name = track_name,\n    track_id = track_uri,\n    album_name = album_name,\n    album_id = album_uri,\n    duration = duration_ms\n  )\n\n# strip the predix in id columns\nstrip_spotify_prefix &lt;- function(x){\n  library(stringr)\n  str_extract(x, \".*:.*:(.*)\", group=1)\n}\nPLAYLIST &lt;- tidy_playlists |&gt;\n  mutate(artist_id = strip_spotify_prefix(artist_id),\n         track_id = strip_spotify_prefix(track_id),\n         album_id = strip_spotify_prefix(album_id))"
  },
  {
    "objectID": "mp03.html#initial-exploration",
    "href": "mp03.html#initial-exploration",
    "title": "Creating the Ultimate Playlist",
    "section": "Initial Exploration",
    "text": "Initial Exploration\nSince our data is imported and cleaned, let’s do some initial exploration with the following questions.\n\nHow many distinct tracks and artists are represented in the playlist data?\n\n\n\nCode\n# Task 4: initial exploration\n# How many distinct tracks and artists are represented in the playlist data?\n\n# Count distinct tracks\ndistinct_tracks &lt;- PLAYLIST |&gt;\n  distinct(track_id) |&gt;\n  nrow()\n\n# Count distinct artists\ndistinct_artists &lt;- PLAYLIST |&gt;\n  distinct(artist_id) |&gt;\n  nrow()\n\n# Output results\ncat(\"Distinct Tracks:\", distinct_tracks, \"\\n\")\n\n\nDistinct Tracks: 1200590 \n\n\nCode\ncat(\"Distinct Artists:\", distinct_artists, \"\\n\")\n\n\nDistinct Artists: 173604 \n\n\n\nWhat are the 5 most popular tracks in the playlist data?\n\n\n\nCode\n# What are the 5 most popular tracks in the playlist data?\nmost_popular_tracks_5 &lt;- PLAYLIST |&gt; group_by(track_id, track_name) |&gt; \n  summarize(n_track = n(), .groups='drop') |&gt;\n  slice_max(n_track, n=5)\nlibrary(knitr)\nkable(most_popular_tracks_5, cap='5 most popular tracks in the playlist data')\n\n\n\n5 most popular tracks in the playlist data\n\n\ntrack_id\ntrack_name\nn_track\n\n\n\n\n7KXjTSCq5nL1LoYtL7XAwS\nHUMBLE.\n13314\n\n\n1xznGGDReH1oQq0xzbwXa3\nOne Dance\n12179\n\n\n7yyRTcZmCiyzzJlNzGC9Ol\nBroccoli (feat. Lil Yachty)\n11845\n\n\n7BKLCZ1jbUBVqRi2FVlTVw\nCloser\n11656\n\n\n3a1lNhkSLSkpJE4MSHpDu9\nCongratulations\n11310\n\n\n\n\n\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\n\nCode\n# What is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\nmost_popular_missing &lt;- PLAYLIST |&gt;\n  group_by(track_id, track_name) |&gt;\n  summarize(n_track = n(), .groups='drop') |&gt;\n  anti_join(SONGS_CLEAN, by=c('track_id'='id')) |&gt;\n  slice_max(n_track, n=1)\nkable(most_popular_missing, cap='Most popular track in the playlist data that does not have a corresponding entry in the song characteristics data')\n\n\n\nMost popular track in the playlist data that does not have a corresponding entry in the song characteristics data\n\n\ntrack_id\ntrack_name\nn_track\n\n\n\n\n1xznGGDReH1oQq0xzbwXa3\nOne Dance\n12179\n\n\n\n\n\n\nAccording to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\n\n\n\nCode\n# According to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\nname_danceable &lt;- SONGS_CLEAN |&gt; \n  slice_max(danceability, n=1) |&gt;\n  pull(name)\n\nn_id &lt;- PLAYLIST |&gt;\n  distinct(playlist_id) |&gt;\n  nrow()\n  \nmost_danceable_track &lt;- PLAYLIST |&gt;\n  filter(track_name == name_danceable) |&gt;\n  group_by(track_name) |&gt;\n  summarize(freqency = n() / n_id )\nkable(most_danceable_track, cap='Most danceable track and how often it appears in a playlist')\n\n\n\nMost danceable track and how often it appears in a playlist\n\n\ntrack_name\nfreqency\n\n\n\n\nFunky Cold Medina\n0.0007535\n\n\n\n\n\n\nWhich playlist has the longest average track length?\n\n\n\nCode\n# Which playlist has the longest average track length?\nplaylist_longest_track_length &lt;- PLAYLIST |&gt;\n  group_by(playlist_id, playlist_name) |&gt;\n  summarize(avg_track_len = mean(duration), .groups='drop') |&gt;\n  slice_max(avg_track_len, n=1)\nkable(playlist_longest_track_length, cap='Playlist with the longest average track length')\n\n\n\nPlaylist with the longest average track length\n\n\nplaylist_id\nplaylist_name\navg_track_len\n\n\n\n\n462471\nMixes\n3868511\n\n\n\n\n\n\nWhat is the most popular playlist on Spotify?\n\n\n\nCode\n# What is the most popular playlist on Spotify?\nmost_popular_playlist &lt;- PLAYLIST |&gt;\n  slice_max(playlist_followers, n=1) |&gt;\n  distinct(playlist_id, playlist_name)\nkable(most_popular_playlist, cap='Most popular playlist on Spotify')\n\n\n\nMost popular playlist on Spotify\n\n\nplaylist_id\nplaylist_name\n\n\n\n\n746359\nBreaking Bad"
  },
  {
    "objectID": "mp03.html#identifying-characteristics-of-popular-songs",
    "href": "mp03.html#identifying-characteristics-of-popular-songs",
    "title": "Creating the Ultimate Playlist",
    "section": "Identifying Characteristics of Popular Songs",
    "text": "Identifying Characteristics of Popular Songs\nNow, we will visually explore our data by answering the following questions using one or more visualizations.\n\nIs the popularity column correlated with the number of playlist appearances? If so, to what degree?\n\n\n\nCode\n# Task 5: visually identifying characteristics of pupular songs\n# inner join to combine datasets\n# Combine playlist and song characteristics using an inner join\ncombined_data &lt;- PLAYLIST |&gt;\n  inner_join(SONGS_CLEAN, by = c(\"track_id\" = \"id\"))\n# Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n\n# Count how many times each track appears in the playlist\nplaylist_counts &lt;- PLAYLIST |&gt;\n  group_by(track_id) |&gt;\n  summarize(playlist_appearances = n(), .groups = \"drop\")\n\n# Join with song characteristics to get popularity\npopularity_data &lt;- playlist_counts |&gt;\n  inner_join(SONGS_CLEAN, by = c(\"track_id\" = \"id\")) |&gt;\n  select(track_id, playlist_appearances, popularity) |&gt;\n  filter(playlist_appearances &lt; 5000)\n\n# Create scatter plot with linear regression line\nggplot(popularity_data, \n       aes(x = playlist_appearances, \n                            y = popularity)) +\n  geom_point(color = \"#0072B2\", alpha = 0.2, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#D55E00\", linetype = \"dashed\") +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Relationship Between Playlist Appearances and Track Popularity\",\n    x = \"Number of Playlist Appearances\",\n    y = \"Popularity Score\",\n    caption = \"Dashed line represents linear regression fit\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Compute Pearson correlation\ncorrelation &lt;- cor(popularity_data$playlist_appearances, popularity_data$popularity)\ncat(\"Correlation between playlist appearances and popularity:\", round(correlation, 3))\n\n\nCorrelation between playlist appearances and popularity: 0.448\n\n\nA positive correlation suggests that more frequently included tracks tend to be more popular. The correlation coefficient (Pearson’s r) tells us the strength is moderate.\n~0.1–0.3 = weak\n~0.3–0.5 = moderate\n~0.5–1.0 = strong\nFor the following questions, we select popularity = 60 as a threshold that defines a “popular” song.\n\nIn what year were the most popular songs released?\n\n\n\nCode\n# In what year were the most popular songs released?\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\nsongs_per_year &lt;- SONGS_CLEAN |&gt;\n  filter(popularity &gt;= 60) |&gt;\n  select(id, name, year) |&gt;\n  group_by(year) |&gt;\n  summarize(num_songs_released = n(), .groups='drop')\n\n# Create static ggplot\np &lt;- ggplot(songs_per_year, aes(x = factor(year), y = num_songs_released)) +\n  geom_bar(stat = \"identity\", fill = \"#56B4E9\", color = \"#0072B2\", alpha = 0.7) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Number of Popular Songs Released by Year\",\n    x = \"Release Year\",\n    y = \"Number of Popular Songs Released\",\n    caption = \"Bar plot showing the number of popular songs released each year\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  scale_x_discrete(breaks = function(x) x[as.numeric(x) %% 5 == 0])  # Show every 5th year\n\n# Convert ggplot to interactive plotly\nggplotly(p)\n\n\n\n\n\n\nCode\n## Find the year(s) with the highest number of popular song releases\nmost_songs_year &lt;- songs_per_year |&gt;\n  filter(num_songs_released == max(num_songs_released)) |&gt;\n  pull(year)\ncat(\"The year with the most popular songs released is:\", most_songs_year)\n\n\nThe year with the most popular songs released is: 2019\n\n\n\nIn what year did danceability peak?\n\n\n\nCode\n# In what year did danceability peak?\npopular_songs &lt;- SONGS_CLEAN |&gt;\n  filter(popularity &gt;= 60)\n\ndan_by_year &lt;- popular_songs |&gt;\n  group_by(year) |&gt;\n  summarize(avg_dan = mean(danceability), .groups='drop')\n\n# Line plot of average danceability by year\np_2 &lt;- ggplot(dan_by_year, aes(x = year, y = avg_dan)) +\n  geom_line(color = \"#009E73\", size = 1.3) +\n  geom_point(color = \"#009E73\", size = 2.5) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Average Danceability of Songs by Release Year\",\n    x = \"Release Year\",\n    y = \"Average Danceability\",\n    caption = \"Danceability score ranges from 0 (not danceable) to 1 (very danceable)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nggplotly(p_2)\n\n\n\n\n\n\nCode\n# Find year(s) with peak danceability\npeak_year &lt;- dan_by_year |&gt;\n  filter(avg_dan == max(avg_dan)) |&gt;\n  pull(year)\n\ncat(\"Danceability peaked in the year:\", peak_year)\n\n\nDanceability peaked in the year: 1955\n\n\n\nWhich decade is most represented on user playlists?\n\n\n\nCode\n# Which decade is most represented on user playlists? (The integer division (%/%) operator may be useful for computing decades from years.)\n# Add a decade column\ndecade_counts &lt;- combined_data |&gt;\n  mutate(decade = (year %/% 10) * 10) |&gt;\n  group_by(decade) |&gt; \n  summarise(song_count = n(), .groups = \"drop\")\n\nmost_common_decade &lt;- decade_counts |&gt;\n  filter(song_count == max(song_count)) |&gt;\n  pull(decade)\n\ncat(\"The most represented decade on user playlists is:\", most_common_decade)\n\n\nThe most represented decade on user playlists is: 2010\n\n\nCode\n# Add a column to control bar color\ndecade_counts &lt;- decade_counts |&gt;\n  mutate(\n    highlight = ifelse(decade == most_common_decade, \"Most Popular\", \"Other\")\n  )\n\np_3 &lt;- ggplot(decade_counts, aes(x = factor(decade), y = song_count/1000, fill = highlight)) +\n  geom_bar(stat = \"identity\", color = \"black\", width = 0.7) +\n  scale_fill_manual(values = c(\"Most Popular\" = \"#D55E00\", \"Other\" = \"#0072B2\")) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Number of Songs on Playlists by Decade\",\n    x = \"Decade\",\n    y = \"Number of Songs (thousand)\",\n    fill = \"\",\n    caption = \"The most popular decade is highlighted\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\"\n  )\nggplotly(p_3)\n\n\n\n\n\n\n\nCreate a plot of key frequency among songs.\n\n\n\nCode\n# Create a plot of key frequency among songs. Because musical keys exist in a ‘cycle’, your plot should use polar (circular) coordinates.\n# Map integer keys to note names\nkey_labels &lt;- c(\n  \"C\", \"C♯/D\", \"D\", \"D♯/E\", \"E\", \"F\",\n  \"F♯/G\", \"G\", \"G♯/A\", \"A\", \"A♯/B\", \"B\"\n)\n\n# Add a key name column\nkey_data &lt;- SONGS_CLEAN |&gt;\n  mutate(key_name = factor(key_labels[key + 1], levels = key_labels))  # +1 for 0-indexing\nkey_counts &lt;- key_data |&gt;\n  group_by(key_name) |&gt;\n  summarise(song_count = n(), .groups = \"drop\")\n\nggplot(key_counts, aes(x = key_name, y = song_count, fill = key_name)) +\n  geom_bar(stat = \"identity\", color = \"black\", width = 1) +\n  coord_polar(start = -pi/12) +  # rotate so C is at the top\n  scale_fill_manual(values = RColorBrewer::brewer.pal(12, \"Paired\")) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Distribution of Musical Keys Among Songs\",\n    x = \"\",\n    y = \"\",\n    caption = \"Keys are mapped in circular order (C to B)\"\n  ) +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\nWhat are the most popular track lengths?\n\n\n\nCode\n# What are the most popular track lengths? (Are short tracks, long tracks, or something in between most commonly included in user playlists?)\n# Convert milliseconds to minutes\ntrack_lengths &lt;- combined_data |&gt;\n  filter(!is.na(duration_ms)) |&gt;\n  mutate(duration_min = duration_ms / 60000) |&gt;\n  filter(duration_min &lt;= 10)\n\n# Get the most frequent duration range\nmode_duration &lt;- track_lengths |&gt;\n  mutate(duration_rounded = round(duration_min, 1)) |&gt;\n  count(duration_rounded) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1) |&gt;\n  pull(duration_rounded)\n\n# Add a line to highlight the peak\np_5 &lt;- ggplot(track_lengths, aes(x = duration_min)) +\n  geom_histogram(binwidth = 0.25, fill = \"#56B4E9\", color = \"black\", alpha = 0.8) +\n  geom_vline(xintercept = mode_duration, color = \"#D55E00\", linetype = \"dashed\", linewidth = 1.2) +\n  annotate(\"text\", x = mode_duration, y = 1560000, label = paste(\"Most Common:\", mode_duration, \"min\"), \n           vjust = -0.5, hjust = 0.1, color = \"#D55E00\", fontface = \"bold\", size = 5) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Distribution of Track Durations in Playlists\",\n    x = \"Track Length (minutes)\",\n    y = \"Number of Tracks\",\n    caption = \"Dashed line indicates the most common track length\"\n  )\nggplotly(p_5)\n\n\n\n\n\n\nWe will also pose and visually answer two more other exploratory questions here.\n\nWhat is the relationship between track popularity and track length?\nHow has the danceability of songs changed over time?\n\n\n\nCode\n# Plotting the relationship between track popularity and length\npopularity_length &lt;- SONGS_CLEAN |&gt;\n  mutate(duration_min = duration_ms / 60000) |&gt;\n  filter(duration_min &lt;= 10)\n\nggplot(popularity_length, aes(x = duration_min, y = popularity)) +\n  geom_point(alpha = 0.05, color = \"#56B4E9\") +\n  geom_smooth(method = \"lm\", color = \"#D55E00\", linetype = \"dashed\", size = 1) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Track Popularity vs. Track Length\",\n    x = \"Track Length (min)\",\n    y = \"Track Popularity\",\n    caption = \"Scatter plot with linear trendline\"\n  ) +\n  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\n# Compute Pearson correlation\ncorr_pop_len &lt;- cor(popularity_length$duration_min, popularity_length$popularity)\ncat(\"Correlation between track length and popularity:\", round(corr_pop_len, 3))\n\n\nCorrelation between track length and popularity: 0.119\n\n\nCode\n# How has the danceability of songs changed over time?\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gganimate)\n\n# Prepare the data: average danceability by year\ndanceability_trend &lt;- SONGS_CLEAN |&gt;\n  group_by(year) |&gt;\n  summarise(avg_danceability = mean(danceability), .groups = \"drop\") |&gt;\n  arrange(year)\n\n# Animated line chart\nggplot(danceability_trend, aes(x = year, y = avg_danceability)) +\n  geom_line(color = \"#0072B2\", linewidth = 1.5) +\n  geom_point(color = \"#D55E00\", size = 3) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Danceability of Songs Over Time\",\n    subtitle = \"Year: {frame_along}\",\n    x = \"Release Year\",\n    y = \"Average Danceability\"\n  ) +\n  transition_reveal(year)"
  },
  {
    "objectID": "mp03.html#building-a-playlist-from-anchor-songs",
    "href": "mp03.html#building-a-playlist-from-anchor-songs",
    "title": "Creating the Ultimate Playlist",
    "section": "Building a Playlist from Anchor Songs",
    "text": "Building a Playlist from Anchor Songs\nBefore building our playlist, we pick Adele’s “Someone Like You” as our “anchor” song. Let’s check it out here. \nWe will find songs that work well in a playlist with the anchor song using the following four heuristics and one of my own. We will identify 20 candidates, 8 of which are not “popular” based on the threshold we set above (popularity &lt;= 60).\n\nWhat other songs commonly appear on playlists along side this song?\nWhat other songs are in the same key and have a similar tempo? (This makes it easy for a skilled DJ to transition from one song to the next.)\nWhat other songs were released by the same artist?\nWhat other songs were released in the same year and have similar levels of acousticness, danceability, etc.?\nWhat other songs have similar mood (energy + valence)?\n\n\n\nCode\n# Task 6: finding related songs\n# Load libraries\nlibrary(dplyr)\nlibrary(readr)\n\n# Define anchor song ID\nanchor_id &lt;- \"4kflIGfjdZJW4ot2ioixTB\"\n\n# Get anchor song details\nanchor_song &lt;- SONGS_CLEAN |&gt; filter(id == anchor_id)\n\n# Define Similar numeric range filter\nrange_filter &lt;- function(df, col, val, tol) {\n  df |&gt; filter(abs(.data[[col]] - val) &lt;= tol)\n}\n\n# Heuristic 1: Co-occurrence in playlists\nco_songs &lt;- PLAYLIST |&gt;\n  filter(playlist_id %in% (\n    PLAYLIST |&gt;\n      filter(track_id == anchor_id) |&gt;\n      pull(playlist_id)\n  )) |&gt;\n  filter(track_id != anchor_id) |&gt;\n  count(track_id, sort = TRUE) |&gt;\n  top_n(10, wt = n) |&gt;\n  pull(track_id)\n\n# Heuristic 2: Same key & similar tempo (±5%)\ntempo_tol &lt;- 0.05 * anchor_song$tempo\nkey_tempo_match &lt;- SONGS_CLEAN |&gt;\n  filter(key == anchor_song$key) |&gt;\n  range_filter(\"tempo\", anchor_song$tempo, tempo_tol) |&gt;\n  pull(id)\n\n# Heuristic 3: Same artist\nsame_artist &lt;- SONGS_CLEAN |&gt;\n  filter(artist == anchor_song$artist, id != anchor_id) |&gt;\n  pull(id)\n\n# Heuristic 4: Same year & similar features\nsimilar_year &lt;- SONGS_CLEAN |&gt;\n  filter(year == anchor_song$year) |&gt;\n  range_filter(\"acousticness\", anchor_song$acousticness, 0.05) |&gt;\n  range_filter(\"danceability\", anchor_song$danceability, 0.05) |&gt;\n  range_filter(\"energy\", anchor_song$energy, 0.05) |&gt;\n  pull(id)\n\n# Heuristic 5: Similar mood (energy + valence)\nmood_match &lt;- SONGS_CLEAN |&gt;\n  range_filter(\"energy\", anchor_song$energy, 0.05) |&gt;\n  range_filter(\"valence\", anchor_song$valence, 0.05) |&gt;\n  pull(id)\n\n# Combine and Filter\nall_candidates &lt;- unique(c(co_songs, key_tempo_match, same_artist, similar_year, mood_match))\nall_candidates &lt;- setdiff(all_candidates, anchor_id)\n\n# Join with song data\ncandidate_songs &lt;- SONGS_CLEAN |&gt;\n  filter(id %in% all_candidates)\n\n# Ensure at least 20 candidates, 8 with popularity &lt; 60\nset.seed(123)\nunder_60 &lt;- candidate_songs |&gt; \n  filter(popularity &lt; 60) |&gt;\n  sample_n(8)\n\nnot_under_60 &lt;- candidate_songs |&gt;\n  filter(popularity &gt;= 60) |&gt;\n  sample_n(12)\n\nfinal_candidates &lt;- bind_rows(under_60, not_under_60)\n\n\nplaylist_pool &lt;- bind_rows(final_candidates, anchor_song)\n\nplaylist_21 &lt;- playlist_pool |&gt;\n  select(id, name, artist, popularity) |&gt;\n  arrange(desc(popularity))\nkable(playlist_21, cap='Potential songs that belong on a playlist with our anchor song')\n\n\n\nPotential songs that belong on a playlist with our anchor song\n\n\n\n\n\n\n\n\nid\nname\nartist\npopularity\n\n\n\n\n4kflIGfjdZJW4ot2ioixTB\nSomeone Like You\nAdele\n78\n\n\n6P7tTFzn6oNa0GL8w8oazE\nMake You Feel My Love\nAdele\n73\n\n\n7IWkJwX9C0J7tHurTD7ViL\nWhen We Were Young\nAdele\n72\n\n\n4j6GMcVcqZf1r0GDqMtYp6\nPrayed For You\nMatt Stell\n71\n\n\n7DUh5iszvXQDhhE0ZEtmUe\nEntra en Mi Vida\nSin Bandera\n67\n\n\n7cK7hDrE7vAesPf8xd5zmb\nStupid Deep\nJon Bellion\n67\n\n\n3DV49ruvI5Sl6iisPZAc2c\nHooked\nDylan Scott\n66\n\n\n3EATISg39vr81RXoosDtil\nSplit It\nMoneybagg Yo\n65\n\n\n0G2wimhVoDYXbQ6csDxtSf\nSex n’ Drugs\nHarrison Sands\n65\n\n\n5TgEJ62DOzBpGxZ7WRsrqb\nWork Song\nHozier\n64\n\n\n1U5pPF8f6APVCoc6t2KfIB\nCalm Down (Bittersweet) [feat. Summer Walker]\nSummer Walker\n64\n\n\n0ZPCxNK4DIrF8KqrN3DxfA\nNegative Energy\nTrippie Redd\n62\n\n\n40RcMOyjymXFKBxlMuQJnA\nWhat I’ve Been Looking For - From “High School Musical”/Soundtrack Version\nSharpay Evans\n60\n\n\n0GDQXt7qRJIDDUmcufomrU\nMove Together\nJames Bay\n49\n\n\n6gfAQs9TWAq91Ehp3eLFUX\nMake It In America\nVictorious Cast\n48\n\n\n5MNX5EUXOPzu17gqwSWnPU\nFair Play\nVan Morrison\n34\n\n\n3j4QPgiDGGipjfLgtikzrL\nThe Letter - Mono Single Version\nThe Box Tops\n31\n\n\n5xFed9QrbeHdj5okNUVZHs\nWPLJ\nThe Mothers Of Invention\n23\n\n\n5sGNawDBNNLOUAEkmplQtl\nMaureen\nEddie Money\n19\n\n\n3EyPEdXmMZT2uplSg2EZRp\nYou Can Depend on Me\nBarbara Mason\n19\n\n\n0L2Dspp7FzoJcQGU8ANaGL\nΧορός δωδεκανησιακός\nΤέτος Δημητριάδης\n0"
  },
  {
    "objectID": "mp03.html#create-your-playlist",
    "href": "mp03.html#create-your-playlist",
    "title": "Creating the Ultimate Playlist",
    "section": "Create Your Playlist",
    "text": "Create Your Playlist\nAbove is a list of our anchor song and playlist candidates, now we can filter down to a playlist of around 12 songs, suitably ordered. We determine an optimal playlist by clustering songs into 3 moods/styles and picking 4 songs from each cluster. Our playlist includes 4 songs which are not “popular.” After the playlist is created, we will visualize its evolution on the various quantitative metrics such as energy, valence, tempo, acousticness, and danceability. The Ultimate Playlist is shown below.\n\n\nCode\n# Task 7: Curate and Analyze Your Ultimate Playlist\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(cluster) # for clustering\n\n# Normalize relevant features for clustering\nfeatures &lt;- playlist_pool |&gt;\n  select(id, name, artist, tempo, key, acousticness, danceability, energy, valence) |&gt;\n  column_to_rownames(\"id\")\n\nscaled_features &lt;- scale(features |&gt; select(-name, -artist))\n\n# Cluster songs into moods/styles (e.g., 3 groups)\nset.seed(123)\nclusters &lt;- kmeans(scaled_features, centers = 3)$cluster\n\nplaylist_pool$cluster &lt;- clusters[playlist_pool$id]\n\n# Pick 4 songs from each cluster (12 total)\ncurated_playlist &lt;- playlist_pool |&gt;\n  group_by(cluster) |&gt;\n  slice_sample(n = 4) |&gt;\n  ungroup()\n\n# Order within playlist based on energy\ncurated_playlist &lt;- curated_playlist |&gt;\n  arrange(desc(energy))\n\n# Add track number\ncurated_playlist &lt;- curated_playlist |&gt;\n  mutate(track_num = row_number())\ncurated_playlist_1 &lt;- curated_playlist |&gt;\n  select(track_num, name, artist, year)\nkable(curated_playlist_1, cap='The Ultimate Playlist')\n\n\n\nThe Ultimate Playlist\n\n\n\n\n\n\n\n\ntrack_num\nname\nartist\nyear\n\n\n\n\n1\nSplit It\nMoneybagg Yo\n2020\n\n\n2\nWhat I’ve Been Looking For - From “High School Musical”/Soundtrack Version\nSharpay Evans\n2006\n\n\n3\nMaureen\nEddie Money\n1978\n\n\n4\nThe Letter - Mono Single Version\nThe Box Tops\n1967\n\n\n5\nFair Play\nVan Morrison\n1974\n\n\n6\nStupid Deep\nJon Bellion\n2018\n\n\n7\nSomeone Like You\nAdele\n2011\n\n\n8\nSex n’ Drugs\nHarrison Sands\n2018\n\n\n9\nMove Together\nJames Bay\n2014\n\n\n10\nYou Can Depend on Me\nBarbara Mason\n1968\n\n\n11\nMake You Feel My Love\nAdele\n2008\n\n\n\n\n\nCode\n# Pivot longer for plotting multiple metrics\nplaylist_long &lt;- curated_playlist |&gt;\n  select(track_num, name, energy, valence, tempo, acousticness, danceability) |&gt;\n  pivot_longer(cols = -c(track_num, name), names_to = \"feature\", values_to = \"value\")\n\n# Create the faceted plot\nggplot(playlist_long, aes(x = track_num, y = value)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"darkred\", size = 2) +\n  facet_wrap(~feature, scales = \"free_y\") +\n  scale_x_continuous(breaks = curated_playlist$track_num, labels = curated_playlist$track_num) +\n  labs(\n    title = \"Feature Progression Across Playlist\",\n    x = \"Track\",\n    y = \"Value\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 14)\n  )\n\n\n\n\n\n\n\n\n\nThe songs are ordered in the playlist based on energy. Thus, for energy metric, it is ‘all fall’. For acousticness, danceability, tempo, and valence, it is ‘rise and fall’. In addition to its musical structure, we consider the thematic unity as ‘travel and emotion’, and give our playlist a name ‘La La Locomotion’."
  },
  {
    "objectID": "mp03.html#conclusion-the-ultimate-playlist",
    "href": "mp03.html#conclusion-the-ultimate-playlist",
    "title": "Creating the Ultimate Playlist",
    "section": "Conclusion: The Ultimate Playlist",
    "text": "Conclusion: The Ultimate Playlist\nWe nominate “La La Locomotion” for the Internet’s Best Playlist award. This playlist uses Adele’s “Someone Like You” as the anchor song, consisting of songs that belong on a playlist with it using various heuristics listed above and then filtered down to 10 songs by clustering. We also use Quarto’s video support to embed “Someone Like You”. Feel free to listen to it again before you go!"
  },
  {
    "objectID": "mp04.html#code-completes-all-instructor-provided-tasks-correctly.-responses-to-open-ended-tasks-are-particularly-insightful-and-creative.",
    "href": "mp04.html#code-completes-all-instructor-provided-tasks-correctly.-responses-to-open-ended-tasks-are-particularly-insightful-and-creative.",
    "title": "Exploring Recent US Political Shifts",
    "section": "Code completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.",
    "text": "Code completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\n\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses."
  },
  {
    "objectID": "mp04.html#tables-and-figures-are-full-publication-quality.-report-includes-at-least-one-animated-visualization-designed-to-effectively-communicate-findings.",
    "href": "mp04.html#tables-and-figures-are-full-publication-quality.-report-includes-at-least-one-animated-visualization-designed-to-effectively-communicate-findings.",
    "title": "Exploring Recent US Political Shifts",
    "section": "Tables and figures are full ‘publication-quality’. Report includes at least one animated visualization designed to effectively communicate findings.",
    "text": "Tables and figures are full ‘publication-quality’. Report includes at least one animated visualization designed to effectively communicate findings.\n\nReport includes interactive (not just animated) visual elements."
  },
  {
    "objectID": "mp04.html#code-is-near-flawless.-code-passes-all-styler-and-lintr-type-analyses-without-issue.",
    "href": "mp04.html#code-is-near-flawless.-code-passes-all-styler-and-lintr-type-analyses-without-issue.",
    "title": "Exploring Recent US Political Shifts",
    "section": "Code is (near) flawless. Code passes all styler and lintr type analyses without issue.",
    "text": "Code is (near) flawless. Code passes all styler and lintr type analyses without issue.\n\nCode takes advantage of advanced Quarto features to improve presentation of results."
  },
  {
    "objectID": "mp04.html#data-import-is-fully-automated-and-efficient-taking-care-to-only-download-from-web-sources-if-not-available-locally.",
    "href": "mp04.html#data-import-is-fully-automated-and-efficient-taking-care-to-only-download-from-web-sources-if-not-available-locally.",
    "title": "Exploring Recent US Political Shifts",
    "section": "Data import is fully-automated and efficient, taking care to only download from web-sources if not available locally.",
    "text": "Data import is fully-automated and efficient, taking care to only download from web-sources if not available locally.\n\nReport uses additional data sources in a way that creates novel insights."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Exploring Recent US Political Shifts",
    "section": "",
    "text": "Code\nggplot() +\n  # Background state outlines\n  geom_sf(data = states_transformed, fill = \"gray98\", color = \"gray80\", size = 0.2) +\n  \n  # Arrows for vote shifts\n  geom_segment(\n    data = joined_all,\n    aes(x = x, y = y, xend = xend, yend = yend, color = shift_direction),\n    arrow = arrow(type = \"closed\", length = unit(0.06, \"inches\")),\n    linewidth = 0.25,\n    alpha = 0.9\n  ) +\n  \n  # NYT-style red/blue\n  scale_color_manual(values = c(\"Right\" = \"#c40000\", \"Left\" = \"#0050d0\")) +\n  \n  # Clean theme\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  \n  # 🔍 Zoom in on the US region (EPSG:2163)\n  coord_sf(\n    xlim = c(-2500000, 3000000),\n    ylim = c(-2700000, 1000000),\n    expand = FALSE,\n    datum = NA\n  )"
  }
]