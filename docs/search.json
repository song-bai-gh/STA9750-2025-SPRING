[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Last Updated: Saturday 03 01, 2025 at 17:28PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "",
    "text": "The New York City’s new Commission to Analyze Taxpayer Spending (CATS) intended to understand the city’s expenses and wondered if there were any opportunities to spend taxpayers’ money more efficiently. As a senior technical analyst, I will help analyze the city payroll and identify the instances where senior agency officials make significantly more than rank-and-file city employees. I will use City payroll data from NYC Open Data and highlight possible savings to be submitted to the commission."
  },
  {
    "objectID": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay-if-needed-assume-a-standard-2000-hour-work-year-and-no-overtime.",
    "href": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay-if-needed-assume-a-standard-2000-hour-work-year-and-no-overtime.",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.1 Which job title has the highest base rate of pay? (If needed, assume a standard 2000 hour work year and no overtime.)",
    "text": "3.1 Which job title has the highest base rate of pay? (If needed, assume a standard 2000 hour work year and no overtime.)\n\n\nShow the code\ntotal_compensation |&gt; mutate(\n  base_rate = case_when(\n    pay_basis == \"per Annum\" ~ base_salary, \n    pay_basis == \"per Hour\" ~ base_salary * 2000,\n    pay_basis == \"per Day\" ~ base_salary * 2000/7.5,\n    TRUE ~ NA_real_  \n  )\n) |&gt; group_by(title_description) |&gt; summarize(mean_rate=mean(base_rate)) |&gt;\n  arrange(desc(mean_rate))\n\n\n# A tibble: 1,977 × 2\n   title_description                                 mean_rate\n   &lt;chr&gt;                                                 &lt;dbl&gt;\n 1 Custodian Engineer                               120870091.\n 2 Member, Civilian Complaint Review Board             639785.\n 3 Medical Investigator                                623926.\n 4 Chairman                                            379618.\n 5 Member Of The Environmental Control Board - Oath    350188.\n 6 Chief Actuary                                       296470.\n 7 Pension Investment Advisor                          295814.\n 8 Chancellor                                          287066.\n 9 Captain - Chief Of Staff                            276588 \n10 First Deputy Mayor                                  274919.\n# ℹ 1,967 more rows\n\n\nWe can see the list of job titles with the highest base rate of pay. But the first title seems an outlier, I would say the member of civilian complaint review board has the highest base rate of pay."
  },
  {
    "objectID": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-payroll-regular-and-overtime-combined",
    "href": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-payroll-regular-and-overtime-combined",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.2 Which individual & in what year had the single highest city total payroll (regular and overtime combined)?",
    "text": "3.2 Which individual & in what year had the single highest city total payroll (regular and overtime combined)?\n\n\nShow the code\ntotal_compensation |&gt; \n  summarize(fiscal_year,first_name,last_name,total_compensation) |&gt;\n  arrange(desc(total_compensation))\n\n\n# A tibble: 6,225,611 × 4\n   fiscal_year first_name last_name   total_compensation\n         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                    &lt;dbl&gt;\n 1        2022 Gregory    Russ                    414707\n 2        2021 Gregory    Russ                    414707\n 3        2020 Gregory    Russ                    414707\n 4        2024 Lisa       Bova-Hiatt              393532\n 5        2024 Steven     Meier                   373320\n 6        2023 David      Banks                   363346\n 7        2023 David      Banks                   363346\n 8        2022 David      Banks                   363346\n 9        2022 Meisha     Ross Porter             363346\n10        2021 Meisha     Ross Porter             363346\n# ℹ 6,225,601 more rows\n\n\nWe can see Gregory Russ had the highest payroll of 414707 in 2020, 2021, and 2022."
  },
  {
    "objectID": "mp01.html#which-individual-worked-the-most-overtime-hours-in-this-data-set",
    "href": "mp01.html#which-individual-worked-the-most-overtime-hours-in-this-data-set",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.3 Which individual worked the most overtime hours in this data set?",
    "text": "3.3 Which individual worked the most overtime hours in this data set?\n\n\nShow the code\ntotal_compensation |&gt; summarize(fiscal_year,first_name,last_name,ot_hours) |&gt;\n  arrange(desc(ot_hours))\n\n\n# A tibble: 6,225,611 × 4\n   fiscal_year first_name last_name   ot_hours\n         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;\n 1        2022 James      Internicola    3693.\n 2        2022 Michael    Thompson       3618.\n 3        2022 Timothy    Sands          3556.\n 4        2014 John       Murphy         3348.\n 5        2022 Seeta      Deochan        3341.\n 6        2022 Dion       Middleton      3328.\n 7        2024 Kashwayne  Burnett        3303.\n 8        2022 Anthony    Messam         3271 \n 9        2022 Davey      Payne          3241 \n10        2022 Andre      Boucaud        3214 \n# ℹ 6,225,601 more rows\n\n\nJames Internicola worked the most overtime hours of 3693."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll-base-and-overtime-pay-per-employee",
    "href": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll-base-and-overtime-pay-per-employee",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.4 Which agency has the highest average total annual payroll (base and overtime pay per employee)?",
    "text": "3.4 Which agency has the highest average total annual payroll (base and overtime pay per employee)?\n\n\nShow the code\ntotal_compensation |&gt; group_by(agency_name) |&gt;\n  summarize(avg_tot_pay=mean(total_compensation)) |&gt;\n  arrange(desc(avg_tot_pay))\n\n\n# A tibble: 169 × 2\n   agency_name                    avg_tot_pay\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 Office Of Racial Equity            151093.\n 2 Commission On Racial Equity        136191 \n 3 Districting Commission             120641.\n 4 Office Of Criminal Justice         117280.\n 5 Office Of Collective Bargainin     114884.\n 6 Financial Info Svcs Agency         111230.\n 7 Office Of The Actuary              106924.\n 8 Bronx Community Board #3           105814.\n 9 Public Administrator-Richmond      105752.\n10 Municipal Water Fin Authority      101300.\n# ℹ 159 more rows\n\n\nOffice of Racial Equity has the highest average total annual payroll."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-most-employees-on-payroll-in-each-year",
    "href": "mp01.html#which-agency-has-the-most-employees-on-payroll-in-each-year",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.5 Which agency has the most employees on payroll in each year?",
    "text": "3.5 Which agency has the most employees on payroll in each year?\n\n\nShow the code\ntotal_compensation |&gt;\n  group_by(fiscal_year, agency_name) |&gt;\n  summarize(num_empl=n(),.groups=\"drop\") |&gt;\n  group_by(fiscal_year) |&gt;\n  slice_max(num_empl,n=1)\n\n\n# A tibble: 11 × 3\n# Groups:   fiscal_year [11]\n   fiscal_year agency_name            num_empl\n         &lt;dbl&gt; &lt;chr&gt;                     &lt;int&gt;\n 1        2014 Dept Of Ed Pedagogical   100589\n 2        2015 Dept Of Ed Pedagogical   111857\n 3        2016 Dept Of Ed Pedagogical   106263\n 4        2017 Dept Of Ed Pedagogical   104629\n 5        2018 Dept Of Ed Pedagogical   107956\n 6        2019 Dept Of Ed Pedagogical   112067\n 7        2020 Dept Of Ed Pedagogical   114999\n 8        2021 Dept Of Ed Pedagogical   113523\n 9        2022 Dept Of Ed Pedagogical   120453\n10        2023 Dept Of Ed Pedagogical   106882\n11        2024 Dept Of Ed Pedagogical   108209\n\n\nDepartment of Ed Pedagogical has the most employees in every year."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-overtime-usage-compared-to-regular-hours",
    "href": "mp01.html#which-agency-has-the-highest-overtime-usage-compared-to-regular-hours",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.6 Which agency has the highest overtime usage (compared to regular hours)?",
    "text": "3.6 Which agency has the highest overtime usage (compared to regular hours)?\n\n\nShow the code\ntotal_compensation |&gt;\n  group_by(agency_name) |&gt;\n  summarize(ot_usage=mean(ot_hours)/mean(regular_hours)) |&gt;\n  arrange(desc(ot_usage))\n\n\n# A tibble: 169 × 2\n   agency_name                  ot_usage\n   &lt;chr&gt;                           &lt;dbl&gt;\n 1 Board Of Election              0.200 \n 2 Fire Department                0.190 \n 3 Department Of Correction       0.185 \n 4 Department Of Sanitation       0.138 \n 5 Police Department              0.125 \n 6 Dept Of Citywide Admin Svcs    0.120 \n 7 Department Of Transportation   0.115 \n 8 Dept. Of Homeless Services     0.109 \n 9 Nyc Housing Authority          0.106 \n10 Admin For Children's Svcs      0.0812\n# ℹ 159 more rows\n\n\nBoard of Election has the highest overtime usage."
  },
  {
    "objectID": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs-that-is-whose-work_location_borough-is-not-one-of-the-five-counties.",
    "href": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs-that-is-whose-work_location_borough-is-not-one-of-the-five-counties.",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.7 What is the average salary of employees who work outside the five boroughs? (That is, whose work_location_borough is not one of the five counties.)",
    "text": "3.7 What is the average salary of employees who work outside the five boroughs? (That is, whose work_location_borough is not one of the five counties.)\n\n\nShow the code\ntotal_compensation |&gt;\n  filter(!work_location_borough %in% c(\"Manhattan\", \"Brooklyn\", \"Queens\", \n                                     \"Bronx\", \"Richmond\")) |&gt;\n  summarize(avg_salary=mean(total_compensation,na.rm=TRUE))\n\n\n# A tibble: 1 × 1\n  avg_salary\n       &lt;dbl&gt;\n1     55503.\n\n\nThe average salary of employees working outside the New York City is 55503."
  },
  {
    "objectID": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "href": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "title": "Commission to Analyze Taxpayer Spending (CATS)",
    "section": "3.8 How much has the city’s aggregate payroll grown over the past 10 years?",
    "text": "3.8 How much has the city’s aggregate payroll grown over the past 10 years?\n\n\nShow the code\ngrowth &lt;- total_compensation |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(agg_pay=mean(total_compensation,na.rm=TRUE))\n\nagg_pay_2024 &lt;- growth |&gt; filter(fiscal_year==\"2024\") |&gt;\n  pull(agg_pay)\n\nagg_pay_2014 &lt;- growth |&gt; filter(fiscal_year==\"2014\") |&gt;\n  pull(agg_pay)\n\ngrowth_amount &lt;- agg_pay_2024 - agg_pay_2014\ngrowth_percentage &lt;- growth_amount/agg_pay_2014*100\ngrowth_amount\n\n\n[1] 13384.23\n\n\nShow the code\ngrowth_percentage\n\n\n[1] 32.2868\n\n\nThe city’s payroll has grown 13384.23 dollars and 32.29% over the past 10 years.\nNow, I will analyze three possible policy changes to see their impact on overall spending. I have two policies suggested and will come up with my own policy proposal for analysis. For each policy, I will i) compute its impact on city payroll, ii) determine any other staffing adjustments required, and iii) make a recommendation to the CATS."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "2025 ‘Greenest Transit’ Awards Go to…",
    "section": "",
    "text": "This year’s Greenest Transit results just came out. The greeest transit agency awards have three categories: Large, Medium and Small, in terms of the sizes of agencies. Now, Let’s announce who the winners are! The greenest agencies are respectively MTA NYC Transit, Birmingham-Jefferson County Transit Authority, and Albany Transit System. The agencies with most emissions avoided are MTA New York City Transit, Hudson Transit Lines, and Hampton Jitney. The agencies with the highest electirfication level are TriMet, University of Georgia, and Connecticut Department of Transportation. We also have “winners” for the worst performance in terms of green and they are Washington State Ferries, SeaStreak, and Alaska Railroad Corporation. The relevent metrics with their values can be found in the appendix. To better demonstrate the ‘green-ness’ of the award winners, we have created some visualizations."
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThis press release announces the winners in each category. The greenest transit for large size agencies is King County Metro, in Seattle, Washington. For medium size agencies, Everett Transit in Everett, Washington is the greenest. The winner for small size agencies goes to RiverCities Transit from Longview, Washington."
  },
  {
    "objectID": "mp02.html#data-import",
    "href": "mp02.html#data-import",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "2 Data Import",
    "text": "2 Data Import\nWe download the EIA State Electricity Profiles, then use it to estimate the environmental impact of the electricity used to run certain transit systems.\n\n\nCode\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(stringr)\nensure_package(dplyr)\n\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n               # * 1000 kWh to MWH \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\n\nWe use the following code to create a table.\n\n\nCode\nensure_package(scales)\n\nEIA_SEP_REPORT |&gt; \n    select(-abbreviation) |&gt;\n    arrange(desc(CO2_MWh)) |&gt;\n    mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n           electricity_price_MWh = dollar(electricity_price_MWh), \n           generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n    rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n           `Primary Source of Electricity Generation`=primary_source, \n           `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n           `Total Generation Capacity (MWh)`= generation_MWh, \n           State=state) |&gt;\n    datatable()"
  },
  {
    "objectID": "mp02.html#initial-analysis-of-sep-data",
    "href": "mp02.html#initial-analysis-of-sep-data",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "3 Initial Analysis of SEP Data",
    "text": "3 Initial Analysis of SEP Data\nHere are some quick facts gained using the EIA_SEP_REPORT data.\n\nHawaii has the most expensive retail electricity with a price of $386 per MWh.\n\n\n\nCode\n# Which state has the most expensive retail electricity?\nstate_highest_price &lt;- EIA_SEP_REPORT |&gt;\n  select(electricity_price_MWh, state) |&gt;\n  slice_max(electricity_price_MWh, n=1)\n\n\n\nWest Virginia, whose primary source of electricity generation is coal, has the ‘dirtiest’ electricity mix with 1925 pounds of CO2 emitted per MWh.\n\n\n\nCode\n# Which state has the 'dirtiest' electricity mix?\nstate_dirtiest_electricity &lt;- EIA_SEP_REPORT |&gt;\n  select(CO2_MWh, primary_source, state) |&gt;\n  slice_max(CO2_MWh, n=1)\n\n\n\nOn average, 805.37 pounds of CO2 are emitted per MWh of electricity produced in the US.\n\n\n\nCode\n# On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US? (Note that you will need to use a suitably weighted average here.)\naverage_co2_emitted &lt;- EIA_SEP_REPORT |&gt;\n  summarize(\n    average_co2 = sum(CO2_MWh * generation_MWh) / sum(generation_MWh)\n  )\n\n\n\nPetroleum is the rarest primary energy source in the US used only in one state. The associated cost of electricity is $386 per MWh and it is used in Hawaii.\n\n\n\nCode\n# What is the rarest primary energy source in the US? \nrarest_source &lt;- EIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(num_state_source = n()) |&gt;\n  slice_min(num_state_source, n = 1)\n\n# What is the associated cost of electricity and where is it used?\nrarest_source_table &lt;- EIA_SEP_REPORT |&gt;\n  select(primary_source, electricity_price_MWh, state) |&gt;\n  filter(primary_source == \"Petroleum\")\n\n\n\nNY’s energy mix is 1.64 times cleaner than that of Texas.\n\n\n\nCode\n# How many times cleaner is NY’s energy mix than that of Texas?\nco2_emitted_NY &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"New York\") |&gt;\n  summarize(CO2_MWh)\n\nco2_emitted_Texas &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"Texas\") |&gt;\n  summarize(CO2_MWh)\n\ntimes_cleaner &lt;- co2_emitted_Texas / co2_emitted_NY\n\n\nWe now download the 2023 Annual Database Energy Consumption report and do some clean up. After that, we recode the Mode column.\n\n\nCode\n# import the data: 2023 Annual Database Energy Consumption\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n    DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                  destfile=NTD_ENERGY_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\n\n\n\nCode\n# do some basic clean-up\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n    replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(across(-c(`Agency Name`, \n                     `Mode`,\n                     `TOS`), \n                  to_numeric_fill_0)) |&gt;\n    group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n    summarize(across(where(is.numeric), sum), \n              .groups = \"keep\") |&gt;\n    mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n    filter(ENERGY &gt; 0) |&gt;\n    select(-ENERGY) |&gt;\n    ungroup()\n\n\n\n\nCode\n# clean up mode column in ntd energy\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"AR\" ~ \"Alaska Railroad\",\n        Mode == \"CB\" ~ \"Commuter Bus\",\n        Mode == \"CC\" ~ \"Cable Car\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"IP\" ~ \"Inclined Plane\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"MB\" ~ \"Bus\",\n        Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n        Mode == \"PB\" ~ \"Publico\",\n        Mode == \"RB\" ~ \"Bus Rapid Transit\",\n        Mode == \"SR\" ~ \"Streetcar Rail\",\n        Mode == \"TB\" ~ \"Trolleybus\",\n        Mode == \"TR\" ~ \"Aerial Tramways\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"YR\" ~ \"Hybrid Rail\",\n        TRUE ~ \"Unknown\"))\n\n\nNext, we download the 2023 Service by Agency report, from which we will extract characteristics of typical passenger trips on each transit service. We will also explore the data by answering some questions using the service data.\n\n\nCode\n# download the 2023 service by agency report\nlibrary(readr)\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n\n\n\nCode\n#clean up the service data\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n    mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n    rename(Agency = agency, \n           City   = max_city, \n           State  = max_state,\n           UPT    = sum_unlinked_passenger_trips_upt, \n           MILES  = sum_passenger_miles) |&gt;\n    select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n    filter(MILES &gt; 0)"
  },
  {
    "objectID": "mp02.html#recoding-the-mode-column",
    "href": "mp02.html#recoding-the-mode-column",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "4 Recoding the mode column",
    "text": "4 Recoding the mode column\n\n\nCode\n# clean up mode column in ntd energy\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"AR\" ~ \"Alaska Railroad\",\n        Mode == \"CB\" ~ \"Commuter Bus\",\n        Mode == \"CC\" ~ \"Cable Car\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"IP\" ~ \"Inclined Plane\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"MB\" ~ \"Bus\",\n        Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n        Mode == \"PB\" ~ \"Publico\",\n        Mode == \"RB\" ~ \"Bus Rapid Transit\",\n        Mode == \"SR\" ~ \"Streetcar Rail\",\n        Mode == \"TB\" ~ \"Trolleybus\",\n        Mode == \"TR\" ~ \"Aerial Tramways\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"YR\" ~ \"Hybrid Rail\",\n        TRUE ~ \"Unknown\"))\n\n\n\n\nCode\n# download the 2023 service by agency report\nlibrary(readr)\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n\n\n\nCode\n#clean up the service data\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n    mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n    rename(Agency = agency, \n           City   = max_city, \n           State  = max_state,\n           UPT    = sum_unlinked_passenger_trips_upt, \n           MILES  = sum_passenger_miles) |&gt;\n    select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n    filter(MILES &gt; 0)"
  },
  {
    "objectID": "mp02.html#explore-ntd-service-data",
    "href": "mp02.html#explore-ntd-service-data",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "4 Explore NTD Service Data",
    "text": "4 Explore NTD Service Data\n\n\nCode\n# Which transit service has the most UPT annually?\nlibrary(knitr)\n\nmost_upt &lt;- NTD_SERVICE |&gt;\n  mutate(UPT = comma(UPT)) |&gt;\n  slice_max(UPT, n = 1)\n\nkable(most_upt, caption = \"Transit Service with the Most UPT Annually\")\n\n\n\nTransit Service with the Most UPT Annually\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\n\n\n\n\nRockford Mass Transit District\nRockford\nIL\n994,754\n5035893\n50058\n\n\n\n\n\n\n\nCode\n# What is the average trip length of a trip on MTA NYC?\nmta_avg_trip &lt;- NTD_SERVICE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  mutate(`MTA Average Trip Length` = MILES / UPT) |&gt;\n  select(`NTD ID`, Agency, City, State, `MTA Average Trip Length`)\n\nkable(mta_avg_trip, caption = \"Average Trip Length on MTA NYC\")\n\n\n\nAverage Trip Length on MTA NYC\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency\nCity\nState\nMTA Average Trip Length\n\n\n\n\n20008\nMTA New York City Transit\nBrooklyn\nNY\n3.644089\n\n\n\n\n\n\n\nCode\n# Which transit service in NYC has the longest average trip length?\nnyc_longest_trip &lt;- NTD_SERVICE |&gt;\n  filter(State == \"NY\", City == \"New York\" | City == \"Brooklyn\") |&gt;\n  mutate(`Average Trip Length` = MILES / UPT) |&gt;\n  slice_max(`Average Trip Length`, n = 1)\n\nkable(nyc_longest_trip, caption = \"Transit Service in NYC with the Longest Average Trip Length\")\n\n\n\nTransit Service in NYC with the Longest Average Trip Length\n\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\nAverage Trip Length\n\n\n\n\nMTA Long Island Rail Road\nNew York\nNY\n83835706\n2033685836\n20100\n24.25799\n\n\n\n\n\n\n\nCode\n# Which state has the fewest total miles travelled by public transit?\nstate_fewest_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(`Total Miles` = comma(sum(MILES, na.rm = TRUE))) |&gt;\n  slice_min(`Total Miles`, n = 1)\n\nkable(state_fewest_miles, caption = \"State with the Fewest Total Miles Travelled by Public Transit\")\n\n\n\nState with the Fewest Total Miles Travelled by Public Transit\n\n\nState\nTotal Miles\n\n\n\n\nWA\n1,059,910,614\n\n\n\n\n\n\n\nCode\n# Are all states represented in this data? If no, which ones are missing? The state.name and state.abb objects we used above may be useful here.\nstates_missing &lt;- EIA_SEP_REPORT |&gt;\n  anti_join(NTD_SERVICE, join_by(\"abbreviation\" == \"State\")) |&gt;\n  rename(`Missing States` = state) |&gt;\n  select(`Missing States`)\n\nkable(states_missing, caption = \"Missing States\")\n\n\n\nMissing States\n\n\nMissing States\n\n\n\n\nArizona\n\n\nArkansas\n\n\nCalifornia\n\n\nColorado\n\n\nHawaii\n\n\nIowa\n\n\nKansas\n\n\nLouisiana\n\n\nMissouri\n\n\nMontana\n\n\nNebraska\n\n\nNevada\n\n\nNew Mexico\n\n\nNorth Dakota\n\n\nOklahoma\n\n\nSouth Dakota\n\n\nTexas\n\n\nUtah\n\n\nWyoming"
  },
  {
    "objectID": "mp02.html#analysis",
    "href": "mp02.html#analysis",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "5 Analysis",
    "text": "5 Analysis\nWe’re now ready to start putting these datasets together and using them to identify America’s greenest transit agencies.\n\n\nCode\n# Task 5: join the three tables\nagency_mode_pair &lt;- NTD_SERVICE |&gt;\n  inner_join(NTD_ENERGY |&gt;\n      select(-c(`Kerosene`, `Bunker Fuel`, `Ethanol`, `Methonal`)), #clean it up \n    join_by(\"NTD ID\" == \"NTD ID\")\n    ) |&gt; \n  inner_join(EIA_SEP_REPORT,\n    join_by(State == \"abbreviation\")\n    ) |&gt;\n  select(-c(`Agency Name`, State)) |&gt;\n  rename(\n    agency = `Agency`,\n    city = `City`,\n    mode = `Mode`,\n    upt = `UPT`,\n    miles = `MILES`,\n    ntd_id = `NTD ID`,\n    biodiesel = `Bio-Diesel`,\n    cnaturalgas = `C Natural Gas`,\n    diesel = `Diesel Fuel`,\n    gasoline = `Gasoline`,\n    liqnatgas = `Liquified Nat Gas`,\n    liqpetgas =`Liquified Petroleum Gas`,\n    electric_battery = `Electric Battery`,\n    electric_propulsion = `Electric Propulsion`,\n    co2_mwh = CO2_MWh\n    ) |&gt;\n  mutate(\n    electricbattery_emission = electric_battery * (co2_mwh / 1000),\n    electricprop_emission = electric_propulsion * (co2_mwh / 1000),\n    total_emission = (\n        electricbattery_emission +\n        electricprop_emission\n      )\n  ) |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(agency_total_emission = sum(total_emission)) |&gt;\n  ungroup()\n\n# Task 6\n# use percentiles to define small, medium, and large agencies. \npercentile_1 &lt;- quantile(agency_mode_pair |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.3)\npercentile_2 &lt;- quantile(agency_mode_pair |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.7)\n\n# updating agency_mode_pair with agency size\n# adding green level\nagency_mode_pair &lt;-\n  agency_mode_pair |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(\n    agency_emission_per_capita = sum(total_emission) / upt,\n    emission_per_transit = sum(total_emission) / miles,\n    green_level = agency_emission_per_capita * emission_per_transit,\n    size = case_when(\n      upt &lt; percentile_1 ~ \"Small\",\n      upt &gt;= percentile_1 & upt &lt;= percentile_2 ~ \"Medium\",\n      upt &gt; percentile_2 ~ \"Large\",\n    )\n    ) |&gt;\n  ungroup()\n\n\nBesed on our analysis, the Greenest Transit Agency award goes to King County Metro for large size, it goes to Everett Transist for medium size, and it goes to RiverCities Transit for small size.\n\n\nCode\n# grouping th data\ngreen_levels &lt;-\n  agency_mode_pair |&gt;\n  group_by(size, ntd_id) |&gt;\n  summarize(\n    agency = first(agency),\n    city = first(city),\n    state = first(state),\n    green_level = first(green_level),\n    .groups = 'drop'\n  ) |&gt;\n  ungroup()\n\n# Select the greenest agency in each size\ngreenest_agency &lt;-\n  green_levels |&gt;\n  group_by(size) |&gt;\n  slice(1) |&gt;\n  select(agency, city, state)\n\n# Compute median green\nmedian_green &lt;- green_levels |&gt;\n  group_by(size) |&gt;\n  summarise(median_green = median(green_level, na.rm = TRUE))\n\n# visualize greenest agenciies\ngreenest_agency |&gt;\n  left_join(median_green, join_by(\"size\" == \"size\")) |&gt;\n  select(agency, city, state) |&gt;\n  kable(caption = \"Greenest Transit Agency\")\n\n\n\nGreenest Transit Agency\n\n\n\n\n\n\n\n\nsize\nagency\ncity\nstate\n\n\n\n\nLarge\nKing County, dba: King County Metro\nSeattle\nWashington\n\n\nMedium\nCity of Everett, dba: Everett Transit\nEverett\nWashington\n\n\nSmall\nCity of Longview, dba: RiverCities Transit\nLongview\nWashington"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "",
    "text": "Green Transit Alliance for Investigation of Variance (GTA IV) gave a series of awards for the greenest public transit angencies. The winners of the various GTA IV awards went to King County Metro, Everett Transit and RiverCities Transit. We will explore US Public Transit systems to assess their environmental efficiency and will use a variety of data sources to i) determine how many riders are served by different transit systems; ii) determine how far each public transit system transports an average rider; and iii) investigate the effective emissions associated with each form of transit."
  },
  {
    "objectID": "mp02_v1.html",
    "href": "mp02_v1.html",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "",
    "text": "Green Transit Alliance for Investigation of Variance (GTA IV) gave a series of awards for the greenest public transit angencies. The winners of the various GTA IV awards went to King County Metro, Everett Transit and RiverCities Transit. We will explore US Public Transit systems to assess their environmental efficiency and will use a variety of data sources to i) determine how many riders are served by different transit systems; ii) determine how far each public transit system transports an average rider; and iii) investigate the effective emissions associated with each form of transit."
  },
  {
    "objectID": "mp02_v1.html#introduction",
    "href": "mp02_v1.html#introduction",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "",
    "text": "Green Transit Alliance for Investigation of Variance (GTA IV) gave a series of awards for the greenest public transit angencies. The winners of the various GTA IV awards went to King County Metro, Everett Transit and RiverCities Transit. We will explore US Public Transit systems to assess their environmental efficiency and will use a variety of data sources to i) determine how many riders are served by different transit systems; ii) determine how far each public transit system transports an average rider; and iii) investigate the effective emissions associated with each form of transit."
  },
  {
    "objectID": "mp02_v1.html#data-import",
    "href": "mp02_v1.html#data-import",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "2 Data Import",
    "text": "2 Data Import\nWe download the EIA State Electricity Profiles, then use it to estimate the environmental impact of the electricity used to run certain transit systems.\n\n\nCode\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(stringr)\nensure_package(dplyr)\n\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n               # * 1000 kWh to MWH \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\n\nWe use the following code to create a table.\n\n\nCode\nensure_package(scales)\n\nEIA_SEP_REPORT |&gt; \n    select(-abbreviation) |&gt;\n    arrange(desc(CO2_MWh)) |&gt;\n    mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n           electricity_price_MWh = dollar(electricity_price_MWh), \n           generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n    rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n           `Primary Source of Electricity Generation`=primary_source, \n           `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n           `Total Generation Capacity (MWh)`= generation_MWh, \n           State=state) |&gt;\n    datatable()"
  },
  {
    "objectID": "mp02_v1.html#initial-analysis-of-sep-data",
    "href": "mp02_v1.html#initial-analysis-of-sep-data",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "3 Initial Analysis of SEP Data",
    "text": "3 Initial Analysis of SEP Data\nHere are some quick facts gained using the EIA_SEP_REPORT data.\n\nHawaii has the most expensive retail electricity with a price of $386 per MWh.\n\n\n\nCode\n# Which state has the most expensive retail electricity?\nstate_highest_price &lt;- EIA_SEP_REPORT |&gt;\n  select(electricity_price_MWh, state) |&gt;\n  slice_max(electricity_price_MWh, n=1)\n\n\n\nWest Virginia, whose primary source of electricity generation is coal, has the ‘dirtiest’ electricity mix with 1925 pounds of CO2 emitted per MWh.\n\n\n\nCode\n# Which state has the 'dirtiest' electricity mix?\nstate_dirtiest_electricity &lt;- EIA_SEP_REPORT |&gt;\n  select(CO2_MWh, primary_source, state) |&gt;\n  slice_max(CO2_MWh, n=1)\n\n\n\nOn average, 805.37 pounds of CO2 are emitted per MWh of electricity produced in the US.\n\n\n\nCode\n# On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US? (Note that you will need to use a suitably weighted average here.)\naverage_co2_emitted &lt;- EIA_SEP_REPORT |&gt;\n  summarize(\n    average_co2 = sum(CO2_MWh * generation_MWh) / sum(generation_MWh)\n  )\n\n\n\nPetroleum is the rarest primary energy source in the US used only in one state. The associated cost of electricity is $386 per MWh and it is used in Hawaii.\n\n\n\nCode\n# What is the rarest primary energy source in the US? \nrarest_source &lt;- EIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(num_state_source = n()) |&gt;\n  slice_min(num_state_source, n = 1)\n\n# What is the associated cost of electricity and where is it used?\nrarest_source_table &lt;- EIA_SEP_REPORT |&gt;\n  select(primary_source, electricity_price_MWh, state) |&gt;\n  filter(primary_source == \"Petroleum\")\n\n\n\nNY’s energy mix is 1.64 times cleaner than that of Texas.\n\n\n\nCode\n# How many times cleaner is NY’s energy mix than that of Texas?\nco2_emitted_NY &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"New York\") |&gt;\n  summarize(CO2_MWh)\n\nco2_emitted_Texas &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"Texas\") |&gt;\n  summarize(CO2_MWh)\n\ntimes_cleaner &lt;- co2_emitted_Texas / co2_emitted_NY\n\n\nWe now download the 2023 Annual Database Energy Consumption report and do some clean up. After that, we recode the Mode column.\n\n\nCode\n# import the data: 2023 Annual Database Energy Consumption\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n    DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                  destfile=NTD_ENERGY_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\n\n\n\nCode\n# do some basic clean-up\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n    replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(across(-c(`Agency Name`, \n                     `Mode`,\n                     `TOS`), \n                  to_numeric_fill_0)) |&gt;\n    group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n    summarize(across(where(is.numeric), sum), \n              .groups = \"keep\") |&gt;\n    mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n    filter(ENERGY &gt; 0) |&gt;\n    select(-ENERGY) |&gt;\n    ungroup()\n\n\n\n\nCode\n# clean up mode column in ntd energy\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"AR\" ~ \"Alaska Railroad\",\n        Mode == \"CB\" ~ \"Commuter Bus\",\n        Mode == \"CC\" ~ \"Cable Car\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"IP\" ~ \"Inclined Plane\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"MB\" ~ \"Bus\",\n        Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n        Mode == \"PB\" ~ \"Publico\",\n        Mode == \"RB\" ~ \"Bus Rapid Transit\",\n        Mode == \"SR\" ~ \"Streetcar Rail\",\n        Mode == \"TB\" ~ \"Trolleybus\",\n        Mode == \"TR\" ~ \"Aerial Tramways\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"YR\" ~ \"Hybrid Rail\",\n        TRUE ~ \"Unknown\"))\n\n\nNext, we download the 2023 Service by Agency report, from which we will extract characteristics of typical passenger trips on each transit service. We will also explore the data by answering some questions using the service data.\n\n\nCode\n# download the 2023 service by agency report\nlibrary(readr)\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n\n\n\nCode\n#clean up the service data\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n    mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n    rename(Agency = agency, \n           City   = max_city, \n           State  = max_state,\n           UPT    = sum_unlinked_passenger_trips_upt, \n           MILES  = sum_passenger_miles) |&gt;\n    select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n    filter(MILES &gt; 0)"
  },
  {
    "objectID": "mp02_v1.html#explore-ntd-service-data",
    "href": "mp02_v1.html#explore-ntd-service-data",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "4 Explore NTD Service Data",
    "text": "4 Explore NTD Service Data\n\n\nCode\n# Which transit service has the most UPT annually?\nlibrary(knitr)\n\nmost_upt &lt;- NTD_SERVICE |&gt;\n  mutate(UPT = comma(UPT)) |&gt;\n  slice_max(UPT, n = 1)\n\nkable(most_upt, caption = \"Transit Service with the Most UPT Annually\")\n\n\n\nTransit Service with the Most UPT Annually\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\n\n\n\n\nRockford Mass Transit District\nRockford\nIL\n994,754\n5035893\n50058\n\n\n\n\n\n\n\nCode\n# What is the average trip length of a trip on MTA NYC?\nmta_avg_trip &lt;- NTD_SERVICE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  mutate(`MTA Average Trip Length` = MILES / UPT) |&gt;\n  select(`NTD ID`, Agency, City, State, `MTA Average Trip Length`)\n\nkable(mta_avg_trip, caption = \"Average Trip Length on MTA NYC\")\n\n\n\nAverage Trip Length on MTA NYC\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency\nCity\nState\nMTA Average Trip Length\n\n\n\n\n20008\nMTA New York City Transit\nBrooklyn\nNY\n3.644089\n\n\n\n\n\n\n\nCode\n# Which transit service in NYC has the longest average trip length?\nnyc_longest_trip &lt;- NTD_SERVICE |&gt;\n  filter(State == \"NY\", City == \"New York\" | City == \"Brooklyn\") |&gt;\n  mutate(`Average Trip Length` = MILES / UPT) |&gt;\n  slice_max(`Average Trip Length`, n = 1)\n\nkable(nyc_longest_trip, caption = \"Transit Service in NYC with the Longest Average Trip Length\")\n\n\n\nTransit Service in NYC with the Longest Average Trip Length\n\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\nAverage Trip Length\n\n\n\n\nMTA Long Island Rail Road\nNew York\nNY\n83835706\n2033685836\n20100\n24.25799\n\n\n\n\n\n\n\nCode\n# Which state has the fewest total miles travelled by public transit?\nstate_fewest_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(`Total Miles` = comma(sum(MILES, na.rm = TRUE))) |&gt;\n  slice_min(`Total Miles`, n = 1)\n\nkable(state_fewest_miles, caption = \"State with the Fewest Total Miles Travelled by Public Transit\")\n\n\n\nState with the Fewest Total Miles Travelled by Public Transit\n\n\nState\nTotal Miles\n\n\n\n\nWA\n1,059,910,614\n\n\n\n\n\n\n\nCode\n# Are all states represented in this data? If no, which ones are missing? The state.name and state.abb objects we used above may be useful here.\nstates_missing &lt;- EIA_SEP_REPORT |&gt;\n  anti_join(NTD_SERVICE, join_by(\"abbreviation\" == \"State\")) |&gt;\n  rename(`Missing States` = state) |&gt;\n  select(`Missing States`)\n\nkable(states_missing, caption = \"Missing States\")\n\n\n\nMissing States\n\n\nMissing States\n\n\n\n\nArizona\n\n\nArkansas\n\n\nCalifornia\n\n\nColorado\n\n\nHawaii\n\n\nIowa\n\n\nKansas\n\n\nLouisiana\n\n\nMissouri\n\n\nMontana\n\n\nNebraska\n\n\nNevada\n\n\nNew Mexico\n\n\nNorth Dakota\n\n\nOklahoma\n\n\nSouth Dakota\n\n\nTexas\n\n\nUtah\n\n\nWyoming"
  },
  {
    "objectID": "mp02_v1.html#analysis",
    "href": "mp02_v1.html#analysis",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "5 Analysis",
    "text": "5 Analysis\nWe’re now ready to start putting these datasets together and using them to identify America’s greenest transit agencies.\n\n\nCode\n# Task 5: join the three tables\nagency_mode_pair &lt;- NTD_SERVICE |&gt;\n  inner_join(NTD_ENERGY |&gt;\n      select(-c(`Kerosene`, `Bunker Fuel`, `Ethanol`, `Methonal`)), #clean it up \n    join_by(\"NTD ID\" == \"NTD ID\")\n    ) |&gt; \n  inner_join(EIA_SEP_REPORT,\n    join_by(State == \"abbreviation\")\n    ) |&gt;\n  select(-c(`Agency Name`, State)) |&gt;\n  rename(\n    agency = `Agency`,\n    city = `City`,\n    mode = `Mode`,\n    upt = `UPT`,\n    miles = `MILES`,\n    ntd_id = `NTD ID`,\n    biodiesel = `Bio-Diesel`,\n    cnaturalgas = `C Natural Gas`,\n    diesel = `Diesel Fuel`,\n    gasoline = `Gasoline`,\n    liqnatgas = `Liquified Nat Gas`,\n    liqpetgas =`Liquified Petroleum Gas`,\n    electric_battery = `Electric Battery`,\n    electric_propulsion = `Electric Propulsion`,\n    co2_mwh = CO2_MWh\n    ) |&gt;\n  mutate(\n    electricbattery_emission = electric_battery * (co2_mwh / 1000),\n    electricprop_emission = electric_propulsion * (co2_mwh / 1000),\n    total_emission = (\n        electricbattery_emission +\n        electricprop_emission\n      )\n  ) |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(agency_total_emission = sum(total_emission)) |&gt;\n  ungroup()\n\n# Task 6\n# use percentiles to define small, medium, and large agencies. \npercentile_1 &lt;- quantile(agency_mode_pair |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.3)\npercentile_2 &lt;- quantile(agency_mode_pair |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.7)\n\n# updating agency_mode_pair with agency size\n# adding green level\nagency_mode_pair &lt;-\n  agency_mode_pair |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(\n    agency_emission_per_capita = sum(total_emission) / upt,\n    emission_per_transit = sum(total_emission) / miles,\n    green_level = agency_emission_per_capita * emission_per_transit,\n    size = case_when(\n      upt &lt; percentile_1 ~ \"Small\",\n      upt &gt;= percentile_1 & upt &lt;= percentile_2 ~ \"Medium\",\n      upt &gt; percentile_2 ~ \"Large\",\n    )\n    ) |&gt;\n  ungroup()\n\n\nBesed on our analysis, the Greenest Transit Agency award goes to King County Metro for large size, it goes to Everett Transist for medium size, and it goes to RiverCities Transit for small size.\n\n\nCode\n# grouping th data\ngreen_levels &lt;-\n  agency_mode_pair |&gt;\n  group_by(size, ntd_id) |&gt;\n  summarize(\n    agency = first(agency),\n    city = first(city),\n    state = first(state),\n    green_level = first(green_level),\n    .groups = 'drop'\n  ) |&gt;\n  ungroup()\n\n# Select the greenest agency in each size\ngreenest_agency &lt;-\n  green_levels |&gt;\n  group_by(size) |&gt;\n  slice(1) |&gt;\n  select(agency, city, state)\n\n# Compute median green\nmedian_green &lt;- green_levels |&gt;\n  group_by(size) |&gt;\n  summarise(median_green = median(green_level, na.rm = TRUE))\n\n# visualize greenest agenciies\ngreenest_agency |&gt;\n  left_join(median_green, join_by(\"size\" == \"size\")) |&gt;\n  select(agency, city, state) |&gt;\n  kable(caption = \"Greenest Transit Agency\")\n\n\n\nGreenest Transit Agency\n\n\n\n\n\n\n\n\nsize\nagency\ncity\nstate\n\n\n\n\nLarge\nKing County, dba: King County Metro\nSeattle\nWashington\n\n\nMedium\nCity of Everett, dba: Everett Transit\nEverett\nWashington\n\n\nSmall\nCity of Longview, dba: RiverCities Transit\nLongview\nWashington"
  },
  {
    "objectID": "mp02_v1.html#conclusion",
    "href": "mp02_v1.html#conclusion",
    "title": "Environmental Efficiency of US Public Transit Systems",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThis press release announces the winners in each category. The greenest transit for large size agencies is King County Metro, in Seattle, Washington. For medium size agencies, Everett Transit in Everett, Washington is the greenest. The winner for small size agencies goes to RiverCities Transit from Longview, Washington."
  },
  {
    "objectID": "mp02.html#greenest-transit-awards",
    "href": "mp02.html#greenest-transit-awards",
    "title": "2025 ‘Greenest Transit’ Awards Go to…",
    "section": "",
    "text": "This year’s Greenest Transit results just came out. The greeest transit agency awards have three categories: Large, Medium and Small, in terms of the sizes of agencies. Now, Let’s announce who the winners are! The greenest agencies are respectively MTA NYC Transit, Birmingham-Jefferson County Transit Authority, and Albany Transit System. The agencies with most emissions avoided are MTA New York City Transit, Hudson Transit Lines, and Hampton Jitney. The agencies with the highest electirfication level are TriMet, University of Georgia, and Connecticut Department of Transportation. We also have “winners” for the worst performance in terms of green and they are Washington State Ferries, SeaStreak, and Alaska Railroad Corporation. The relevent metrics with their values can be found in the appendix. To better demonstrate the ‘green-ness’ of the award winners, we have created some visualizations."
  },
  {
    "objectID": "mp02.html#appendix",
    "href": "mp02.html#appendix",
    "title": "2025 ‘Greenest Transit’ Awards Go to…",
    "section": "2 Appendix",
    "text": "2 Appendix\n\n\nCode\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(dplyr)\nensure_package(stringr)\nensure_package(tidyr)\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\n\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n               # * 1000 kWh to MWH \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\nensure_package(scales)\nensure_package(DT)\n\nlibrary(knitr)\n# Which state has the most expensive retail electricity?\nstate_most_expensive &lt;- EIA_SEP_REPORT |&gt;\n  slice_max(electricity_price_MWh, n = 1) |&gt;\n  mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n         electricity_price_MWh = dollar(electricity_price_MWh), \n         generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n  rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n         `Primary Source of Electricity Generation`=primary_source, \n         `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n         `Total Generation Capacity (MWh)`= generation_MWh, \n         State=state)\nkable(state_most_expensive, caption = \"State with the Most Expensive Retail Electricity\")\n\n\n\nState with the Most Expensive Retail Electricity\n\n\n\n\n\n\n\n\n\n\nPounds of CO2 Emitted per MWh of Electricity Produced\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nTotal Generation Capacity (MWh)\nState\nabbreviation\n\n\n\n\n1,444\nPetroleum\n$386\n9,194,164\nHawaii\nHI\n\n\n\n\n\nCode\n# Which state has the ‘dirtiest’ electricity mix?\nstate_dirtiest &lt;- EIA_SEP_REPORT |&gt;\n  slice_max(CO2_MWh, n = 1) |&gt;\n  mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n         electricity_price_MWh = dollar(electricity_price_MWh), \n         generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n  rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n         `Primary Source of Electricity Generation`=primary_source, \n         `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n         `Total Generation Capacity (MWh)`= generation_MWh, \n         State=state)\nkable(state_dirtiest, caption = \"State with the 'Dirtiest' Electricity Mix\")\n\n\n\nState with the ‘Dirtiest’ Electricity Mix\n\n\n\n\n\n\n\n\n\n\nPounds of CO2 Emitted per MWh of Electricity Produced\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nTotal Generation Capacity (MWh)\nState\nabbreviation\n\n\n\n\n1,925\nCoal\n$102.60\n52,286,784\nWest Virginia\nWV\n\n\n\n\n\nCode\n# On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US?\navg_co2 &lt;- EIA_SEP_REPORT |&gt;\n  summarize(avg_co2 = sum(CO2_MWh * generation_MWh) / sum(generation_MWh)) |&gt;\n  mutate(avg_co2= number(avg_co2, big.mark = \",\")) |&gt;\n  rename(`Average CO2 emissions per MWh`=avg_co2)\nkable(avg_co2, caption = \"Average CO2 Emissions per MWh in the US\")\n\n\n\nAverage CO2 Emissions per MWh in the US\n\n\nAverage CO2 emissions per MWh\n\n\n\n\n805\n\n\n\n\n\nCode\n# What is the rarest primary energy source in the US?\nrarest_source &lt;- EIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(Number = n()) |&gt;\n  slice_min(Number, n = 1) |&gt;\n  pull(primary_source)\n# What is the associated cost of electricity and where is it used?\nrarest &lt;- EIA_SEP_REPORT |&gt;\n  filter(primary_source==rarest_source) |&gt;\n  select(primary_source, electricity_price_MWh, state) |&gt;\n  mutate(electricity_price_MWh = dollar(electricity_price_MWh)) |&gt;\n  rename(`Primary Source of Electricity Generation`=primary_source, \n         `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n         State=state)\nkable(rarest, caption =\"Rarest Source and Its Cost and Location\")\n\n\n\nRarest Source and Its Cost and Location\n\n\n\n\n\n\n\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nState\n\n\n\n\nPetroleum\n$386\nHawaii\n\n\n\n\n\nCode\n# My home state, Texas, has a reputation as being the home of “dirty fossil fuels” \n# while NY has a reputation as a leader in clean energy. How many times cleaner is \n# NY’s energy mix than that of Texas?\nco2_ny &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"New York\") |&gt;\n  pull(CO2_MWh)\nco2_tx &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"Texas\") |&gt;\n  pull(CO2_MWh)\ntimes_cleaner &lt;- EIA_SEP_REPORT |&gt;\n  filter(state %in% c(\"New York\",\"Texas\")) |&gt;\n  mutate(times = CO2_MWh / co2_ny) |&gt;\n  select(state, primary_source, CO2_MWh, times) |&gt;\n  rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n         `Primary Source of Electricity Generation`=primary_source, \n         `New York is Cleaner than this State with Times`=times,\n         State=state)\nkable(times_cleaner,caption=\"New York is cleaner than Texas\")\n\n\n\nNew York is cleaner than Texas\n\n\n\n\n\n\n\n\nState\nPrimary Source of Electricity Generation\nPounds of CO2 Emitted per MWh of Electricity Produced\nNew York is Cleaner than this State with Times\n\n\n\n\nNew York\nNatural gas\n522\n1.000000\n\n\nTexas\nNatural gas\n855\n1.637931\n\n\n\n\n\nCode\n# 2023 ntd energy\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n  DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                      destfile=NTD_ENERGY_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\n# basic clean up\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n  x &lt;- if_else(x == \"-\", NA, x)\n  replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n  select(-c(`Reporter Type`, \n            `Reporting Module`, \n            `Other Fuel`, \n            `Other Fuel Description`)) |&gt;\n  mutate(across(-c(`Agency Name`, \n                   `Mode`,\n                   `TOS`), \n                to_numeric_fill_0)) |&gt;\n  group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n  summarize(across(where(is.numeric), sum), \n            .groups = \"keep\") |&gt;\n  mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n  filter(ENERGY &gt; 0) |&gt;\n  select(-ENERGY) |&gt;\n  ungroup()\n\n## This code needs to be modified\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n  mutate(Mode=case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\", \n    Mode == \"CB\" ~ \"Commuter Bus\", \n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\", \n    Mode == \"DR\" ~ \"Demand Response\", \n    Mode == \"FB\" ~ \"Ferryboat\", \n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"IP\" ~ \"Inclined Plane\", \n    Mode == \"LR\" ~ \"Light Rail\", \n    Mode == \"MB\" ~ \"Bus\", \n    Mode == \"MG\" ~ \"Monorail/Automated Guideway\", \n    Mode == \"PB\" ~ \"Publico\", \n    Mode == \"RB\" ~ \"Bus Rapid Transit\", \n    Mode == \"SR\" ~ \"Streetcar Rail\", \n    Mode == \"TB\" ~ \"Trolleybus\", \n    Mode == \"TR\" ~ \"Aerial Tramway\", \n    Mode == \"VP\" ~ \"Vanpool\", \n    Mode == \"YR\" ~ \"Hybrid Rail\", \n    TRUE ~ \"Unknown\"))\n\n# download the service by agency\nlibrary(readr)\n\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n  DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                      destfile=NTD_SERVICE_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n# clean it up\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n  mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n  rename(Agency = agency, \n         City   = max_city, \n         State  = max_state,\n         UPT    = sum_unlinked_passenger_trips_upt, \n         MILES  = sum_passenger_miles) |&gt;\n  select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n  filter(MILES &gt; 0)\n\n# Which transit service has the most UPT annually?\nmost_upt &lt;- NTD_SERVICE |&gt;\n  slice_max(UPT, n=1) |&gt;\n  mutate(UPT=number(UPT, big.mark=\",\"),\n         MILES=number(MILES,big.mark=\",\"))\nkable(most_upt, caption=\"Transit Service with the Most UPT Annually\")\n\n\n\nTransit Service with the Most UPT Annually\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\n\n\n\n\nMTA New York City Transit\nBrooklyn\nNY\n2,632,003,044\n9,591,253,658\n20008\n\n\n\n\n\nCode\n# What is the average trip length of a trip on MTA NYC?\navg_length &lt;- NTD_SERVICE |&gt;\n  filter(Agency==\"MTA New York City Transit\") |&gt;\n  mutate(avg_length=MILES/UPT, big.mark=\",\") |&gt;\n  select(`NTD ID`, Agency, avg_length) |&gt;\n  rename(`Average Length`=avg_length)\nkable(avg_length,caption=\"Average Length of a Trip on MTA NYC\")\n\n\n\nAverage Length of a Trip on MTA NYC\n\n\nNTD ID\nAgency\nAverage Length\n\n\n\n\n20008\nMTA New York City Transit\n3.644089\n\n\n\n\n\nCode\n# Which transit service in NYC has the longest average trip length?\nlongest_transit &lt;- NTD_SERVICE |&gt;\n  filter(City %in% c(\"Brooklyn\", \"New York\", \"Staten Island\")) |&gt;\n  mutate(avg_trip_length=MILES/UPT) |&gt;\n  slice_max(avg_trip_length, n=1) |&gt;\n  rename(`Average Trip Length`=avg_trip_length)\nkable(longest_transit, caption=\"Transit Service in NYC with the longest average trip length\")\n\n\n\nTransit Service in NYC with the longest average trip length\n\n\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nUPT\nMILES\nNTD ID\nAverage Trip Length\n\n\n\n\nMTA Long Island Rail Road\nNew York\nNY\n83835706\n2033685836\n20100\n24.25799\n\n\n\n\n\nCode\n# Which state has the fewest total miles travelled by public transit?\nstate_fewest_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(total_miles=sum(MILES)) |&gt;\n  slice_min(total_miles, n=1)\nkable(state_fewest_miles, caption=\"State with the Fewest Total Miles\")\n\n\n\nState with the Fewest Total Miles\n\n\nState\ntotal_miles\n\n\n\n\nNH\n3749892\n\n\n\n\n\nCode\n# Are all states represented in this data? If no, which ones are missing? The \n# state.name and state.abb objects we used above may be useful here.\nmissing_states &lt;- EIA_SEP_REPORT |&gt;\n  anti_join(NTD_SERVICE, join_by(\"abbreviation\"==\"State\")) |&gt;\n  select(state) |&gt;\n  rename(`Missing States`=state)\nkable(missing_states, caption=\"States that are not represented\")\n\n\n\nStates that are not represented\n\n\nMissing States\n\n\n\n\nArizona\n\n\nArkansas\n\n\nCalifornia\n\n\nColorado\n\n\nHawaii\n\n\nIowa\n\n\nKansas\n\n\nLouisiana\n\n\nMissouri\n\n\nMontana\n\n\nNebraska\n\n\nNevada\n\n\nNew Mexico\n\n\nNorth Dakota\n\n\nOklahoma\n\n\nSouth Dakota\n\n\nTexas\n\n\nUtah\n\n\nWyoming\n\n\n\n\n\nCode\n# join the three tables\nagency_mode &lt;- NTD_SERVICE |&gt;\n  inner_join(NTD_ENERGY |&gt;\n               select(-`Bunker Fuel`,-Ethanol, -Methonal,-Kerosene), \n             join_by(`NTD ID`==`NTD ID`)\n             ) |&gt;\n  inner_join(EIA_SEP_REPORT, join_by(State == abbreviation)) |&gt;\n  select(-`Agency Name`, -State) |&gt;\n  rename(\n    agency=Agency,\n    mode=Mode,\n    biodiesel=`Bio-Diesel`,\n    cnatgas=`C Natural Gas`,\n    diesel=`Diesel Fuel`,\n    ebat=`Electric Battery`,\n    epro=`Electric Propulsion`,\n    gas=Gasoline,\n    hy=Hydrogen,\n    lnatgas=`Liquified Nat Gas`,\n    lpetgas=`Liquified Petroleum Gas`,\n    co2_mwh=CO2_MWh\n  ) |&gt;\n  group_by(agency, mode)\n\n# co2_vol_mass &lt;- read_xlsx(\"data/mp02/co2_vol_mass.xlsx\")\n\nagency_mode &lt;- agency_mode |&gt; \n  mutate(total_co2_emissions=\n           biodiesel*22.45+\n           cnatgas*0.134/1000*120.85+\n           diesel*22.45+\n           ebat/1000*co2_mwh+\n           epro/1000*co2_mwh+\n           gas*20.86+\n           lnatgas*0.134/1000*120.85+\n           lpetgas*12.68\n         ) \n\nagency_mode2 &lt;- agency_mode |&gt;\n  select(-biodiesel,-cnatgas,-diesel,\n         -ebat, -epro, -gas,\n         -hy, -lnatgas, -lpetgas,\n         -co2_mwh, -primary_source, -electricity_price_MWh,\n         -generation_MWh\n         ) |&gt;\n  group_by(`NTD ID`) |&gt;\n  mutate(tot_emissions_agency=sum(total_co2_emissions))\n\nagency_mode3 &lt;- agency_mode2 |&gt;\n  select(`NTD ID`, agency, City, \n         state, UPT, MILES, \n         tot_emissions_agency) |&gt;\n  mutate(per_upt = tot_emissions_agency/UPT,\n         per_passenger_mile = per_upt/MILES) |&gt;\n  ungroup()\n\n# lowest emissions per upt\nlowest_emissions_upt &lt;- agency_mode3 |&gt;\n  select(`NTD ID`, agency, City,\n         state, per_upt) |&gt;\n  slice_min(per_upt, n=1)\nkable(lowest_emissions_upt, caption=\"Agency with the lowest co2 emissions per upt\")\n\n\n\nAgency with the lowest co2 emissions per upt\n\n\n\n\n\n\n\n\n\nNTD ID\nagency\nCity\nstate\nper_upt\n\n\n\n\n23\nCity of Seattle, dba: Seattle Center Monorail\nSeattle\nWashington\n0.0691556\n\n\n\n\n\nCode\n# lowest emissions per passenger mile\nlowest_emi_upt_mile &lt;- agency_mode3 |&gt;\n  select(`NTD ID`, agency, City,\n         state, per_passenger_mile) |&gt;\n  mutate(per_pm_mg=per_passenger_mile*453.592*1000) |&gt; # CO2 emissions in miligrams\n  slice_min(per_pm_mg, n=1, with_ties = FALSE)\nkable(lowest_emi_upt_mile, caption=\"Agency with the lowest emissions(miligram) per passenger mile\")\n\n\n\nAgency with the lowest emissions(miligram) per passenger mile\n\n\n\n\n\n\n\n\n\n\nNTD ID\nagency\nCity\nstate\nper_passenger_mile\nper_pm_mg\n\n\n\n\n20008\nMTA New York City Transit\nBrooklyn\nNew York\n0\n2.8e-05\n\n\n\n\n\nCode\n# finding the values that will define small, medium, and large sized agencies. \nsize_1 &lt;- quantile(agency_mode3 |&gt; select(UPT) |&gt; unique() |&gt; pull(UPT), 0.1)\nsize_2 &lt;- quantile(agency_mode3 |&gt; select(UPT) |&gt; unique() |&gt; pull(UPT), 0.4)\nsize_3 &lt;- quantile(agency_mode3 |&gt; select(UPT) |&gt; unique() |&gt; pull(UPT), 0.7)\n\nagency_mode4 &lt;- agency_mode3 |&gt;\n  filter(UPT &gt; size_1) |&gt;\n  mutate(size = case_when(\n    UPT &lt;= size_2 ~ \"Small\",\n    UPT &gt; size_2 & UPT &lt;= size_3 ~ \"Medium\",\n    UPT &gt; size_3 ~ \"Large\")) |&gt;\n  mutate(per_pm_mg=per_passenger_mile*453.592*1000)\n\n# greenest  agencies with different sizes\ngreenest_agencies &lt;- agency_mode4 |&gt;\n  select(size, agency, City,state, per_pm_mg) |&gt;\n  unique() |&gt;\n  group_by(size) |&gt;\n  slice_min(per_pm_mg, n=1)\nkable(greenest_agencies,caption=\"The greenest agencies by size\")\n\n\n\nThe greenest agencies by size\n\n\n\n\n\n\n\n\n\nsize\nagency\nCity\nstate\nper_pm_mg\n\n\n\n\nLarge\nMTA New York City Transit\nBrooklyn\nNew York\n0.0000280\n\n\nMedium\nBirmingham-Jefferson County Transit Authority\nBirmingham\nAlabama\n0.0139570\n\n\nSmall\nCity of Albany , dba: Albany Transit System\nAlbany\nGeorgia\n0.0251236\n\n\n\n\n\nCode\n# most emissions avioded\nmost_emissions_avoided &lt;- agency_mode4 |&gt;\n  unique() |&gt;\n  mutate(emission_avoided=MILES/49*20.86-tot_emissions_agency) |&gt;\n  group_by(size) |&gt;\n  slice_max(emission_avoided,n=1) |&gt;\n  select(size, agency, City, state, emission_avoided)\nkable(most_emissions_avoided, caption=\"Agencies with the most emissions avioded\")\n\n\n\nAgencies with the most emissions avioded\n\n\n\n\n\n\n\n\n\nsize\nagency\nCity\nstate\nemission_avoided\n\n\n\n\nLarge\nMTA New York City Transit\nBrooklyn\nNew York\n2523693827\n\n\nMedium\nHudson Transit Lines, Inc., dba: Short Line\nMahwah\nNew Jersey\n17934431\n\n\nSmall\nHampton Jitney, Inc.\nCalverton\nNew York\n11184078\n\n\n\n\n\nCode\n# highest percentage of electrification\nhighest_electrification &lt;- agency_mode |&gt;\n  mutate(elec=ebat+epro, other=biodiesel+cnatgas+\n           diesel+gas+hy+lnatgas+lpetgas) |&gt;\n  group_by(agency) |&gt;\n  mutate(tot_elec=sum(elec), tot_other=sum(other)) |&gt;\n  unique() |&gt;\n  mutate(elec_level=tot_elec/tot_other)\n\nhighest_electrification_agency &lt;- highest_electrification |&gt;\n  inner_join(agency_mode4, join_by(\"NTD ID\"==\"NTD ID\")) |&gt;\n  group_by(size) |&gt;\n  filter(elec_level!=Inf) |&gt;\n  slice_max(elec_level,n=1)|&gt;\n  select(size,agency.x,City.x,state.x,elec_level) |&gt;\n  unique()\nkable(highest_electrification_agency, caption=\"Agencies with the highest electrification level\")\n\n\n\nAgencies with the highest electrification level\n\n\n\n\n\n\n\n\n\nsize\nagency.x\nCity.x\nstate.x\nelec_level\n\n\n\n\nLarge\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet\nPortland\nOregon\n117.43770\n\n\nMedium\nUniversity of Georgia, dba: Transportation and Parking Services\nAthens\nGeorgia\n10.36036\n\n\nSmall\nConnecticut Department of Transportation\nNewington\nConnecticut\n14.84685\n\n\n\n\n\nCode\n# worst of green agency\nworst_green_agency &lt;- highest_electrification |&gt;\n  inner_join(agency_mode4, join_by(\"NTD ID\"==\"NTD ID\")) |&gt;\n  group_by(size) |&gt;\n  filter(elec_level==0) |&gt;\n  slice_max(total_co2_emissions,n=1)|&gt;\n  select(size,agency.x,City.x,state.x,total_co2_emissions) |&gt;\n  unique()\nkable(worst_green_agency, caption=\"Agencies that are most 'un-green'\")\n\n\n\nAgencies that are most ‘un-green’\n\n\n\n\n\n\n\n\n\nsize\nagency.x\nCity.x\nstate.x\ntotal_co2_emissions\n\n\n\n\nLarge\nWashington State Ferries\nOlympia\nWashington\n344220193\n\n\nMedium\nSeaStreak, LLC\nAtlantic Highlands\nNew Jersey\n66376478\n\n\nSmall\nAlaska Railroad Corporation\nAnchorage\nAlaska\n21854963\n\n\n\n\n\nCode\n# visualization\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Example data \ngreenest_agencies &lt;- data.frame(\n  size = c('Small', 'Medium', 'Large'),\n  agency = c('Agency A', 'Agency B', 'Agency C'),\n  City = c('City X', 'City Y', 'City Z'),\n  state = c('State 1', 'State 2', 'State 3'),\n  per_pm_mg = c(0.025, 0.013, 0.000)\n)\n\n# Create the bar plot\nggplot(greenest_agencies, aes(x = size, y = per_pm_mg, fill = size)) +\n  geom_bar(stat = 'identity', show.legend = FALSE) +\n  geom_text(aes(label = round(per_pm_mg, 3)), vjust = -0.3, size = 5) +\n  scale_fill_brewer(palette = \"Greens\") +\n  labs(\n    title = 'Greenest Agencies by Size',\n    x = 'Agency Size',\n    y = 'Green Score (per pm mg)'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n#another vis\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n# Here is a sample structure\nmost_emissions_avoided &lt;- data.frame(\n  size = c('Small', 'Medium', 'Large'),\n  agency = c('Agency A', 'Agency B', 'Agency C'),\n  City = c('City X', 'City Y', 'City Z'),\n  state = c('State 1', 'State 2', 'State 3'),\n  emission_avoided = c(11000000, 17000000, 2523000000)\n)\n\n# Create a bar plot for the most emissions avoided by agency size\nggplot(most_emissions_avoided, aes(x = size, y = emission_avoided, fill = size)) +\n  geom_bar(stat = 'identity', show.legend = FALSE) +\n  geom_text(aes(label = round(emission_avoided, 0)), vjust = -0.3, size = 5) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(\n    title = 'Most Emissions Avoided by Agencies',\n    x = 'Agency Size',\n    y = 'Emissions Avoided (in units)'\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "My test doc",
    "section": "",
    "text": "this is italic and this is bold.\nI want link to Youtube."
  },
  {
    "objectID": "test.html#quarto",
    "href": "test.html#quarto",
    "title": "My test doc",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "test.html#running-code",
    "href": "test.html#running-code",
    "title": "My test doc",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "test.html#big-section",
    "href": "test.html#big-section",
    "title": "My test doc",
    "section": "Big Section",
    "text": "Big Section\ntext\n\nLittle Section\nlittle Text\n\\(\\frac{2}{3}\\) is more than \\(\\frac{1}{2}\\)\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "test_pre.html#quarto",
    "href": "test_pre.html#quarto",
    "title": "Test Presentation",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "test_pre.html#bullets",
    "href": "test_pre.html#bullets",
    "title": "Test Presentation",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "test_pre.html#code",
    "href": "test_pre.html#code",
    "title": "Test Presentation",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "test_pre.html#this-is-a-header",
    "href": "test_pre.html#this-is-a-header",
    "title": "Test Presentation",
    "section": "This is a header",
    "text": "This is a header\nsomething"
  },
  {
    "objectID": "test_pre.html#this-is-another-header",
    "href": "test_pre.html#this-is-another-header",
    "title": "Test Presentation",
    "section": "This is another header",
    "text": "This is another header"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "MP03",
    "section": "",
    "text": "This is the anchor song used to create the ultimate playlist"
  },
  {
    "objectID": "mp03.html#internets-best-playlist",
    "href": "mp03.html#internets-best-playlist",
    "title": "MP03",
    "section": "",
    "text": "This is the anchor song used to create the ultimate playlist"
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Creating the Ultimate Playlist",
    "section": "Introduction",
    "text": "Introduction\nIn this project, we will explore the world of music analytics aiming to create The Ultimate Playlist. In detail, we will use two data exports available from Spotify to identify the most popular songs on the platform, and also the characteristics of those songs. By using this data, we will create the ultimate playlist."
  },
  {
    "objectID": "mp03.html#conclusion",
    "href": "mp03.html#conclusion",
    "title": "Creating the Ultimate Playlist",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "mp03.html#data-acquisition",
    "href": "mp03.html#data-acquisition",
    "title": "Creating the Ultimate Playlist",
    "section": "Data Acquisition",
    "text": "Data Acquisition\nThe two Spotify data exports we will use are a data set of songs and their characteristics, and an export of user-created playlists.\n\nSong Characteristics\nFirst, we write a function called load_songs to download the Spotify song dataset from https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv and then read it into R.\n\n\nCode\n# Ensure packages are loaded\nlibrary(dplyr)\n\n# Task 1: Song Characteristics Dataset\nload_songs &lt;- function() {\n  # Define directory and file paths\n  dir_path &lt;- \"data/mp03\"\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name)\n  url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # Download the file if it doesn't exist\n  if (!file.exists(file_path)) {\n    download.file(url, destfile = file_path, method = \"auto\")\n  }\n  \n  # Read CSV into a data frame\n  songs_df &lt;- read.csv(file_path, stringsAsFactors = FALSE)\n  \n  # Return a well-formatted data frame\n  return(songs_df)\n}\n\n\nThe artists column of this data set is a bit oddly formatted: it contains multiple artists in a “list-type” format. We use the following code to split the artists across multiple rows.\n\n\nCode\n# Split the artists \nlibrary(tidyr)\nlibrary(stringr)\nclean_artist_string &lt;- function(x){\n  str_replace_all(x, \"\\\\['\", \"\") |&gt; \n    str_replace_all(\"'\\\\]\", \"\") \n}\nSONGS &lt;- load_songs()\nSONGS_CLEAN &lt;- SONGS |&gt; \n  separate_longer_delim(artists, \"', '\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\n\n\n\nPlaylists\nNext, we will download the Spotify Playlist dataset from spotify_million_playlist_dataset by writting a function called load_playlists to download all files from this repository (data1 directory). We store them locally, read them into R, and then concatenate them into a list object.\n\n\nCode\n# Task 2: Playlist dataset\n# Write a function to download playlists\nload_playlists &lt;- function(data_dir = \"spotify_data\", base_url = \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\", \n                           slices = seq(0, 999000, by = 1000)) {\n  if (!dir.exists(data_dir)) {\n    dir.create(data_dir)\n  }\n  \n  playlists &lt;- list()\n  \n  for (start in slices) {\n    end &lt;- start + 999\n    file_name &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n    file_path &lt;- file.path(data_dir, file_name)\n    file_url &lt;- paste0(base_url, file_name)\n    \n    # Check if file exists on the server\n    response &lt;- tryCatch({\n      httr::HEAD(file_url) # Send a HEAD request to check if the file exists\n    }, error = function(e) {\n      return(NULL)\n    })\n    \n    if (is.null(response) || response$status_code != 200) {\n      message(sprintf(\"File not found: %s\", file_name))\n      next  # Skip this file if it doesn't exist\n    }\n    \n    # Download the file if not already downloaded\n    if (!file.exists(file_path)) {\n      message(sprintf(\"Downloading %s...\", file_name))\n      download.file(file_url, destfile = file_path, mode = \"wb\")\n    } else {\n      message(sprintf(\"Already downloaded: %s\", file_name))\n    }\n    \n    # Read JSON file and extract playlists\n    json_data &lt;- jsonlite::fromJSON(file_path)\n    playlists[[file_name]] &lt;- json_data$playlists\n  }\n  \n  return(playlists)\n}\n\nall_playlists &lt;- load_playlists()\n\n\nThen, by using functions from the tidyr, purrr, and dplyr packages, we convert the playlist data into the rectangular format as a table of the following columns:\n\nPlaylist Name (playlist_name)\nPlaylist ID (playlist_id)\nPlaylist Position (playlist_position)\nPlaylist Followers (playlist_followers)\nArtist Name (artist_name)\nArtist ID (artist_id)\nTrack Name (track_name)\nTrack ID (track_id)\nAlbum Name (album_name)\nAlbum ID (album_id)\nDuration (duration)\n\nwhere each row is one “track” from a playlist. We also clean up the ID columns by striping the spotify:type: prefix.\n\n\nCode\n# Task 3: Rectangle the playlist data\nlibrary(tidyverse)\n\n# Flatten all playlists into a single data frame\ntidy_playlists &lt;- all_playlists |&gt;\n  # Combine all playlist chunks\n  bind_rows() |&gt;\n  # Use purrr::map_df to unnest the 'tracks' list-column\n  mutate(playlist_name = name,\n         playlist_id = pid,\n         playlist_position = row_number(),\n         playlist_followers = num_followers) |&gt;\n  select(playlist_name, playlist_id, playlist_position, playlist_followers, tracks) |&gt;\n  unnest(tracks, keep_empty = TRUE) |&gt;\n  transmute(\n    playlist_name,\n    playlist_id,\n    playlist_position,\n    playlist_followers,\n    artist_name = artist_name,\n    artist_id = artist_uri,\n    track_name = track_name,\n    track_id = track_uri,\n    album_name = album_name,\n    album_id = album_uri,\n    duration = duration_ms\n  )\n\n# strip the predix in id columns\nstrip_spotify_prefix &lt;- function(x){\n  library(stringr)\n  str_extract(x, \".*:.*:(.*)\", group=1)\n}\nPLAYLIST &lt;- tidy_playlists |&gt;\n  mutate(artist_id = strip_spotify_prefix(artist_id),\n         track_id = strip_spotify_prefix(track_id),\n         album_id = strip_spotify_prefix(album_id))"
  },
  {
    "objectID": "mp03.html#initial-exploration",
    "href": "mp03.html#initial-exploration",
    "title": "Creating the Ultimate Playlist",
    "section": "Initial Exploration",
    "text": "Initial Exploration\nSince our data is imported and cleaned, let’s do some initial exploration with the following questions.\n\nHow many distinct tracks and artists are represented in the playlist data?\n\n\n\nCode\n# Task 4: initial exploration\n# How many distinct tracks and artists are represented in the playlist data?\n\n# Count distinct tracks\ndistinct_tracks &lt;- PLAYLIST |&gt;\n  distinct(track_id) |&gt;\n  nrow()\n\n# Count distinct artists\ndistinct_artists &lt;- PLAYLIST |&gt;\n  distinct(artist_id) |&gt;\n  nrow()\n\n# Output results\ncat(\"Distinct Tracks:\", distinct_tracks, \"\\n\")\n\n\nDistinct Tracks: 1200590 \n\n\nCode\ncat(\"Distinct Artists:\", distinct_artists, \"\\n\")\n\n\nDistinct Artists: 173604 \n\n\n\nWhat are the 5 most popular tracks in the playlist data?\n\n\n\nCode\n# What are the 5 most popular tracks in the playlist data?\nmost_popular_tracks_5 &lt;- PLAYLIST |&gt; group_by(track_id, track_name) |&gt; \n  summarize(n_track = n(), .groups='drop') |&gt;\n  slice_max(n_track, n=5)\nlibrary(knitr)\nkable(most_popular_tracks_5, cap='5 most popular tracks in the playlist data')\n\n\n\n5 most popular tracks in the playlist data\n\n\ntrack_id\ntrack_name\nn_track\n\n\n\n\n7KXjTSCq5nL1LoYtL7XAwS\nHUMBLE.\n13314\n\n\n1xznGGDReH1oQq0xzbwXa3\nOne Dance\n12179\n\n\n7yyRTcZmCiyzzJlNzGC9Ol\nBroccoli (feat. Lil Yachty)\n11845\n\n\n7BKLCZ1jbUBVqRi2FVlTVw\nCloser\n11656\n\n\n3a1lNhkSLSkpJE4MSHpDu9\nCongratulations\n11310\n\n\n\n\n\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\n\nCode\n# What is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\nmost_popular_missing &lt;- PLAYLIST |&gt;\n  group_by(track_id, track_name) |&gt;\n  summarize(n_track = n(), .groups='drop') |&gt;\n  anti_join(SONGS_CLEAN, by=c('track_id'='id')) |&gt;\n  slice_max(n_track, n=1)\nkable(most_popular_missing, cap='Most popular track in the playlist data that does not have a corresponding entry in the song characteristics data')\n\n\n\nMost popular track in the playlist data that does not have a corresponding entry in the song characteristics data\n\n\ntrack_id\ntrack_name\nn_track\n\n\n\n\n1xznGGDReH1oQq0xzbwXa3\nOne Dance\n12179\n\n\n\n\n\n\nAccording to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\n\n\n\nCode\n# According to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\nname_danceable &lt;- SONGS_CLEAN |&gt; \n  slice_max(danceability, n=1) |&gt;\n  pull(name)\n\nn_id &lt;- PLAYLIST |&gt;\n  distinct(playlist_id) |&gt;\n  nrow()\n  \nmost_danceable_track &lt;- PLAYLIST |&gt;\n  filter(track_name == name_danceable) |&gt;\n  group_by(track_name) |&gt;\n  summarize(freqency = n() / n_id )\nkable(most_danceable_track, cap='Most danceable track and how often it appears in a playlist')\n\n\n\nMost danceable track and how often it appears in a playlist\n\n\ntrack_name\nfreqency\n\n\n\n\nFunky Cold Medina\n0.0007535\n\n\n\n\n\n\nWhich playlist has the longest average track length?\n\n\n\nCode\n# Which playlist has the longest average track length?\nplaylist_longest_track_length &lt;- PLAYLIST |&gt;\n  group_by(playlist_id, playlist_name) |&gt;\n  summarize(avg_track_len = mean(duration), .groups='drop') |&gt;\n  slice_max(avg_track_len, n=1)\nkable(playlist_longest_track_length, cap='Playlist with the longest average track length')\n\n\n\nPlaylist with the longest average track length\n\n\nplaylist_id\nplaylist_name\navg_track_len\n\n\n\n\n462471\nMixes\n3868511\n\n\n\n\n\n\nWhat is the most popular playlist on Spotify?\n\n\n\nCode\n# What is the most popular playlist on Spotify?\nmost_popular_playlist &lt;- PLAYLIST |&gt;\n  slice_max(playlist_followers, n=1) |&gt;\n  distinct(playlist_id, playlist_name)\nkable(most_popular_playlist, cap='Most popular playlist on Spotify')\n\n\n\nMost popular playlist on Spotify\n\n\nplaylist_id\nplaylist_name\n\n\n\n\n746359\nBreaking Bad"
  },
  {
    "objectID": "mp03.html#identifying-characteristics-of-popular-songs",
    "href": "mp03.html#identifying-characteristics-of-popular-songs",
    "title": "Creating the Ultimate Playlist",
    "section": "Identifying Characteristics of Popular Songs",
    "text": "Identifying Characteristics of Popular Songs\nNow, we will visually explore our data by answering the following questions using one or more visualizations.\n\nIs the popularity column correlated with the number of playlist appearances? If so, to what degree?\n\n\n\nCode\n# Task 5: visually identifying characteristics of pupular songs\n# inner join to combine datasets\n# Combine playlist and song characteristics using an inner join\ncombined_data &lt;- PLAYLIST |&gt;\n  inner_join(SONGS_CLEAN, by = c(\"track_id\" = \"id\"))\n# Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n\n# Count how many times each track appears in the playlist\nplaylist_counts &lt;- PLAYLIST |&gt;\n  group_by(track_id) |&gt;\n  summarize(playlist_appearances = n(), .groups = \"drop\")\n\n# Join with song characteristics to get popularity\npopularity_data &lt;- playlist_counts |&gt;\n  inner_join(SONGS_CLEAN, by = c(\"track_id\" = \"id\")) |&gt;\n  select(track_id, playlist_appearances, popularity) |&gt;\n  filter(playlist_appearances &lt; 5000)\n\n# Create scatter plot with linear regression line\nggplot(popularity_data, \n       aes(x = playlist_appearances, \n                            y = popularity)) +\n  geom_point(color = \"#0072B2\", alpha = 0.2, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#D55E00\", linetype = \"dashed\") +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Relationship Between Playlist Appearances and Track Popularity\",\n    x = \"Number of Playlist Appearances\",\n    y = \"Popularity Score\",\n    caption = \"Dashed line represents linear regression fit\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Compute Pearson correlation\ncorrelation &lt;- cor(popularity_data$playlist_appearances, popularity_data$popularity)\ncat(\"Correlation between playlist appearances and popularity:\", round(correlation, 3))\n\n\nCorrelation between playlist appearances and popularity: 0.448\n\n\nA positive correlation suggests that more frequently included tracks tend to be more popular. The correlation coefficient (Pearson’s r) tells us the strength is moderate.\n~0.1–0.3 = weak\n~0.3–0.5 = moderate\n~0.5–1.0 = strong\nFor the following questions, we select popularity = 60 as a threshold that defines a “popular” song.\n\nIn what year were the most popular songs released?\n\n\n\nCode\n# In what year were the most popular songs released?\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\nsongs_per_year &lt;- SONGS_CLEAN |&gt;\n  filter(popularity &gt;= 60) |&gt;\n  select(id, name, year) |&gt;\n  group_by(year) |&gt;\n  summarize(num_songs_released = n(), .groups='drop')\n\n# Create static ggplot\np &lt;- ggplot(songs_per_year, aes(x = factor(year), y = num_songs_released)) +\n  geom_bar(stat = \"identity\", fill = \"#56B4E9\", color = \"#0072B2\", alpha = 0.7) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Number of Popular Songs Released by Year\",\n    x = \"Release Year\",\n    y = \"Number of Popular Songs Released\",\n    caption = \"Bar plot showing the number of popular songs released each year\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  scale_x_discrete(breaks = function(x) x[as.numeric(x) %% 5 == 0])  # Show every 5th year\n\n# Convert ggplot to interactive plotly\nggplotly(p)\n\n\n\n\n\n\nCode\n## Find the year(s) with the highest number of popular song releases\nmost_songs_year &lt;- songs_per_year |&gt;\n  filter(num_songs_released == max(num_songs_released)) |&gt;\n  pull(year)\ncat(\"The year with the most popular songs released is:\", most_songs_year)\n\n\nThe year with the most popular songs released is: 2019\n\n\n\nIn what year did danceability peak?\n\n\n\nCode\n# In what year did danceability peak?\npopular_songs &lt;- SONGS_CLEAN |&gt;\n  filter(popularity &gt;= 60)\n\ndan_by_year &lt;- popular_songs |&gt;\n  group_by(year) |&gt;\n  summarize(avg_dan = mean(danceability), .groups='drop')\n\n# Line plot of average danceability by year\np_2 &lt;- ggplot(dan_by_year, aes(x = year, y = avg_dan)) +\n  geom_line(color = \"#009E73\", size = 1.3) +\n  geom_point(color = \"#009E73\", size = 2.5) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Average Danceability of Songs by Release Year\",\n    x = \"Release Year\",\n    y = \"Average Danceability\",\n    caption = \"Danceability score ranges from 0 (not danceable) to 1 (very danceable)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nggplotly(p_2)\n\n\n\n\n\n\nCode\n# Find year(s) with peak danceability\npeak_year &lt;- dan_by_year |&gt;\n  filter(avg_dan == max(avg_dan)) |&gt;\n  pull(year)\n\ncat(\"Danceability peaked in the year:\", peak_year)\n\n\nDanceability peaked in the year: 1955\n\n\n\nWhich decade is most represented on user playlists?\n\n\n\nCode\n# Which decade is most represented on user playlists? (The integer division (%/%) operator may be useful for computing decades from years.)\n# Add a decade column\ndecade_counts &lt;- combined_data |&gt;\n  mutate(decade = (year %/% 10) * 10) |&gt;\n  group_by(decade) |&gt; \n  summarise(song_count = n(), .groups = \"drop\")\n\nmost_common_decade &lt;- decade_counts |&gt;\n  filter(song_count == max(song_count)) |&gt;\n  pull(decade)\n\ncat(\"The most represented decade on user playlists is:\", most_common_decade)\n\n\nThe most represented decade on user playlists is: 2010\n\n\nCode\n# Add a column to control bar color\ndecade_counts &lt;- decade_counts |&gt;\n  mutate(\n    highlight = ifelse(decade == most_common_decade, \"Most Popular\", \"Other\")\n  )\n\np_3 &lt;- ggplot(decade_counts, aes(x = factor(decade), y = song_count/1000, fill = highlight)) +\n  geom_bar(stat = \"identity\", color = \"black\", width = 0.7) +\n  scale_fill_manual(values = c(\"Most Popular\" = \"#D55E00\", \"Other\" = \"#0072B2\")) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Number of Songs on Playlists by Decade\",\n    x = \"Decade\",\n    y = \"Number of Songs (thousand)\",\n    fill = \"\",\n    caption = \"The most popular decade is highlighted\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\"\n  )\nggplotly(p_3)\n\n\n\n\n\n\n\nCreate a plot of key frequency among songs.\n\n\n\nCode\n# Create a plot of key frequency among songs. Because musical keys exist in a ‘cycle’, your plot should use polar (circular) coordinates.\n# Map integer keys to note names\nkey_labels &lt;- c(\n  \"C\", \"C♯/D\", \"D\", \"D♯/E\", \"E\", \"F\",\n  \"F♯/G\", \"G\", \"G♯/A\", \"A\", \"A♯/B\", \"B\"\n)\n\n# Add a key name column\nkey_data &lt;- SONGS_CLEAN |&gt;\n  mutate(key_name = factor(key_labels[key + 1], levels = key_labels))  # +1 for 0-indexing\nkey_counts &lt;- key_data |&gt;\n  group_by(key_name) |&gt;\n  summarise(song_count = n(), .groups = \"drop\")\n\nggplot(key_counts, aes(x = key_name, y = song_count, fill = key_name)) +\n  geom_bar(stat = \"identity\", color = \"black\", width = 1) +\n  coord_polar(start = -pi/12) +  # rotate so C is at the top\n  scale_fill_manual(values = RColorBrewer::brewer.pal(12, \"Paired\")) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Distribution of Musical Keys Among Songs\",\n    x = \"\",\n    y = \"\",\n    caption = \"Keys are mapped in circular order (C to B)\"\n  ) +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\nWhat are the most popular track lengths?\n\n\n\nCode\n# What are the most popular track lengths? (Are short tracks, long tracks, or something in between most commonly included in user playlists?)\n# Convert milliseconds to minutes\ntrack_lengths &lt;- combined_data |&gt;\n  filter(!is.na(duration_ms)) |&gt;\n  mutate(duration_min = duration_ms / 60000) |&gt;\n  filter(duration_min &lt;= 10)\n\n# Get the most frequent duration range\nmode_duration &lt;- track_lengths |&gt;\n  mutate(duration_rounded = round(duration_min, 1)) |&gt;\n  count(duration_rounded) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1) |&gt;\n  pull(duration_rounded)\n\n# Add a line to highlight the peak\np_5 &lt;- ggplot(track_lengths, aes(x = duration_min)) +\n  geom_histogram(binwidth = 0.25, fill = \"#56B4E9\", color = \"black\", alpha = 0.8) +\n  geom_vline(xintercept = mode_duration, color = \"#D55E00\", linetype = \"dashed\", linewidth = 1.2) +\n  annotate(\"text\", x = mode_duration, y = 1560000, label = paste(\"Most Common:\", mode_duration, \"min\"), \n           vjust = -0.5, hjust = 0.1, color = \"#D55E00\", fontface = \"bold\", size = 5) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Distribution of Track Durations in Playlists\",\n    x = \"Track Length (minutes)\",\n    y = \"Number of Tracks\",\n    caption = \"Dashed line indicates the most common track length\"\n  )\nggplotly(p_5)\n\n\n\n\n\n\nWe will also pose and visually answer two more other exploratory questions here.\n\nWhat is the relationship between track popularity and track length?\nHow has the danceability of songs changed over time?\n\n\n\nCode\n# Plotting the relationship between track popularity and length\npopularity_length &lt;- SONGS_CLEAN |&gt;\n  mutate(duration_min = duration_ms / 60000) |&gt;\n  filter(duration_min &lt;= 10)\n\nggplot(popularity_length, aes(x = duration_min, y = popularity)) +\n  geom_point(alpha = 0.05, color = \"#56B4E9\") +\n  geom_smooth(method = \"lm\", color = \"#D55E00\", linetype = \"dashed\", size = 1) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Track Popularity vs. Track Length\",\n    x = \"Track Length (min)\",\n    y = \"Track Popularity\",\n    caption = \"Scatter plot with linear trendline\"\n  ) +\n  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\n# Compute Pearson correlation\ncorr_pop_len &lt;- cor(popularity_length$duration_min, popularity_length$popularity)\ncat(\"Correlation between track length and popularity:\", round(corr_pop_len, 3))\n\n\nCorrelation between track length and popularity: 0.119\n\n\nCode\n# How has the danceability of songs changed over time?\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gganimate)\n\n# Prepare the data: average danceability by year\ndanceability_trend &lt;- SONGS_CLEAN |&gt;\n  group_by(year) |&gt;\n  summarise(avg_danceability = mean(danceability), .groups = \"drop\") |&gt;\n  arrange(year)\n\n# Animated line chart\nggplot(danceability_trend, aes(x = year, y = avg_danceability)) +\n  geom_line(color = \"#0072B2\", linewidth = 1.5) +\n  geom_point(color = \"#D55E00\", size = 3) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Danceability of Songs Over Time\",\n    subtitle = \"Year: {frame_along}\",\n    x = \"Release Year\",\n    y = \"Average Danceability\"\n  ) +\n  transition_reveal(year)"
  },
  {
    "objectID": "mp03.html#building-a-playlist-from-anchor-songs",
    "href": "mp03.html#building-a-playlist-from-anchor-songs",
    "title": "Creating the Ultimate Playlist",
    "section": "Building a Playlist from Anchor Songs",
    "text": "Building a Playlist from Anchor Songs\nBefore building our playlist, we pick Adele’s “Someone Like You” as our “anchor” song. Let’s check it out here. \nWe will find songs that work well in a playlist with the anchor song using the following four heuristics and one of my own. We will identify 20 candidates, 8 of which are not “popular” based on the threshold we set above (popularity &lt;= 60).\n\nWhat other songs commonly appear on playlists along side this song?\nWhat other songs are in the same key and have a similar tempo? (This makes it easy for a skilled DJ to transition from one song to the next.)\nWhat other songs were released by the same artist?\nWhat other songs were released in the same year and have similar levels of acousticness, danceability, etc.?\nWhat other songs have similar mood (energy + valence)?\n\n\n\nCode\n# Task 6: finding related songs\n# Load libraries\nlibrary(dplyr)\nlibrary(readr)\n\n# Define anchor song ID\nanchor_id &lt;- \"4kflIGfjdZJW4ot2ioixTB\"\n\n# Get anchor song details\nanchor_song &lt;- SONGS_CLEAN |&gt; filter(id == anchor_id)\n\n# Define Similar numeric range filter\nrange_filter &lt;- function(df, col, val, tol) {\n  df |&gt; filter(abs(.data[[col]] - val) &lt;= tol)\n}\n\n# Heuristic 1: Co-occurrence in playlists\nco_songs &lt;- PLAYLIST |&gt;\n  filter(playlist_id %in% (\n    PLAYLIST |&gt;\n      filter(track_id == anchor_id) |&gt;\n      pull(playlist_id)\n  )) |&gt;\n  filter(track_id != anchor_id) |&gt;\n  count(track_id, sort = TRUE) |&gt;\n  top_n(10, wt = n) |&gt;\n  pull(track_id)\n\n# Heuristic 2: Same key & similar tempo (±5%)\ntempo_tol &lt;- 0.05 * anchor_song$tempo\nkey_tempo_match &lt;- SONGS_CLEAN |&gt;\n  filter(key == anchor_song$key) |&gt;\n  range_filter(\"tempo\", anchor_song$tempo, tempo_tol) |&gt;\n  pull(id)\n\n# Heuristic 3: Same artist\nsame_artist &lt;- SONGS_CLEAN |&gt;\n  filter(artist == anchor_song$artist, id != anchor_id) |&gt;\n  pull(id)\n\n# Heuristic 4: Same year & similar features\nsimilar_year &lt;- SONGS_CLEAN |&gt;\n  filter(year == anchor_song$year) |&gt;\n  range_filter(\"acousticness\", anchor_song$acousticness, 0.05) |&gt;\n  range_filter(\"danceability\", anchor_song$danceability, 0.05) |&gt;\n  range_filter(\"energy\", anchor_song$energy, 0.05) |&gt;\n  pull(id)\n\n# Heuristic 5: Similar mood (energy + valence)\nmood_match &lt;- SONGS_CLEAN |&gt;\n  range_filter(\"energy\", anchor_song$energy, 0.05) |&gt;\n  range_filter(\"valence\", anchor_song$valence, 0.05) |&gt;\n  pull(id)\n\n# Combine and Filter\nall_candidates &lt;- unique(c(co_songs, key_tempo_match, same_artist, similar_year, mood_match))\nall_candidates &lt;- setdiff(all_candidates, anchor_id)\n\n# Join with song data\ncandidate_songs &lt;- SONGS_CLEAN |&gt;\n  filter(id %in% all_candidates)\n\n# Ensure at least 20 candidates, 8 with popularity &lt; 60\nset.seed(123)\nunder_60 &lt;- candidate_songs |&gt; \n  filter(popularity &lt; 60) |&gt;\n  sample_n(8)\n\nnot_under_60 &lt;- candidate_songs |&gt;\n  filter(popularity &gt;= 60) |&gt;\n  sample_n(12)\n\nfinal_candidates &lt;- bind_rows(under_60, not_under_60)\n\n\nplaylist_pool &lt;- bind_rows(final_candidates, anchor_song)\n\nplaylist_21 &lt;- playlist_pool |&gt;\n  select(id, name, artist, popularity) |&gt;\n  arrange(desc(popularity))\nkable(playlist_21, cap='Potential songs that belong on a playlist with our anchor song')\n\n\n\nPotential songs that belong on a playlist with our anchor song\n\n\n\n\n\n\n\n\nid\nname\nartist\npopularity\n\n\n\n\n4kflIGfjdZJW4ot2ioixTB\nSomeone Like You\nAdele\n78\n\n\n6P7tTFzn6oNa0GL8w8oazE\nMake You Feel My Love\nAdele\n73\n\n\n7IWkJwX9C0J7tHurTD7ViL\nWhen We Were Young\nAdele\n72\n\n\n4j6GMcVcqZf1r0GDqMtYp6\nPrayed For You\nMatt Stell\n71\n\n\n7DUh5iszvXQDhhE0ZEtmUe\nEntra en Mi Vida\nSin Bandera\n67\n\n\n7cK7hDrE7vAesPf8xd5zmb\nStupid Deep\nJon Bellion\n67\n\n\n3DV49ruvI5Sl6iisPZAc2c\nHooked\nDylan Scott\n66\n\n\n3EATISg39vr81RXoosDtil\nSplit It\nMoneybagg Yo\n65\n\n\n0G2wimhVoDYXbQ6csDxtSf\nSex n’ Drugs\nHarrison Sands\n65\n\n\n5TgEJ62DOzBpGxZ7WRsrqb\nWork Song\nHozier\n64\n\n\n1U5pPF8f6APVCoc6t2KfIB\nCalm Down (Bittersweet) [feat. Summer Walker]\nSummer Walker\n64\n\n\n0ZPCxNK4DIrF8KqrN3DxfA\nNegative Energy\nTrippie Redd\n62\n\n\n40RcMOyjymXFKBxlMuQJnA\nWhat I’ve Been Looking For - From “High School Musical”/Soundtrack Version\nSharpay Evans\n60\n\n\n0GDQXt7qRJIDDUmcufomrU\nMove Together\nJames Bay\n49\n\n\n6gfAQs9TWAq91Ehp3eLFUX\nMake It In America\nVictorious Cast\n48\n\n\n5MNX5EUXOPzu17gqwSWnPU\nFair Play\nVan Morrison\n34\n\n\n3j4QPgiDGGipjfLgtikzrL\nThe Letter - Mono Single Version\nThe Box Tops\n31\n\n\n5xFed9QrbeHdj5okNUVZHs\nWPLJ\nThe Mothers Of Invention\n23\n\n\n5sGNawDBNNLOUAEkmplQtl\nMaureen\nEddie Money\n19\n\n\n3EyPEdXmMZT2uplSg2EZRp\nYou Can Depend on Me\nBarbara Mason\n19\n\n\n0L2Dspp7FzoJcQGU8ANaGL\nΧορός δωδεκανησιακός\nΤέτος Δημητριάδης\n0"
  },
  {
    "objectID": "mp03.html#create-your-playlist",
    "href": "mp03.html#create-your-playlist",
    "title": "Creating the Ultimate Playlist",
    "section": "Create Your Playlist",
    "text": "Create Your Playlist\nAbove is a list of our anchor song and playlist candidates, now we can filter down to a playlist of around 12 songs, suitably ordered. We determine an optimal playlist by clustering songs into 3 moods/styles and picking 4 songs from each cluster. Our playlist includes 4 songs which are not “popular.” After the playlist is created, we will visualize its evolution on the various quantitative metrics such as energy, valence, tempo, acousticness, and danceability. The Ultimate Playlist is shown below.\n\n\nCode\n# Task 7: Curate and Analyze Your Ultimate Playlist\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(cluster) # for clustering\n\n# Normalize relevant features for clustering\nfeatures &lt;- playlist_pool |&gt;\n  select(id, name, artist, tempo, key, acousticness, danceability, energy, valence) |&gt;\n  column_to_rownames(\"id\")\n\nscaled_features &lt;- scale(features |&gt; select(-name, -artist))\n\n# Cluster songs into moods/styles (e.g., 3 groups)\nset.seed(123)\nclusters &lt;- kmeans(scaled_features, centers = 3)$cluster\n\nplaylist_pool$cluster &lt;- clusters[playlist_pool$id]\n\n# Pick 4 songs from each cluster (12 total)\ncurated_playlist &lt;- playlist_pool |&gt;\n  group_by(cluster) |&gt;\n  slice_sample(n = 4) |&gt;\n  ungroup()\n\n# Order within playlist based on energy\ncurated_playlist &lt;- curated_playlist |&gt;\n  arrange(desc(energy))\n\n# Add track number\ncurated_playlist &lt;- curated_playlist |&gt;\n  mutate(track_num = row_number())\ncurated_playlist_1 &lt;- curated_playlist |&gt;\n  select(track_num, name, artist, year)\nkable(curated_playlist_1, cap='The Ultimate Playlist')\n\n\n\nThe Ultimate Playlist\n\n\n\n\n\n\n\n\ntrack_num\nname\nartist\nyear\n\n\n\n\n1\nSplit It\nMoneybagg Yo\n2020\n\n\n2\nWhat I’ve Been Looking For - From “High School Musical”/Soundtrack Version\nSharpay Evans\n2006\n\n\n3\nMaureen\nEddie Money\n1978\n\n\n4\nThe Letter - Mono Single Version\nThe Box Tops\n1967\n\n\n5\nFair Play\nVan Morrison\n1974\n\n\n6\nStupid Deep\nJon Bellion\n2018\n\n\n7\nSomeone Like You\nAdele\n2011\n\n\n8\nSex n’ Drugs\nHarrison Sands\n2018\n\n\n9\nMove Together\nJames Bay\n2014\n\n\n10\nYou Can Depend on Me\nBarbara Mason\n1968\n\n\n11\nMake You Feel My Love\nAdele\n2008\n\n\n\n\n\nCode\n# Pivot longer for plotting multiple metrics\nplaylist_long &lt;- curated_playlist |&gt;\n  select(track_num, name, energy, valence, tempo, acousticness, danceability) |&gt;\n  pivot_longer(cols = -c(track_num, name), names_to = \"feature\", values_to = \"value\")\n\n# Create the faceted plot\nggplot(playlist_long, aes(x = track_num, y = value)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"darkred\", size = 2) +\n  facet_wrap(~feature, scales = \"free_y\") +\n  scale_x_continuous(breaks = curated_playlist$track_num, labels = curated_playlist$track_num) +\n  labs(\n    title = \"Feature Progression Across Playlist\",\n    x = \"Track\",\n    y = \"Value\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 14)\n  )\n\n\n\n\n\n\n\n\n\nThe songs are ordered in the playlist based on energy. Thus, for energy metric, it is ‘all fall’. For acousticness, danceability, tempo, and valence, it is ‘rise and fall’. In addition to its musical structure, we consider the thematic unity as ‘travel and emotion’, and give our playlist a name ‘La La Locomotion’."
  },
  {
    "objectID": "mp03.html#conclusion-the-ultimate-playlist",
    "href": "mp03.html#conclusion-the-ultimate-playlist",
    "title": "Creating the Ultimate Playlist",
    "section": "Conclusion: The Ultimate Playlist",
    "text": "Conclusion: The Ultimate Playlist\nWe nominate “La La Locomotion” for the Internet’s Best Playlist award. This playlist uses Adele’s “Someone Like You” as the anchor song, consisting of songs that belong on a playlist with it using various heuristics listed above and then filtered down to 10 songs by clustering. We also use Quarto’s video support to embed “Someone Like You”. Feel free to listen to it again before you go!"
  },
  {
    "objectID": "mp04.html#code-completes-all-instructor-provided-tasks-correctly.-responses-to-open-ended-tasks-are-particularly-insightful-and-creative.",
    "href": "mp04.html#code-completes-all-instructor-provided-tasks-correctly.-responses-to-open-ended-tasks-are-particularly-insightful-and-creative.",
    "title": "Exploring Recent US Political Shifts",
    "section": "Code completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.",
    "text": "Code completes all instructor-provided tasks correctly. Responses to open-ended tasks are particularly insightful and creative.\n\nReport exhibits particularly creative insights drawn from thorough student-initiated analyses."
  },
  {
    "objectID": "mp04.html#tables-and-figures-are-full-publication-quality.-report-includes-at-least-one-animated-visualization-designed-to-effectively-communicate-findings.",
    "href": "mp04.html#tables-and-figures-are-full-publication-quality.-report-includes-at-least-one-animated-visualization-designed-to-effectively-communicate-findings.",
    "title": "Exploring Recent US Political Shifts",
    "section": "Tables and figures are full ‘publication-quality’. Report includes at least one animated visualization designed to effectively communicate findings.",
    "text": "Tables and figures are full ‘publication-quality’. Report includes at least one animated visualization designed to effectively communicate findings.\n\nReport includes interactive (not just animated) visual elements."
  },
  {
    "objectID": "mp04.html#code-is-near-flawless.-code-passes-all-styler-and-lintr-type-analyses-without-issue.",
    "href": "mp04.html#code-is-near-flawless.-code-passes-all-styler-and-lintr-type-analyses-without-issue.",
    "title": "Exploring Recent US Political Shifts",
    "section": "Code is (near) flawless. Code passes all styler and lintr type analyses without issue.",
    "text": "Code is (near) flawless. Code passes all styler and lintr type analyses without issue.\n\nCode takes advantage of advanced Quarto features to improve presentation of results."
  },
  {
    "objectID": "mp04.html#data-import-is-fully-automated-and-efficient-taking-care-to-only-download-from-web-sources-if-not-available-locally.",
    "href": "mp04.html#data-import-is-fully-automated-and-efficient-taking-care-to-only-download-from-web-sources-if-not-available-locally.",
    "title": "Exploring Recent US Political Shifts",
    "section": "Data import is fully-automated and efficient, taking care to only download from web-sources if not available locally.",
    "text": "Data import is fully-automated and efficient, taking care to only download from web-sources if not available locally.\n\nReport uses additional data sources in a way that creates novel insights."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "",
    "text": "Let’s dispense with the euphemisms: 2024 was not just an election. It was a reckoning. Donald J. Trump’s resounding victory wasn’t a fluke, a quirk of turnout, or the result of some gaffe-prone Democratic campaign. It was the culmination of a political realignment that has been building for nearly a decade — and it has now arrived in full.\nThe era of technocratic centrism is over. The populist majority is here."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "",
    "text": "Let’s dispense with the euphemisms: 2024 was not just an election. It was a reckoning. Donald J. Trump’s resounding victory wasn’t a fluke, a quirk of turnout, or the result of some gaffe-prone Democratic campaign. It was the culmination of a political realignment that has been building for nearly a decade — and it has now arrived in full.\nThe era of technocratic centrism is over. The populist majority is here."
  },
  {
    "objectID": "mp04.html#task-1-us-county-shapefiles",
    "href": "mp04.html#task-1-us-county-shapefiles",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "Task 1: US County Shapefiles",
    "text": "Task 1: US County Shapefiles\nTo explore recent US political shifts, we need to collect election result data. We begin by downloading a shapefile containing US county (and equivalent) boundaries from the US Census Bureau.\n\n\nCode\n# Set directory and file paths\ndir_path &lt;- \"data/mp04\"\nzip_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_500k.zip\"\nzip_file &lt;- file.path(dir_path, \"cb_2023_us_county_500k.zip\")\n\n# Create directory if it doesn't exist\nif (!dir.exists(dir_path)) {\n  dir.create(dir_path, recursive = TRUE)\n}\n\n# Download the zip file if it doesn't already exist\nif (!file.exists(zip_file)) {\n  message(\"Downloading shapefile...\")\n  download.file(zip_url, destfile = zip_file, mode = \"wb\")\n} else {\n  message(\"Shapefile zip already exists. Skipping download.\")\n}\n\n# Unzip the file if shapefile components are not yet extracted\nshapefile_base &lt;- file.path(dir_path, \"cb_2023_us_county_500k\")\nif (!file.exists(paste0(shapefile_base, \".shp\"))) {\n  message(\"Extracting shapefile...\")\n  unzip(zip_file, exdir = dir_path)\n} else {\n  message(\"Shapefile already extracted. Skipping unzip.\")\n}\n\n\nFor years, pundits have dismissed Trump’s coalition as a “base” — rural, uneducated, angry. In 2016, they called it a protest vote. In 2020, they called it a fluke saved by the pandemic. Now, in 2024, after flipping back Rust Belt counties, surging Latino support in South Texas and Nevada, and making inroads with Black working-class voters in urban cores, Trump’s movement can no longer be ignored. This wasn’t a rerun — it was an expansion."
  },
  {
    "objectID": "mp04.html#task-3-acquire-2020-us-presidential-election-results",
    "href": "mp04.html#task-3-acquire-2020-us-presidential-election-results",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "Task 3: Acquire 2020 US Presidential Election Results",
    "text": "Task 3: Acquire 2020 US Presidential Election Results\nNext, we simply modify our code from Task 2 to acquire 2020 US Presidential Election Results.\n\n\nCode\nget_2020_election_results &lt;- function(state) {\n  # Create safe directory\n  dir_create(\"data/elections_2020\")\n  \n  # Format Wikipedia URL part\n  state_url &lt;- str_replace_all(state, \" \", \"_\")\n  url_path &lt;- paste0(\"wiki/2020_United_States_presidential_election_in_\", state_url)\n  full_url &lt;- paste0(\"https://en.wikipedia.org/\", url_path)\n  \n  # Cached file path\n  local_file &lt;- file.path(\"data/elections_2020\", paste0(state, \".html\"))\n  \n  # Download and cache page\n  if (!file_exists(local_file)) {\n    message(\"Downloading page for \", state)\n    resp &lt;- request(\"https://en.wikipedia.org/\") |&gt;\n      req_url_path(url_path) |&gt;\n      req_perform()\n    writeBin(resp_body_raw(resp), local_file)\n  } else {\n    message(\"Using cached page for \", state)\n  }\n  \n  # Parse HTML\n  page &lt;- read_html(local_file)\n  tables &lt;- html_elements(page, \"table\")\n  df_list &lt;- lapply(tables, html_table, fill = TRUE)\n  \n  # Define possible headers that indicate a county-level results table\n  county_keywords &lt;- c(\"county\", \"parish\", \"borough\", \"municipality\")\n  \n  # Find correct table\n  selected_table &lt;- NULL\n  for (df in df_list) {\n    headers &lt;- tolower(names(df))\n    if (any(str_detect(headers, str_c(county_keywords, collapse = \"|\")))) {\n      selected_table &lt;- df\n      break\n    }\n  }\n  \n  if (is.null(selected_table)) {\n    warning(\"No county-level table found for: \", state)\n    return(NULL)\n  }\n  \n  # Clean up the selected table\n  names(selected_table) &lt;- names(selected_table) |&gt;\n    str_replace_all(\"\\\\[.*?\\\\]\", \"\") |&gt;\n    make.unique()\n  cleaned &lt;- selected_table |&gt;\n    mutate(state = state)\n  \n  return(cleaned)\n}\n\n# Example usage\nstates &lt;- state.name\nresults_list_2020 &lt;- map(states, safely(get_2020_election_results))\n\n\nDemocrats spent the last four years talking down to the very Americans they used to represent. They mocked parents concerned about schools, they ignored inflation until it was too late, and they alienated the working poor with climate mandates and identity politics. Trump did something radical: he listened. He talked about the price of gas, the border crisis, and the dignity of American work. And America responded."
  },
  {
    "objectID": "mp04.html#task-2-acquire-2024-us-presidential-election-results",
    "href": "mp04.html#task-2-acquire-2024-us-presidential-election-results",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "Task 2: Acquire 2024 US Presidential Election Results",
    "text": "Task 2: Acquire 2024 US Presidential Election Results\nWe will next obtain county-level election results for each of the 50 US states from Wikipedia.\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(fs)\n\nget_2024_election_results &lt;- function(state) {\n  # Create safe directory\n  dir_create(\"data/elections_2024\")\n  \n  # Format Wikipedia URL part\n  state_url &lt;- str_replace_all(state, \" \", \"_\")\n  url_path &lt;- paste0(\"wiki/2024_United_States_presidential_election_in_\", state_url)\n  full_url &lt;- paste0(\"https://en.wikipedia.org/wiki/\", url_path)\n  \n  # Cached file path\n  local_file &lt;- file.path(\"data/elections_2024\", paste0(state, \".html\"))\n  \n  # Download and cache page\n  if (!file_exists(local_file)) {\n    message(\"Downloading page for \", state)\n    resp &lt;- request(\"https://en.wikipedia.org/\") |&gt;\n      req_url_path(url_path) |&gt;\n      req_perform()\n    writeBin(resp_body_raw(resp), local_file)\n  } else {\n    message(\"Using cached page for \", state)\n  }\n  \n  # Parse HTML\n  page &lt;- read_html(local_file)\n  tables &lt;- html_elements(page, \"table\")\n  df_list &lt;- lapply(tables, html_table, fill = TRUE)\n  \n  # Define possible headers that indicate a county-level results table\n  county_keywords &lt;- c(\"county\", \"parish\", \"borough\", \"municipality\")\n  \n  # Find correct table\n  selected_table &lt;- NULL\n  for (df in df_list) {\n    headers &lt;- tolower(names(df))\n    if (any(str_detect(headers, str_c(county_keywords, collapse = \"|\")))) {\n      selected_table &lt;- df\n      break\n    }\n  }\n  \n  if (is.null(selected_table)) {\n    warning(\"No county-level table found for: \", state)\n    return(NULL)\n  }\n  \n  # Clean up the selected table\n  names(selected_table) &lt;- names(selected_table) |&gt;\n    str_replace_all(\"\\\\[.*?\\\\]\", \"\") |&gt;\n    make.unique()\n  cleaned &lt;- selected_table |&gt;\n    mutate(state = state)\n  \n  \n  return(cleaned)\n}\n\nstates &lt;- state.name\nresults_list_2024 &lt;- map(states, safely(get_2024_election_results))\n\n\nAcross the Midwest, counties that voted for Obama twice and then Trump twice have now gone even redder. In Pennsylvania and Michigan, union households voted Republican by margins not seen since the Reagan era. In Arizona, Hispanic precincts once assumed to be safe blue turned a defiant shade of red. Even New York and California saw double-digit swings in working-class boroughs and districts. This isn’t just cultural grievance — it’s economic rebellion."
  },
  {
    "objectID": "mp04.html#task-4-initial-analysis-questions",
    "href": "mp04.html#task-4-initial-analysis-questions",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "Task 4: Initial Analysis Questions",
    "text": "Task 4: Initial Analysis Questions\nAt this point, we have all of the data needed and we will combine the three data files (county shapes, 2020 results, 2024 results) and use them to answer the following questions.\n\n1. Which county or counties cast the most votes for Trump (in absolute terms) in 2024?\n\n\nCode\n# Keep only relevant columns for joining\ncounty_shapes &lt;- county_shapes |&gt;\n  select(GEOID, NAME, STATEFP, geometry)\n\n# Extract and bind 2020 results\nvalid_2020 &lt;- results_list_2020 |&gt; keep(~ !is.null(.x$result)) |&gt; map(\"result\")\nelection_2020_df &lt;- bind_rows(valid_2020)\n\n# Extract and bind 2024 results\nvalid_2024 &lt;- results_list_2024 |&gt; keep(~ !is.null(.x$result)) |&gt; map(\"result\")\nelection_2024_df &lt;- bind_rows(valid_2024)\n\n# Helper function to clean names\nclean_name &lt;- function(x) {\n  x |&gt; tolower() |&gt; str_replace_all(\"[^a-z]\", \"\")\n}\n\n# Prepare election data with cleaned keys\ncounty_col_2020 &lt;- names(election_2020_df)[1]\n\nelection_2020_df &lt;- election_2020_df |&gt;\n  mutate(county_clean = clean_name(.data[[county_col_2020]]),\n         state_clean = clean_name(state))\n\ncounty_col_2024 &lt;- names(election_2024_df)[1]\n\nelection_2024_df &lt;- election_2024_df |&gt;\n  mutate(county_clean = clean_name(.data[[county_col_2024]]),\n         state_clean = clean_name(state))\n\n# Prepare shapefile data with cleaned keys\ncounty_shapes &lt;- county_shapes |&gt;\n  mutate(county_clean = clean_name(NAME),\n         state_clean = clean_name(state.name[as.integer(STATEFP)]))  # converts STATEFP to full name\n\n# Join with 2020\njoined_2020 &lt;- left_join(county_shapes, election_2020_df,\n                         by = c(\"county_clean\", \"state_clean\"))\n\n# Join with 2024\njoined_all &lt;- left_join(joined_2020, election_2024_df,\n                        by = c(\"county_clean\", \"state_clean\"),\n                        suffix = c(\"_2020\", \"_2024\"))\n\n# 1.Which county or counties cast the most votes for Trump (in absolute terms) in 2024?\n#names(joined_all)\njoined_all &lt;- joined_all |&gt;\n  mutate(trump_votes_2024 = parse_number(`Donald TrumpRepublican_2024`))\ntop_trump_county &lt;- joined_all |&gt;\n  slice_max(trump_votes_2024) |&gt;\n  select(NAME, state_clean, trump_votes_2024)\nlibrary(knitr)\nkable(top_trump_county,cap=\"County with the most votes for Trump in 2024\")\n\n\n\nCounty with the most votes for Trump in 2024\n\n\n\n\n\n\n\n\nNAME\nstate_clean\ntrump_votes_2024\ngeometry\n\n\n\n\nJefferson\nkentucky\n144553\nMULTIPOLYGON (((-89.1476 38…\n\n\n\n\n\nIn 2024, Jefferson County, Kentucky cast the most votes for Donald Trump of any county in the nation, delivering 144,553 votes for the former president.\nIn a year defined by surprising shifts and reawakened coalitions, it was this unlikely southern urban stronghold — more commonly known for bourbon than for MAGA hats — that handed Trump his single largest county-level haul. The message? Even in places where Democrats once held sway, the red tide surged.\n\n\n2. Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\n\n\nCode\n# 2.Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    biden_votes_2020 = parse_number(`Joe BidenDemocratic.1`)\n    )\ntop_biden_county &lt;- joined_all |&gt;\n  slice_max(biden_votes_2020) |&gt;\n  select(NAME, state_clean, biden_votes_2020)\nkable(top_biden_county, cap=\"County with the most votes for Biden in 2020\")\n\n\n\nCounty with the most votes for Biden in 2020\n\n\nNAME\nstate_clean\nbiden_votes_2020\ngeometry\n\n\n\n\nMacon\nalabama\n81.49\nMULTIPOLYGON (((-86.02301 3…\n\n\n\n\n\nIn the 2020 election, Macon County, Alabama gave Joe Biden his strongest showing in the country by vote share, with 81.49% of all ballots cast going to the Democratic nominee. This deep-blue pocket in the Deep South stood out as one of Biden’s most loyal counties, delivering a resounding show of support.\n\n\n3. Which county or counties had the largest shift towards Trump (in absolute terms) in 2024?\n\n\nCode\n# 3.Which county or counties had the largest shift towards Trump (in absolute terms) in 2024?\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    trump_votes_2020 = parse_number(`Donald TrumpRepublican_2020`)\n  )\n# the change in vote count for Trump from 2020 to 2024 in each county\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    trump_vote_shift = trump_votes_2024 - trump_votes_2020\n  )\n# Identify the County with the Largest Increase\nlargest_shift_county &lt;- joined_all |&gt;\n  slice_max(trump_vote_shift) |&gt;\n  select(NAME, state_clean, trump_votes_2020, trump_votes_2024, trump_vote_shift)\nkable(largest_shift_county, cap=\"County with the largest shift towards Trump in 2024\")\n\n\n\nCounty with the largest shift towards Trump in 2024\n\n\n\n\n\n\n\n\n\n\nNAME\nstate_clean\ntrump_votes_2020\ntrump_votes_2024\ntrump_vote_shift\ngeometry\n\n\n\n\nBaldwin\nalabama\n83544\n95798\n12254\nMULTIPOLYGON (((-88.02858 3…\n\n\n\n\n\nIn the 2024 election, Baldwin County, Alabama recorded the largest shift toward Donald Trump in absolute terms, delivering 12,254 more votes for him than it did in 2020. The coastal county emerged as a stronghold of growing Republican enthusiasm, reflecting a deepening red trend in the South.\n\n\n4. Which state had the smallest shift towards Trump in 2024?\n\n\nCode\n# 4. Which state had the smallest shift towards Trump in 2024?\n# Group by the cleaned state name and sum Trump votes for both years:\nstate_shifts &lt;- joined_all |&gt;\n  group_by(state_clean) |&gt;\n  summarise(\n    trump_2020 = sum(trump_votes_2020, na.rm = TRUE),\n    trump_2024 = sum(trump_votes_2024, na.rm = TRUE),\n    shift = trump_2024 - trump_2020\n  )\n#finding the smallest (most negative or least positive) value of shift:\nsmallest_shift_state &lt;- state_shifts |&gt;\n  slice_min(shift)\nkable(smallest_shift_state, cap=\"State with the smallest shift towards Trump in 2024\")\n\n\n\nState with the smallest shift towards Trump in 2024\n\n\n\n\n\n\n\n\n\nstate_clean\ntrump_2020\ntrump_2024\nshift\ngeometry\n\n\n\n\noregon\n209432\n199688\n-9744\nMULTIPOLYGON (((-76.33033 3…\n\n\n\n\n\nIn 2024, Oregon stood out as the state least moved by the red wave, registering the smallest shift toward Donald Trump—in fact, a shift in the opposite direction. Trump received 9,744 fewer votes there than in 2020, making Oregon a rare outlier in an election that otherwise saw Republican gains across much of the country.\n\n\n5. What is the largest county, by area, in this data set?\n\n\nCode\n# 5. What is the largest county, by area, in this data set?\n# To accurately calculate area, reproject to an equal-area CRS like EPSG: 5070 (USA Contiguous Albers Equal Area):\njoined_proj &lt;- st_transform(joined_all, crs = 5070)\n\n# Find the maximum area and Get the largest county\njoined_proj &lt;- joined_proj |&gt;\n  mutate(area_km2 = as.numeric(st_area(geometry)) / 1e6)\n\nlargest_county &lt;- joined_proj |&gt;\n  slice_max(area_km2) |&gt;\n  select(NAME, state_clean, area_km2)\nkable(largest_county, cap=\"The Largest County by area in this data set\")\n\n\n\nThe Largest County by area in this data set\n\n\nNAME\nstate_clean\narea_km2\ngeometry\n\n\n\n\nYukon-Koyukuk\nalaska\n382993.9\nMULTIPOLYGON (((-3555277 55…\n\n\n\n\n\nThe largest county by area in the dataset is Yukon-Koyukuk, Alaska, spanning a vast 382,994 square kilometers. Remote, rugged, and largely untouched, this enormous stretch of Alaskan wilderness dwarfs entire states and stands as a stark reminder of the sheer scale of America’s frontier.\n\n\n6. Which county has the highest voter density (voters per unit of area) in 2020?\n\n\nCode\n# 6.Which county has the highest voter density (voters per unit of area) in 2020?\n# Total votes cast in 2020 per county\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    biden_2020 = parse_number(`Joe BidenDemocratic`),\n    total_votes_2020 = biden_2020 + trump_votes_2020\n  )\n#Use an equal-area CRS EPSG: 5070 for accurate measurement:\nlibrary(sf)\n\njoined_proj &lt;- st_transform(joined_all, crs = 5070)\n\njoined_proj &lt;- joined_proj |&gt;\n  mutate(\n    area_km2 = as.numeric(st_area(geometry)) / 1e6  # convert from m² to km²\n  )\n# Calculate Voter Density\njoined_proj &lt;- joined_proj |&gt;\n  mutate(\n    voter_density_2020 = total_votes_2020 / area_km2\n  )\n#Find County with Highest Voter Density\nhighest_density_county &lt;- joined_proj |&gt;\n  slice_max(voter_density_2020) |&gt;\n  select(NAME, state_clean, total_votes_2020, area_km2, voter_density_2020)\nkable(highest_density_county, cap=\"County with the highest voter density in 2020\")\n\n\n\nCounty with the highest voter density in 2020\n\n\n\n\n\n\n\n\n\n\nNAME\nstate_clean\ntotal_votes_2020\narea_km2\nvoter_density_2020\ngeometry\n\n\n\n\nWashington\noregon\n309013\n970.7681\n318.318\nMULTIPOLYGON (((1705808 157…\n\n\n\n\n\nIn the 2020 election, Washington County, Oregon topped the nation in voter density, packing 318 voters per square kilometer into its modest footprint. Nestled in the Portland metro area, this suburban hub proved that civic engagement can thrive where communities are closely knit and densely settled.\n\n\n7. Which county had the largest increase in voter turnout in 2024?\n\n\nCode\n# 7.Which county had the largest increase in voter turnout in 2024?\n# calculate total votes for 2020 and 2024 per county:\nlibrary(dplyr)\nlibrary(readr)\n\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    kamala_2024 = parse_number(`Kamala HarrisDemocratic`),\n    total_votes_2024 = kamala_2024 + trump_votes_2024\n  )\n#Compute the Turnout Change\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    turnout_increase = total_votes_2024 - total_votes_2020\n  )\n#Identify the County with the Largest Increase\nlargest_turnout_county &lt;- joined_all |&gt;\n  slice_max(turnout_increase) |&gt;\n  select(NAME, state_clean, total_votes_2020, total_votes_2024, turnout_increase)\nkable(largest_turnout_county, cap=\"County with the largest increase in voter turnout in 2024\")\n\n\n\nCounty with the largest increase in voter turnout in 2024\n\n\n\n\n\n\n\n\n\n\nNAME\nstate_clean\ntotal_votes_2020\ntotal_votes_2024\nturnout_increase\ngeometry\n\n\n\n\nBaldwin\nalabama\n108122\n120732\n12610\nMULTIPOLYGON (((-88.02858 3…\n\n\n\n\n\nBaldwin County, Alabama saw the largest surge in voter turnout in the 2024 election, with 12,610 more ballots cast than in 2020. This coastal county not only deepened its support for Donald Trump but also drew more voters to the polls than any other, signaling a sharp rise in political engagement."
  },
  {
    "objectID": "mp04.html#task-5-reproduce-nyt-figure",
    "href": "mp04.html#task-5-reproduce-nyt-figure",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "Task 5: Reproduce NYT Figure",
    "text": "Task 5: Reproduce NYT Figure\nHaving confirmed our data is mainly reliable, we are now ready to reproduce the NYT Figure.\n\n\nCode\n# Compute the shift (as a percentage of votes cast) rightwards for each county.\n#Compute the percentage of votes Trump received in both elections:\nlibrary(dplyr)\n\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    trump_pct_2020 = trump_votes_2020 / total_votes_2020,\n    trump_pct_2024 = trump_votes_2024 / total_votes_2024\n  )\n#Calculate the change in Trump's vote percentage from 2020 to 2024:\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    rightward_shift = trump_pct_2024 - trump_pct_2020\n  )\n\n# \nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Use Albers Equal Area projection (like NYT)\nproj_crs &lt;- 2163\njoined_all &lt;- st_transform(joined_all, proj_crs)\n\n# Compute centroids\njoined_all &lt;- joined_all |&gt;\n  mutate(centroid = st_centroid(geometry)) |&gt;\n  mutate(\n    x = st_coordinates(centroid)[, 1],\n    y = st_coordinates(centroid)[, 2]\n  )\n\n# Set arrow scale — adjust to taste\narrow_scale &lt;- 10000000  # shorter arrows\n\n# Direction + scaled endpoints\njoined_all &lt;- joined_all |&gt;\n  mutate(\n    shift_direction = ifelse(rightward_shift &gt; 0, \"Right\", \"Left\"),\n    shift_length = abs(rightward_shift) * arrow_scale,\n    xend = x + ifelse(rightward_shift &gt; 0, shift_length, -shift_length),\n    yend = y\n  )\n\nstates &lt;- joined_all |&gt;\n  group_by(STATEFP, state_clean) |&gt;\n  summarize(geometry = st_union(geometry), .groups = \"drop\")\n\n# Reposition Alaska and Hawaii\nscale_and_shift &lt;- function(sf_obj, scale = 1, shift = c(0, 0)) {\n  geom &lt;- st_geometry(sf_obj)\n  center &lt;- st_centroid(st_union(geom))\n  new_geom &lt;- (geom - center) * scale + center + shift\n  st_geometry(sf_obj) &lt;- st_sfc(new_geom, crs = st_crs(sf_obj))\n  sf_obj\n}\n\nalaska &lt;- states |&gt; filter(STATEFP == \"02\") |&gt;\n  scale_and_shift(scale = 0.35, shift = c(1300000, -5000000))\nhawaii &lt;- states |&gt; filter(STATEFP == \"15\") |&gt;\n  scale_and_shift(scale = 1, shift = c(5200000, -1400000))\nmainland &lt;- states |&gt; filter(!STATEFP %in% c(\"02\", \"15\"))\n\nstates_transformed &lt;- bind_rows(mainland, alaska, hawaii)\n\n\n\n\nCode\nggplot() +\n  # State backgrounds\n  geom_sf(data = states_transformed, fill = \"gray98\", color = \"gray80\", size = 0.2) +\n  \n  # Arrows showing county-level vote shifts\n  geom_segment(\n    data = joined_all,\n    aes(x = x, y = y, xend = xend, yend = yend, color = shift_direction),\n    arrow = arrow(type = \"closed\", length = unit(0.06, \"inches\")),\n    linewidth = 0.25,\n    alpha = 0.9\n  ) +\n\n  # Manual color scale: red for rightward, blue for leftward\n  scale_color_manual(\n    values = c(\"Right\" = \"#c40000\", \"Left\" = \"#0050d0\"),\n    guide = \"none\"\n  ) +\n\n  # Title, subtitle, and caption\n  labs(\n    title = \"Where the 2024 Election Shifted\",\n    subtitle = \"Each arrow shows the direction and magnitude of vote shift by county\\nRed = Shift toward Trump, Blue = Shift toward Democrats\",\n    caption = \"Source: County-level election data, 2020–2024\"\n  ) +\n\n  # Minimalist theme with emphasis on data\n  theme_void(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 13, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 9, hjust = 0.5, margin = margin(t = 10)),\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n\n  # Focused zoom on continental US (in projected coordinates)\n  coord_sf(\n    xlim = c(-2500000, 2500000),\n    ylim = c(-2700000, 1100000),\n    expand = FALSE,\n    datum = NA\n  )"
  },
  {
    "objectID": "mp04.html#task-6-additional-analysis-and-figure-creation",
    "href": "mp04.html#task-6-additional-analysis-and-figure-creation",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "Task 6: Additional Analysis and Figure Creation",
    "text": "Task 6: Additional Analysis and Figure Creation\nTheme: 2024 Was a National Rightward Shift, Not a Fluke\n\n✅ Talking Point 1: The Median County Shifted Right by More Than 5 Percentage Points\n\n“This wasn’t a marginal drift — it was a median-level red surge. Over half the counties in America shifted more than 5 percentage points toward Trump.”\n\nStatistical Test: Using the infer package, we test:\n\nNull hypothesis: Median county shift is less than or equal to 5%. Alternative hypothesis: Median shift &gt; 5%.\n\n\n\nCode\njoined_all %&gt;%\n  specify(response = rightward_shift) %&gt;%\n  hypothesize(null = \"point\", mu = 5) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\") %&gt;%\n  get_p_value(obs_stat = median(joined_all$rightward_shift, na.rm = TRUE),\n              direction = \"right\")\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       1\n\n\nIf p &lt; 0.05, we reject the null and conclude a significant rightward shift.\n🎯 Plot: Histogram of county-level shift\n\n\nCode\njoined_all %&gt;%\n  ggplot(aes(x = rightward_shift)) +\n  geom_histogram(binwidth = 1, fill = \"pink\", color = \"white\", alpha = 0.8) +\n  geom_vline(aes(xintercept = median(rightward_shift, na.rm = TRUE)), color = \"black\", linetype = \"dashed\") +\n  labs(\n    title = \"Rightward Shift in 2024\",\n    subtitle = \"Distribution of county-level partisan shifts\",\n    x = \"Trump % (2024) − Trump % (2020)\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n✅ Talking Point 2: Turnout Rose Dramatically in Red Counties, Not Blue Ones\n\n“Trump didn’t just win more votes — he brought more voters. The biggest turnout surges were in deep red counties.”\n\nComputation: Group counties by 2020 Trump share (e.g., above/below 50%), and compare turnout_increase.\nStatistical Test: Two-sample test to compare turnout_increase between red and blue counties.\n\n\nCode\njoined_all_clean &lt;- joined_all %&gt;%\n  filter(!is.na(red_2020)) %&gt;%\n  mutate(red_2020 = as.logical(red_2020))  # ensure it's logical\n\nlibrary(infer)\n\njoined_all_clean %&gt;%\n  specify(turnout_increase ~ red_2020) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(TRUE, FALSE)) %&gt;%\n  get_p_value(\n    obs_stat = with(joined_all_clean,\n                    mean(turnout_increase[red_2020], na.rm = TRUE) -\n                      mean(turnout_increase[!red_2020], na.rm = TRUE)),\n    direction = \"right\"\n  )\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n🎯 Plot: Boxplot of turnout change by 2020 partisan alignment\n\n\nCode\njoined_all %&gt;%\n  filter(!is.na(red_2020)) %&gt;%\n  ggplot(aes(x = as.factor(red_2020), y = turnout_increase, fill = red_2020)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.7) +\n  scale_fill_manual(values = c(\"blue\", \"red\"), labels = c(\"Biden 2020\", \"Trump 2020\")) +\n  labs(\n    title = \"Turnout Change from 2020 to 2024\",\n    subtitle = \"Trump counties retained more of their voters\",\n    x = \"2020 Winner\",\n    y = \"Turnout Increase (2024 vs 2020)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n✅ Talking Point 3: Democratic Strongholds Lost Ground Even Where Biden Previously Dominated\n\n“Even counties that went 80% for Biden in 2020 saw a drop in support. The erosion is real, even in the bluest ZIP codes.”\n\nComputation: Filter counties with Biden vote share &gt; 70% in 2020 and look at Trump shift.\n\n\nCode\nblue_strongholds &lt;- joined_all %&gt;% filter(biden_2020 &gt; 70)\nsummary(blue_strongholds$rightward_shift)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.01255  0.01051  0.01958  0.01964  0.02807  0.06240 \n\n\nStatistical Test: Is the average shift in those counties still rightward (i.e., Trump gained even in blue bastions)?\n\n\nCode\nlibrary(infer)\nblue_strongholds %&gt;%\n  specify(response = rightward_shift) %&gt;%\n  hypothesize(null = \"point\", mu = 0) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\") %&gt;%\n  get_p_value(obs_stat = mean(blue_strongholds$rightward_shift, na.rm = TRUE),\n              direction = \"right\")\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n🎯 Plot: Bar plot of top counties by vote gain\n\n\nCode\njoined_all %&gt;%\n  mutate(winner_2020 = ifelse(trump_pct_2020 &gt; 0.5, \"Trump\", \"Biden\")) %&gt;%\n  group_by(NAME, state_clean, winner_2020) %&gt;%\n  mutate(vote_gain = total_votes_2024 - total_votes_2020) %&gt;%\n  ungroup() %&gt;%\n  filter(!is.na(vote_gain)) %&gt;%\n  slice_max(order_by = vote_gain, n = 10) %&gt;%\n  ggplot(aes(x = reorder(NAME, vote_gain), y = vote_gain, fill = winner_2020)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\"Biden\" = \"blue\", \"Trump\" = \"red\")) +\n  labs(\n    title = \"Top 10 Counties by Raw Vote Increase\",\n    subtitle = \"Most top-gainers were Trump-aligned in 2020\",\n    x = \"County\",\n    y = \"Net Vote Gain (2024 vs 2020)\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "mp04.html#conclusion",
    "href": "mp04.html#conclusion",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "Conclusion",
    "text": "Conclusion\nWhat we’re witnessing is a tectonic shift — a movement of working-class voters across race, region, and religion toward a party they now see as fighting for them. The GOP is no longer the party of Paul Ryan; it’s the party of working families, small towns, and forgotten cities. Trump didn’t just win an election — he reshaped the map.\nOf course, the legacy press will resist this. They’ll cry foul, blame misinformation, or moan about democracy. But it’s the people who choose — and in 2024, the people made it clear: the Trump coalition is not a glitch in the system. It is the system now.\nIf Democrats want to cling to coastal technocracy, let them. The heartland has chosen its champion.\nHistory will mark 2024 as the year the political pendulum didn’t just swing — it broke loose."
  },
  {
    "objectID": "mp04.html#theme-2024-was-a-national-rightward-shift-not-a-fluke",
    "href": "mp04.html#theme-2024-was-a-national-rightward-shift-not-a-fluke",
    "title": "Op-Ed: The Trump Realignment — 2024 Wasn’t Just a Win, It Was a Revolution",
    "section": "Theme: 2024 Was a National Rightward Shift, Not a Fluke",
    "text": "Theme: 2024 Was a National Rightward Shift, Not a Fluke\n\nTalking Point 1: The Median County Shifted Right by More Than 5 Percentage Points\n\n“This wasn’t a marginal drift — it was a median-level red surge. Over half the counties in America shifted more than 5 percentage points toward Trump.”\n\nStatistical Test: Using the infer package, we test:\n\nNull hypothesis: Median county shift is less than or equal to 5%. Alternative hypothesis: Median shift &gt; 5%.\n\n\n\nCode\njoined_all %&gt;%\n  specify(response = rightward_shift) %&gt;%\n  hypothesize(null = \"point\", mu = 5) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\") %&gt;%\n  get_p_value(obs_stat = median(joined_all$rightward_shift, na.rm = TRUE),\n              direction = \"right\")\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       1\n\n\nIf p &lt; 0.05, we reject the null and conclude a significant rightward shift.\n\n\nTalking Point 2: Turnout Rose Dramatically in Red Counties, Not Blue Ones\n\n“Trump didn’t just win more votes — he brought more voters. The biggest turnout surges were in deep red counties.”\n\nComputation: Group counties by 2020 Trump share (e.g., above/below 50%), and compare turnout_increase.\n\n\nCode\njoined_all &lt;- joined_all %&gt;%\n  mutate(red_2020 = trump_pct_2020 &gt; 0.5)\n\nlibrary(dplyr)\njoined_all %&gt;%\n  group_by(red_2020) %&gt;%\n  summarize(median_turnout_increase = median(turnout_increase, na.rm = TRUE))\n\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8511502 ymin: -3753916 xmax: 3745363 ymax: 4848309\nProjected CRS: NAD27 / US National Atlas Equal Area\n# A tibble: 3 × 3\n  red_2020 median_turnout_increase                                      geometry\n  &lt;lgl&gt;                      &lt;dbl&gt;                            &lt;MULTIPOLYGON [m]&gt;\n1 FALSE                       -756 (((1124048 -1356909, 1124440 -1356547, 11248…\n2 TRUE                         -94 (((318025 -1849908, 318122.5 -1850227, 31817…\n3 NA                            NA (((-7798791 -3753878, -7798822 -3753901, -77…\n\n\nStatistical Test: Two-sample test to compare turnout_increase between red and blue counties.\n\n\nCode\njoined_all_clean &lt;- joined_all %&gt;%\n  filter(!is.na(red_2020)) %&gt;%\n  mutate(red_2020 = as.logical(red_2020))  # ensure it's logical\n\nlibrary(infer)\n\njoined_all_clean %&gt;%\n  specify(turnout_increase ~ red_2020) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(TRUE, FALSE)) %&gt;%\n  get_p_value(\n    obs_stat = with(joined_all_clean,\n                    mean(turnout_increase[red_2020], na.rm = TRUE) -\n                      mean(turnout_increase[!red_2020], na.rm = TRUE)),\n    direction = \"right\"\n  )\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n###Talking Point 3: Democratic Strongholds Lost Ground Even Where Biden Previously Dominated\n\n“Even counties that went 80% for Biden in 2020 saw a drop in support. The erosion is real, even in the bluest ZIP codes.”\n\nComputation: Filter counties with Biden vote share &gt; 70% in 2020 and look at Trump shift.\n\n\nCode\nblue_strongholds &lt;- joined_all %&gt;% filter(biden_2020 &gt; 70)\nsummary(blue_strongholds$rightward_shift)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.01255  0.01051  0.01958  0.01964  0.02807  0.06240 \n\n\nStatistical Test: Is the average shift in those counties still rightward (i.e., Trump gained even in blue bastions)?\n\n\nCode\nlibrary(infer)\nblue_strongholds %&gt;%\n  specify(response = rightward_shift) %&gt;%\n  hypothesize(null = \"point\", mu = 0) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\") %&gt;%\n  get_p_value(obs_stat = mean(blue_strongholds$rightward_shift, na.rm = TRUE),\n              direction = \"right\")\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0"
  },
  {
    "objectID": "h.html",
    "href": "h.html",
    "title": "STA 9750 Submission Material",
    "section": "",
    "text": "STA 9750"
  },
  {
    "objectID": "cp.html",
    "href": "cp.html",
    "title": "Clean job titles",
    "section": "",
    "text": "ai = read.csv(“~/sta9750-2025-spring/data/cp/my_data.csv”) employment = read.csv(“~/sta9750-2025-spring/data/cp/employment projections.csv”)\nlibrary(dplyr) glimpse(ai) glimpse(employment)\nlibrary(dplyr) library(stringr) library(tidyr)\n\nClean job titles\nai &lt;- ai |&gt; mutate(Job.title.clean = str_to_lower(str_trim(Job.titiles)))\nemployment &lt;- employment |&gt; mutate(Occupation.Title.clean = str_to_lower(str_trim(Occupation.Title)))\n\n\nOccupation.Title contains multiple titles separated by *, we split those:\nemployment &lt;- employment |&gt; separate_rows(Occupation.Title.clean, sep = “\\*“) |&gt; mutate(Occupation.Title.clean = str_trim(Occupation.Title.clean))\n\n\nUse a fuzzy join as exact matches may not work\ninstall.packages(“fuzzyjoin”) library(fuzzyjoin)\ncombined &lt;- stringdist_inner_join(ai, employment, by = c(“Job.title.clean” = “Occupation.Title.clean”), method = “jw”, max_dist = 0.15) # adjust as needed\n\n\nremove helper columns\ncombined &lt;- combined |&gt; select(-Job.title.clean, -Occupation.Title.clean)\nlibrary(ggplot2) library(readr) # Convert AI Impact % to numeric combined &lt;- combined |&gt; mutate(AI.Impact.Numeric = as.numeric(str_remove(AI.Impact, “%”)), Employment.Growth = Employment.Percent.Change..2023.2033)\n\n\nScatter plot: AI impact vs. employment growth\nggplot(combined, aes(x = AI.Impact.Numeric, y = Employment.Growth)) + geom_point(alpha = 0.6, color = “steelblue”) + geom_smooth(method = “lm”, se = FALSE, color = “red”) + labs(title = “AI Impact vs. Projected Employment Growth”, x = “AI Impact (%)”, y = “Employment Growth (2023–2033, %)”, caption = “Data: AI Impact and U.S. Employment Projections”) + theme_minimal()\n\n\nCorrelation coefficient\ncor.test(combined\\(AI.Impact.Numeric, combined\\)Employment.Growth)\nggplot(combined, aes(x = AI.Impact.Numeric, y = Employment.Growth, color = Domain)) + geom_point(alpha = 0.7, size = 2) + geom_smooth(method = “lm”, se = FALSE, color = “black”) + labs( title = “AI Impact vs. Employment Growth (Colored by Domain)”, x = “AI Impact (%)”, y = “Employment Growth (2023–2033, %)”, color = “Occupational Domain” ) + theme_minimal() + theme(legend.position = “bottom”)\n\n\nClean or Simplify Education Labels\ncombined &lt;- combined |&gt; mutate(Education.Simple = case_when( str_detect(Typical.Entry.Level.Education, “Bachelor”) ~ “Bachelor’s”, str_detect(Typical.Entry.Level.Education, “Master”) ~ “Master’s”, str_detect(Typical.Entry.Level.Education, “Doctor”) ~ “Doctoral”, str_detect(Typical.Entry.Level.Education, “high school”) ~ “High school”, TRUE ~ “Other” ))\n\n\nColor by Typical Entry-Level Education\nggplot(combined, aes(x = AI.Impact.Numeric, y = Employment.Growth, color = Education.Simple)) + geom_point(alpha = 0.7, size = 2) + geom_smooth(method = “lm”, se = FALSE, color = “black”) + labs( title = “AI Impact vs. Employment Growth (Colored by Education Level)”, x = “AI Impact (%)”, y = “Employment Growth (2023–2033, %)”, color = “Education Level” ) + theme_minimal() + theme(legend.position = “bottom”)\n\n\nFacet by Domain\nggplot(combined, aes(x = AI.Impact.Numeric, y = Employment.Growth)) + geom_point(aes(color = Domain), alpha = 0.7, size = 2) + geom_smooth(method = “lm”, se = FALSE, color = “black”) + facet_wrap(~ Domain, scales = “free_y”) + labs( title = “AI Impact vs. Employment Growth by Occupational Domain”, x = “AI Impact (%)”, y = “Employment Growth (2023–2033, %)” ) + theme_minimal() + theme(legend.position = “none”)\n\n\nReorder Education Levels\ncombined &lt;- combined %&gt;% mutate(Education.Level = factor(Typical.Entry.Level.Education, levels = c(“No formal educational credential”, “Postsecondary nondegree award”, “High school diploma or equivalent”, “Some college, no degree”, “Associate’s degree”, “Bachelor’s degree”, “Master’s degree”, “Doctoral or professional degree”)))\n\n\nFacet by Education Level\nggplot(combined, aes(x = AI.Impact.Numeric, y = Employment.Growth)) + geom_point(aes(color = Education.Level), alpha = 0.7, size = 2) + geom_smooth(method = “lm”, se = FALSE, color = “black”) + facet_wrap(~ Education.Level, scales = “free_y”) + labs( title = “AI Impact vs. Employment Growth by Education Level”, x = “AI Impact (%)”, y = “Employment Growth (2023–2033, %)” ) + theme_minimal() + theme(legend.position = “none”)\n\n\nDefine thresholds\nimpact_threshold &lt;- 80 # AI impact above 90% growth_threshold &lt;- 5 # Employment growth above 10%\n\n\nFlag key occupations\ncombined &lt;- combined |&gt; mutate(Key.Occupation = ifelse(AI.Impact.Numeric &gt;= impact_threshold & Employment.Growth &gt;= growth_threshold, TRUE, FALSE))\n\n\nHighlight in Scatter Plot\nggplot(combined, aes(x = AI.Impact.Numeric, y = Employment.Growth)) + geom_point(aes(color = Key.Occupation), alpha = 0.6, size = 2) + geom_smooth(method = “lm”, se = FALSE, color = “gray40”) + scale_color_manual(values = c(“FALSE” = “lightgray”, “TRUE” = “firebrick”)) + labs( title = “Highlighting Occupations with High AI Impact and Job Growth”, x = “AI Impact (%)”, y = “Employment Growth (2023–2033, %)”, color = “Key Occupation” ) + theme_minimal()\n\n\nLabel Key Occupations\nlibrary(ggrepel)\nggplot(combined, aes(x = AI.Impact.Numeric, y = Employment.Growth)) + geom_point(aes(color = Key.Occupation), alpha = 0.6, size = 2) + geom_text_repel( data = subset(combined, Key.Occupation == TRUE), aes(label = Job.titiles), size = 3, max.overlaps = 20 ) + geom_smooth(method = “lm”, se = FALSE, color = “gray40”) + scale_color_manual(values = c(“FALSE” = “lightgray”, “TRUE” = “firebrick”)) + labs( title = “Key Occupations with High AI Impact and Job Growth”, x = “AI Impact (%)”, y = “Employment Growth (%)”, color = “Highlighted” ) + theme_minimal()\n\n\nCreate the Table\nkey_jobs &lt;- combined %&gt;% filter(Key.Occupation == TRUE) %&gt;% select(Job.titiles, Domain, AI.Impact.Numeric, Employment.Growth, Median.Annual.Wage.2024, Typical.Entry.Level.Education) %&gt;% arrange(desc(AI.Impact.Numeric), desc(Employment.Growth)) |&gt; unique()\n\n\nStatic Table for Quarto Report\nknitr::kable( head(key_jobs, 10), caption = “Top 10 Key Occupations with High AI Impact and Employment Growth”, digits = 2, align = “l” )\n\n\nInteractive Table (if using Quarto HTML)\nDT::datatable( key_jobs, options = list(pageLength = 10), caption = “Key Occupations with High AI Impact and Employment Growth” )\n\n\nStep 1: Prepare Data for Clustering\n\n\nClean wage to numeric\ncombined &lt;- combined %&gt;% mutate(Wage = as.numeric(gsub(“,”, ““, Median.Annual.Wage.2024)))\n\n\nSelect and scale features\nclustering_data &lt;- combined %&gt;% select(AI.Impact.Numeric, Employment.Growth, Wage) %&gt;% scale() # standardize\nclustering_data_clean &lt;- clustering_data %&gt;% as.data.frame() %&gt;% filter(complete.cases(.)) # removes rows with NA or NaN combined_clean &lt;- combined[complete.cases(clustering_data), ]\n\n\nStep 2: Run k-means (e.g., 3 clusters)\nset.seed(123) kmeans_result &lt;- kmeans(clustering_data_clean, centers = 3, nstart = 25)\n\n\nAdd cluster assignments to original data\ncombined_clean\\(Cluster &lt;- as.factor(kmeans_result\\)cluster)\n\n\nStep 3: Label Clusters (manually or heuristically)\naggregate(cbind(AI.Impact.Numeric, Employment.Growth, Wage) ~ Cluster, data = combined_clean, FUN = mean) combined_clean &lt;- combined_clean %&gt;% mutate(Cluster.Label = recode(Cluster, 1 = “Transforming”, 2 = “Declining”, 3 = “Safe”))\n\n\nStep 4: Visualize the Clusters\nggplot(combined_clean, aes(x = AI.Impact.Numeric, y = Employment.Growth, color = Cluster.Label)) + geom_point(alpha = 0.7, size = 2) + labs( title = “Clustered Occupation Types”, x = “AI Impact (%)”, y = “Employment Growth (2023–2033, %)”, color = “Occupation Type” ) + theme_minimal()\n\n\nStep 5: Optional — View Cluster Composition\ncombined_clean %&gt;% count(Cluster.Label) %&gt;% arrange(desc(n))\n\n\nStep 1: Create Ranking Tables\n\n\nEnsure numeric AI Impact and Growth\ncombined &lt;- combined %&gt;% mutate( AI.Impact.Numeric = as.numeric(gsub(“%”, ““, AI.Impact)), Employment.Growth = Employment.Percent.Change..2023.2033, Wage = as.numeric(gsub(”,“,”“, Median.Annual.Wage.2024)) )\n\n\nTop 10 AI-Enhanced\nai_enhanced &lt;- combined %&gt;% filter(AI.Impact.Numeric &gt;= 85, Employment.Growth &gt; 5) %&gt;% arrange(desc(AI.Impact.Numeric), desc(Employment.Growth)) %&gt;% select(Job.titiles, Domain, AI.Impact.Numeric, Employment.Growth, Wage) %&gt;% head(10)\n\n\nTop 10 At-Risk\nat_risk &lt;- combined %&gt;% filter(AI.Impact.Numeric &gt;= 85, Employment.Growth &lt; 0) %&gt;% arrange(desc(AI.Impact.Numeric), Employment.Growth) %&gt;% select(Job.titiles, Domain, AI.Impact.Numeric, Employment.Growth, Wage) %&gt;% head(10)\n\n\nStep 2A: Show as Tables (Static)\nknitr::kable(ai_enhanced, caption = “Top 10 AI-Enhanced Occupations”) knitr::kable(at_risk, caption = “Top 10 At-Risk Occupations”)\n\n\nStep 3: Chart Option (Side-by-Side Dot Plot)\nai_enhanced\\(Category &lt;- \"AI-Enhanced\"\nat_risk\\)Category &lt;- “At-Risk”\ntop_jobs &lt;- bind_rows(ai_enhanced, at_risk)\nggplot(top_jobs, aes(x = reorder(Job.titiles, AI.Impact.Numeric), y = AI.Impact.Numeric, fill = Category)) + geom_col(show.legend = FALSE) + coord_flip() + facet_wrap(~ Category, scales = “free_y”) + labs( title = “Top 10 At-Risk and AI-Enhanced Jobs”, x = “Job Title”, y = “AI Impact (%)” ) + theme_minimal()"
  },
  {
    "objectID": "summary.html#breifly-how-the-specific-analyses-help-to-address-the-motivating-question",
    "href": "summary.html#breifly-how-the-specific-analyses-help-to-address-the-motivating-question",
    "title": "Final Summary Report",
    "section": "2 breifly how the specific analyses help to address the motivating question",
    "text": "2 breifly how the specific analyses help to address the motivating question"
  },
  {
    "objectID": "summary.html#the-choice-of-data-used-including-discussion-of-any-limitations",
    "href": "summary.html#the-choice-of-data-used-including-discussion-of-any-limitations",
    "title": "Final Summary Report",
    "section": "3 the choice of data used, including discussion of any limitations",
    "text": "3 the choice of data used, including discussion of any limitations\nWe selected two data sources for our analyses. One is “AI Exposure Dataset”, which estimates the proportion of work tasks within each occupation that are implemeted by AI models or systems, along with AI Impact ratio and numerical count of human-performed tasks. It provides AI-related data for over 4,000 job titles and is one of the few data sources we can find about AI affecting employment. We can use it to do analysis of which job roles are most likely to be replaced or enhanced. But the job titles in this dataset are unstandardized without official occupation codes, which makes it harder for us to merge it with the other one. The AI Impact ratios are not derived from labor market outcomes and there is no methodology about how they are computed."
  },
  {
    "objectID": "summary.html#visualization-of-most-important-findings",
    "href": "summary.html#visualization-of-most-important-findings",
    "title": "Final Summary Report",
    "section": "4 visualization of most important findings",
    "text": "4 visualization of most important findings\n\n\n\n\n\n\n\n\n\nThe above is a scatter plot showing the relationship between a job’s AI Workload Ratio and the projected employment change from 2023 to 2033, broken down by each industry. We can see AI Risk is consistently low to moderate across all industries. Industries such as Data & IT and Medical & Healthcare show high employment growth despite AI Risk. Industries such as Administrative & Clerical and Sales & Marketing show higher AI Workload Ratios and little to no growth.\n\n\n\n\n\n\n\n\n\nBy creating multiple heatmaps that displays the contrasts in each education level based on the task count, job count and AI Impact Ratio, we obtained the following findings. Most jobs are unlikely to be fully automated by AI, even if they have a high or low number of tasks. This suggests that while AI can streamline processes, full job displacement remains limited. Jobs with fewer tasks and higher job counts are more vulnerable, particularly those requiring High School Diploma, Bachelor’s Degree and Doctoral or Professional Degree.\nOne more visual to be added later"
  },
  {
    "objectID": "summary.html#relation-to-prior-work",
    "href": "summary.html#relation-to-prior-work",
    "title": "Final Summary Report",
    "section": "5 Relation to Prior Work",
    "text": "5 Relation to Prior Work\nAI is reshaping the U.S. job market by accelarating employment shifts, favoring highly skilled roles, and eroding low pay retail jobs. U.S. BLS currently assumes its employment effects will align with historical patterns of gradual technological change. Even when technologies significantly change how tasks are performed, they do not always result in job loss, so their projections focus on task-level impact rather than broad societal effects. Our projects aims to find the correlation between multiple factors such as education and employment levels as well as job titles and industries AI will affect either positively or negatively."
  },
  {
    "objectID": "summary.html#potential-next-steps",
    "href": "summary.html#potential-next-steps",
    "title": "Final Summary Report",
    "section": "6 potential next steps",
    "text": "6 potential next steps\nlink to individual reports"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Final Summary Report",
    "section": "",
    "text": "Have you ever watched a video that a father sits in a couch playing with his kids. Then, a line shows up saying “You may be replaced by AI at work, but never at home”. That video aimed to encourage people to share more time with their family which was great. But if thinking closely, you might ask “How would I even afford a home if I was replaced at work”? This really spoils the touching moment. Now, you are probably eager to know if you will be repalced by AI at work. For a wider range of audiences, that question becomes “How will jobs be affected by AI”?\nWe intended to collect data about the AI impact on employment and come up with findings based on the analysis of the data. The analysis will help us understand how the labour market will change during the next decade. If your job is “AI Enhanced”, then you don’t need to worry at all. Oppositely, you should be thankful for the age of AI. If your job is “Safe”, you could think that as you would not lose your job, but you might want to learn some new skills to be more adaptive to automation. If your job is “At Risk”, there is a high probability that you will be replaced by AI. You could plan on changing to another job which is not risky and preparing early so that you would survive in the end. It could be a good thing if you got prepared for an enhanced job bofore you were replaced because normally you would get a much higher salary from an “AI Enhanced” occupation than that from an “At Risk” one.\nTherefore, our analysis matters to everybody. It is important either for you to prepare early if you have a job right now, or for you to plan on which job to do in the future if you are about to enter the job market."
  },
  {
    "objectID": "summary.html#motivations-and-importance-of-analysis",
    "href": "summary.html#motivations-and-importance-of-analysis",
    "title": "Final Summary Report",
    "section": "",
    "text": "Have you ever watched a video that a father sits in a couch playing with his kids. Then, a line shows up saying “You may be replaced by AI at work, but never at home”. That video aimed to encourage people to share more time with their family which was great. But if thinking closely, you might ask “How would I even afford a home if I was replaced at work”? This really spoils the touching moment. Now, you are probably eager to know if you will be repalced by AI at work. For a wider range of audiences, that question becomes “How will jobs be affected by AI”?\nWe intended to collect data about the AI impact on employment and come up with findings based on the analysis of the data. The analysis will help us understand how the labour market will change during the next decade. If your job is “AI Enhanced”, then you don’t need to worry at all. Oppositely, you should be thankful for the age of AI. If your job is “Safe”, you could think that as you would not lose your job, but you might want to learn some new skills to be more adaptive to automation. If your job is “At Risk”, there is a high probability that you will be replaced by AI. You could plan on changing to another job which is not risky and preparing early so that you would survive in the end. It could be a good thing if you got prepared for an enhanced job bofore you were replaced because normally you would get a much higher salary from an “AI Enhanced” occupation than that from an “At Risk” one. Therefore, our analysis matters to everybody. It is important either for you to prepare early if you have a job right now, or for you to plan on which job to do in the future if you are about to enter the job market."
  },
  {
    "objectID": "summary.html#how-will-jobs-be-affected-by-ai",
    "href": "summary.html#how-will-jobs-be-affected-by-ai",
    "title": "Final Summary Report",
    "section": "",
    "text": "Have you ever watched a video that a father sits in a couch playing with his kids. Then, a line shows up saying “You may be replaced by AI at work, but never at home”. That video aimed to encourage people to share more time with their family which was great. But if thinking closely, you might ask “How would I even afford a home if I was replaced at work”? This really spoils the touching moment. Now, you are probably eager to know if you will be repalced by AI at work. For a wider range of audiences, that question becomes “How will jobs be affected by AI”?\nWe intended to collect data about the AI impact on employment and come up with findings based on the analysis of the data. The analysis will help us understand how the labour market will change during the next decade. If your job is “AI Enhanced”, then you don’t need to worry at all. Oppositely, you should be thankful for the age of AI. If your job is “Safe”, you could think that as you would not lose your job, but you might want to learn some new skills to be more adaptive to automation. If your job is “At Risk”, there is a high probability that you will be replaced by AI. You could plan on changing to another job which is not risky and preparing early so that you would survive in the end. It could be a good thing if you got prepared for an enhanced job bofore you were replaced because normally you would get a much higher salary from an “AI Enhanced” occupation than that from an “At Risk” one.\nTherefore, our analysis matters to everybody. It is important either for you to prepare early if you have a job right now, or for you to plan on which job to do in the future if you are about to enter the job market."
  },
  {
    "objectID": "summary.html#specific-analyses",
    "href": "summary.html#specific-analyses",
    "title": "Final Summary Report",
    "section": "2 Specific Analyses",
    "text": "2 Specific Analyses\nTo better answer the motivating question, we were trying to answer three specific questions first using our combined dataset. The first specific question is “What job titles and industries have the highest AI Workload Ratio and how will it affect the job market”? AI Workload Ratio is a computed value by dividing the number of human-performed tasks by the number of AI models associated with each job title. It represents how dependent a job role is on AI implementation. Job titles and industries with the highest ratio are least dependent on AI, which means tasks in these fields are most done by human-beings compared to other fields. At least, there is no sign of completely replacing by AI or these jobs are harder to be replaced.\nThe second question is “How does the AI Impact ratio correlate with the number of tasks performed and the education level associated with different job titles”? The AI Impact ratio represents AI’s influence on each job title. By comparing the impact ratio and the number of AI-performed tasks, we can verify if one job role is really affected by AI and replaceable. The correlation can show the relationship between the AI impact and the number of AI-performed tasks associated with all job roles. The result will tell us whether a job with high AI impact has many of its tasks done by AI models.\nThe third question is “Is there a relationship between AI Impact and projected employment growth by occupation, and what does it reveal about the future of work”? An occupation with high projected employment growth and high AI Impact could be considered as “AI Enhanced”. An occupation with low or negative employment growth and high AI Impact might be “At Risk”. Other jobs could be “Safe” in the following ten years. If there a strong relationship between AI Impact and projected employment growth, either positive or negative, our job market will be affected significantly by AI."
  },
  {
    "objectID": "summary.html#data-sources",
    "href": "summary.html#data-sources",
    "title": "Final Summary Report",
    "section": "3 Data Sources",
    "text": "3 Data Sources\nWe selected two data sources for our analyses. One is “AI Exposure Dataset”, which estimates the proportion of work tasks within each occupation that are implemeted by AI models or systems, along with AI Impact ratio and numerical count of human-performed tasks. It provides AI-related data for over 4,000 job titles and is one of the few data sources we can find about AI affecting employment. We can use it to do analysis of which job roles are most likely to be replaced or enhanced. But the job titles in this dataset are unstandardized without official occupation codes, which makes it harder for us to merge it with the other one. The AI Impact ratios are not derived from labor market outcomes and there is no methodology about how they are computed.\nThe other dataset contains 2023-2033 U.S. Bureau of Labor Statistics projections, including occupation titles with official occupation codes, employment change, median wages and entry-level education. This dataset is from U.S. BLS, an official and trusted source. It includes rich occupational details like education and wages. We can see which job titles are growing or shrinking in the following years and how that correlats with AI Impact. However, it does not have any AI-specific estimates and the projections could be outdated quickly, especially in tech-driven fields."
  },
  {
    "objectID": "summary.html#potential-future-work",
    "href": "summary.html#potential-future-work",
    "title": "Final Summary Report",
    "section": "6 Potential Future Work",
    "text": "6 Potential Future Work\nIf we had more time, we would refine fuzzy matching and analyze past employment trends. We could also use additional data sources like analyzed LinkedIn or Indeed postings, and Glassdoor or company datasets.\nlink to individual reports"
  },
  {
    "objectID": "individual.html",
    "href": "individual.html",
    "title": "AI Impact and Employment Growth: Mapping the Future of Work",
    "section": "",
    "text": "Artificial Intelligence (AI) is reshaping the modern workforce. This report explores how AI’s projected impact on job tasks correlates with anticipated employment growth or decline over the next decade. Specifically, we ask:\n\nIs there a relationship between AI impact and projected employment growth by occupation, and what does it reveal about the future of work?"
  },
  {
    "objectID": "individual.html#introduction",
    "href": "individual.html#introduction",
    "title": "AI Impact and Employment Growth: Mapping the Future of Work",
    "section": "",
    "text": "Artificial Intelligence (AI) is reshaping the modern workforce. This report explores how AI’s projected impact on job tasks correlates with anticipated employment growth or decline over the next decade. Specifically, we ask:\n\nIs there a relationship between AI impact and projected employment growth by occupation, and what does it reveal about the future of work?"
  },
  {
    "objectID": "individual.html#data-sources",
    "href": "individual.html#data-sources",
    "title": "AI Impact and Employment Growth: Mapping the Future of Work",
    "section": "2 Data Sources",
    "text": "2 Data Sources\n\nAI Impact Dataset: Contains AI workload ratios and task information by job title.\nEmployment Projections Dataset: Includes projected job growth, median wages, and education requirements by occupation.\n\nBoth datasets required cleaning and merging based on job titles, which often involved multiple aliases or fuzzy matches."
  },
  {
    "objectID": "individual.html#data-processing",
    "href": "individual.html#data-processing",
    "title": "AI Impact and Employment Growth: Mapping the Future of Work",
    "section": "3 Data Processing",
    "text": "3 Data Processing\n\n\nCode\nai = read.csv(\"~/sta9750-2025-spring/data/cp/my_data.csv\")\nemployment = read.csv(\"~/sta9750-2025-spring/data/cp/employment projections.csv\")\n\n#library(dplyr)\n#glimpse(ai)\n#glimpse(employment)\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\n\n# Clean job titles\nai &lt;- ai |&gt;\n  mutate(Job.title.clean = str_to_lower(str_trim(Job.titiles)))\n\nemployment &lt;- employment |&gt;\n  mutate(Occupation.Title.clean = str_to_lower(str_trim(Occupation.Title)))\n\n# Occupation.Title contains multiple titles separated by *, we split those:\nemployment &lt;- employment |&gt;\n  separate_rows(Occupation.Title.clean, sep = \"\\\\*\") |&gt;\n  mutate(Occupation.Title.clean = str_trim(Occupation.Title.clean))\n\n# Use a fuzzy join as exact matches may not work\n# install.packages(\"fuzzyjoin\")\nlibrary(fuzzyjoin)\n\ncombined &lt;- stringdist_inner_join(ai, employment,\n                                  by = c(\"Job.title.clean\" = \"Occupation.Title.clean\"),\n                                  method = \"jw\", max_dist = 0.15) \n\n# remove helper columns\ncombined &lt;- combined |&gt; select(-Job.title.clean, -Occupation.Title.clean)"
  },
  {
    "objectID": "individual.html#ai-impact-vs.-projected-employment-growth",
    "href": "individual.html#ai-impact-vs.-projected-employment-growth",
    "title": "AI Impact and Employment Growth: Mapping the Future of Work",
    "section": "4 AI Impact vs. Projected Employment Growth",
    "text": "4 AI Impact vs. Projected Employment Growth\nTo explore the relationship between AI exposure and job market trends, I created a scatter plot comparing the AI Impact (%) of each occupation with its projected employment growth (2023–2033). AI impact scores were converted from percentage strings to numeric values, and employment growth was taken directly from U.S. Bureau of Labor Statistics projections.\n\n\nCode\nlibrary(ggplot2)\nlibrary(readr)\n# Convert AI Impact % to numeric\ncombined &lt;- combined |&gt;\n  mutate(AI.Impact.Numeric = as.numeric(str_remove(AI.Impact, \"%\")),\n         Employment.Growth = Employment.Percent.Change..2023.2033)\n\n# Scatter plot: AI impact vs. employment growth\nggplot(combined, aes(x = AI.Impact.Numeric, y = Employment.Growth)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"AI Impact vs. Projected Employment Growth\",\n       x = \"AI Impact (%)\",\n       y = \"Employment Growth (2023–2033, %)\",\n       caption = \"Data: AI Impact and U.S. Employment Projections\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe chart shows a wide dispersion of data points, with no strong visual trend indicating a direct relationship between AI exposure and job growth. The red regression line represents a linear model, which shows only a slight negative slope. This suggests that, on average, occupations with higher AI impact tend to have slightly lower projected growth, but the correlation is weak.\nTo quantitatively assess the relationship between AI exposure and projected job growth, I conducted a Pearson correlation test between:\n\ncombined$AI.Impact.Numeric (AI impact score as a percentage), and\ncombined$Employment.Growth (projected percent change in employment from 2023 to 2033).\n\n\n\nCode\n# Correlation coefficient\ncor.test(combined$AI.Impact.Numeric, combined$Employment.Growth)\n\n\n\n4.1 Test Results\n\nt = -6.7359, df = 4124, p-value = 1.854e-11\n\n\ncorrelation coefficient = -0.104\n\n\n95% CI: [-0.134, -0.074]\n\nThe correlation coefficient is -0.104, indicating a very weak negative correlation between AI impact and employment growth. The p-value &lt; 0.0001 confirms this relationship is statistically significant, meaning the likelihood of observing this pattern by chance is extremely low. However, the magnitude of the correlation is small, suggesting that AI exposure explains very little of the variation in employment growth across occupations.\nThese results reinforce the earlier visual findings. While AI may play a role in shaping job trajectories, it is not the sole or dominant factor influencing projected employment changes. Some highly impacted jobs are expected to grow — possibly due to AI augmentation rather than replacement.\n\n\n4.2 Interactive Visualization: AI Impact vs. Employment Growth by Domain\n\n\nCode\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Base ggplot\np &lt;- ggplot(combined, aes(x = AI.Impact.Numeric, \n                          y = Employment.Growth,\n                          color = Domain,\n                          text = paste(\n                            \"Job Title:\", Job.titiles,\n                            \"&lt;br&gt;AI Impact:\", AI.Impact,\n                            \"&lt;br&gt;Employment Growth:\", Employment.Growth, \"%\",\n                            \"&lt;br&gt;Domain:\", Domain))) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"AI Impact vs. Employment Growth (Interactive by Domain)\",\n    x = \"AI Impact (%)\",\n    y = \"Employment Growth (2023–2033, %)\",\n    color = \"Occupational Domain\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n# Convert to interactive plot\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\nTo enhance interpretability and user engagement, the scatter plot of AI impact versus projected employment growth was upgraded to an interactive graphic using the plotly library in R. In this version of the chart, each point represents an individual occupation. Points are color-coded by occupational domain, such as Healthcare, IT, or Clerical work. Users can hover over points to reveal job-specific details, including:\n\nJob title\nAI impact score (%)\nProjected employment growth (%)\nOccupational domain\n\nThis visualization helps bridge the gap between macro-level insights (e.g., AI is negatively correlated with growth) and micro-level stories (e.g., specific job roles that defy the trend). It supports a richer understanding of how AI may transform work differently across occupational domains.\n\n\n\nCode\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(readr)\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"AI Impact vs. Employment Growth Dashboard\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"domain\", \"Select Domain(s):\", \n                  choices = sort(unique(combined$Domain)), \n                  selected = unique(combined$Domain),\n                  multiple = TRUE),\n      sliderInput(\"aiImpact\", \"AI Impact Range (%)\", \n                  min = 0, max = 100, value = c(0, 100)),\n      downloadButton(\"downloadData\", \"Download Filtered Data\")\n    ),\n    \n    mainPanel(\n      plotlyOutput(\"interactivePlot\", height = \"700px\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  # Reactive data based on inputs\n  filtered_data &lt;- reactive({\n    combined %&gt;%\n      filter(Domain %in% input$domain,\n             AI.Impact.Numeric &gt;= input$aiImpact[1],\n             AI.Impact.Numeric &lt;= input$aiImpact[2])\n  })\n  \n  # Interactive plot\n  output$interactivePlot &lt;- renderPlotly({\n    p &lt;- ggplot(filtered_data(), aes(x = AI.Impact.Numeric, \n                                     y = Employment.Growth)) +\n      geom_point(aes(color = Domain, text = paste(\n        \"Job:\", Job.titiles,\n        \"&lt;br&gt;AI Impact:\", AI.Impact,\n        \"&lt;br&gt;Growth:\", Employment.Growth, \"%\",\n        \"&lt;br&gt;Domain:\", Domain\n      )), alpha = 0.7, size = 2) +\n      geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n      facet_wrap(~ Domain, scales = \"free_y\") +\n      labs(\n        title = \"AI Impact vs. Employment Growth by Domain\",\n        x = \"AI Impact (%)\",\n        y = \"Employment Growth (%)\"\n      ) +\n      theme_minimal() +\n      theme(legend.position = \"none\")\n    \n    ggplotly(p, tooltip = \"text\")\n  })\n  \n  # Download filtered dataset\n  output$downloadData &lt;- downloadHandler(\n    filename = function() {\n      paste(\"filtered_ai_employment_data_\", Sys.Date(), \".csv\", sep = \"\")\n    },\n    content = function(file) {\n      write.csv(filtered_data(), file, row.names = FALSE)\n    }\n  )\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\n\nShiny applications not supported in static R Markdown documents"
  }
]